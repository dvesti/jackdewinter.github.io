var tipuesearch = {"pages":[{"title":"Measuring Testing in Python Scripts","text":"Introduction As part of the process of creating a Markdown Linter to use with my personal website, I firmly believe that it is imperative that I have solid testing on that linter and the tools necessary to test the linter. In my previous article on Scenario Testing Python Scripts , I described the in-process framework that I use for testing Python scripts from within PyTest. That framework ensures that I can properly test Python scripts from the start of the script, increasing my confidence that they are tested properly. To properly figure out how my tests are doing and what their impact is, I turned on a number of features that are available with PyTest. The features either make testing easier or measure the impact of those tests and relay that information. This article describes my PyTest configuration and how that configuration provides a benefit to my development process. Adding Needed Packages to PyTest There are four main Python packages that I use in conjunction with PyTest. The pytest-console-scripts package is the main one, allowing PyTest to be invoked from the command line. Since I am in favor of automating process where possible, this is a necessity. From a test execution point of view, the pytest-timeout is used to set a timeout on each test, ensuring that a single runaway test does not cause the set of tests to fail to complete. For reporting, the pytest-html package is useful for creating an HTML summary of the test results. The pytest-cov package adds coverage of the source code, with reporting of that coverage built in. I have found that all of these packages help me in my development of Python scripts, so I highly recommend these packages. Depending on the Python package manager and environment in use, there will be slightly different methods to install these packages. For plain Python this is usually: pip install pytest-console-scripts == 0 .20 pytest-cov == 2 .8.1 pytest-timeout == 1 .3.3 pytest-html == 2 .0.1 As I have used pipenv a lot in my professional Python development, all of my personal projects use it for setting up the environment and it's dependencies. Similar to the line above, to install these packages into pipenv requires executing the following line in the project's directory: pipenv install pytest-console-scripts == 0 .20 pytest-cov == 2 .8.1 pytest-timeout == 1 .3.3 pytest-html == 2 .0.1 Configuring PyTest For Those Packages Unless information is provided on the command line, PyTest will search for a configuration file to use. By default, setup.cfg is the name of the configuration file it uses. The following fragment of my setup.cfg file takes care of the configuration for those PyTest packages. [tool:pytest] testpaths=./test cache_dir=./build/test/.pytest_cache junit_family=xunit2 addopts=--timeout=10 --cov --cov-branch --cov-fail-under=90 --strict-markers -ra --cov-report xml:report/coverage.xml --cov-report html:report/coverage --junitxml=report/tests.xml --html=report/report.html While all configuration is important, the following sections are most important in the setting up of PyTest for measuring the effects of testing: testpaths=./test - relative path where PyTest will scan for tests addopts/--junitxml - creates a junit-xml style report file at given path addopts/--cov - record coverage information for everything addopts/--cov-branch - enables branch coverage addopts/--cov-report - types of report to generate and their destination paths default/--cov-config - configuration file for coverage, defaulting to .coveragerc In order, the first two configuration items tells PyTest where to look for tests to execute and where to place the JUnit-styled XML report with the results of each test. The next three configuration items turn on coverage collection, enable branch coverage, and specifies what types of coverage reports to produce and where to place them. Finally, because the --cov-config is not set, the default location for the coverage configuration file is set to .coveragerc . For all of my projects, the default .coveragerc that I use, with a small change to the source= line is: [run] source = pyscan [report] exclude_lines = # Have to re-enable the standard pragma pragma: no cover # Don't complain about missing debug-only code: def __repr__ if self\\.debug # Don't complain if tests don't hit defensive assertion code: raise AssertionError raise NotImplementedError # Don't complain if non-runnable code isn't run: if 0: if __name__ == .__main__.: To be honest, this .coveragerc template is something I picked up somewhere, but it works, and works well for my needs. The exclude lines work in all case that I have come across, so I haven't touched them in the 2+ years that I have been writing code in Python. Benefits Of This Configuration Given the setup from the last section, there are two main benefits that I get from this setup. The first benefit is machine readable XML information generated for the test results and the test coverage. While this is not immediately consumable in it's current form, that data can be harvested in the future to provide concise information about what has been tested. The second benefit is to provide human readable information about the tests that have been executed. The HTML file located at report/report.html relays the results of the last series of tests while the HTML file located at report/coverage/index.html relays the coverage information for the last series of tests. Both of these pieces of information are useful for different reasons. In the case of the test results HTML, the information presented on the test results page is mostly the same information as is displayed by PyTest when executed on the command line. Some useful changes are present, such as seeing all of the test information at once, instead of just a . for a successful test, a F for a failed test, and so on. I have found that having this information available on one page allows me to more quickly debug an issue that is affecting multiple tests, instead of scrolling through the command line output one test at a time. In the case of the test coverage HTML, the information presented on this page is invaluable. For each source file in the Python project being tested, there is a page that clearly shows which lines of each Python script are exercised by the tests, By using these pages as a guide, I can determine what tests I need to add to ensure that the scripts are properly covered. By using these two tools together, I can quickly determine what tests to add, and when tests fail, I can determine why they failed and look for patterns in the failures. This enables me to quickly figure out where the blind spots are in my testing, and to address them quickly. This in turn can help me to figure out the best way to improve the quality of the project I am working on. If this finds an issue with an existing requirement, that requirement can be adjusted or a new requirement added to fulfil the deficiency. If the requirements were all right and the code it was testing was incorrect, that code can be addressed. If the coverage page shows that code was written but not tested, a new test function can be introduced to cover that scenario. Each observation and its appropriate action work to improve the quality of the software project. What Was Accomplished This article showed how to setup PyTest using a configuration file. With that configuration file, it was set up to provide timeouts for tests, provide output on the test results, and provide a coverage report of how well the tests covered the scripts under test. This was all accomplished to better understand the impact of tests on a project and provide better information on how they succeed (test coverage) or fail (test results). By understanding this information, the quality of the software can be measured and improved on if needed. What Is Next? In the next article, I will briefly describe the PyScan tool I have written, and how it takes the XML information generate by the --junitxml=report/tests.xml option and the --cov-report xml:report/coverage.xml option and produces concise summaries of that information. I will also give a number of examples of how I use this information during my development of Python projects.","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/","loc":"https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/"},{"title":"Scenario Testing Python Scripts","text":"Introduction As part of the process of creating a Markdown Linter to use with my personal website, I firmly believe that it is imperative that I have solid testing on that linter and the tools necessary to test the linter. This testing includes executing those Python tool scripts from start to finish and verifying that everything is working properly. From my experience, one of the most efficient ways to scenario test the project's Python scripts is to use an in-process framework for running Python scripts. Because of the way that Python works, it is very feasible to scenario test the Python scripts using the in-process framework which I describe in this article. To show how the framework works in practice, I reference my PyScan project to illustrate how I use this framework to test the scenarios in that project. Specifically, I talk about the pytest_execute.py file which contains the bulk of the code I use to write scenario tests with. 1 Determine the Requirements As with most of my projects, the first thing I do for any new project is to cleanly determine and document the requirements for the project. Even though this project is a single component used to test the tools and other components, I feel strongly that it is still important to follow those guidelines to ensure the right component is built in the right way. The basic requirements are pretty easy to define for this in-process test component: execute the Python script independently and capture all relevant information about it's execution, verifying that information against expected values. The devil is in the details however. I believe that a good definition of \"execute the Python script\" must include the ability to set the current working directory and arguments for the command line. For a good definition of \"capture all relevant information\", I believe the requirements must include capturing of the script's return code as well as any output to standard out (stdout) and standard error (stderr). As this component executes the script in-process, any attempts to exit the script prematurely must be properly captured, and the state of the test must be returned to what it was at the beginning of the test. Finally, to satisfy the \"verifying\" requirement, the component must have easy to use comparison functions, with informative output on any differences that arise during verification. Finding a balance between too many bulky requirements and too few lean requirements is a tough balance to achieve. In this case, I feel that I have achieved that balance by ensuring all of the major parts of the requirements are specified at a high enough level to be able to communicate clearly without ambiguity. Here's hoping I get the balance right! Capture Relevant Information The first thing to take care of is a class that will contain the information to satisfy the \"capture all relevant information\" requirement above. As the requirement specifies the 3 things that need to be captured, all that is left to do is to create a class to encapsulate these variables as follows: class InProcessResult : \"\"\" Class to provide for an encapsulation of the results of an execution. \"\"\" def __init__ ( self , return_code , std_out , std_err ): self . return_code = return_code self . std_out = std_out self . std_err = std_err Executing the Script Now that there is an object to collect the information about the script's execution, a simple function is needed to collect that information. In the InProcessExecution base class, the invoke_main function serves this purpose. def invoke_main ( self , arguments = None , cwd = None ): \"\"\" Invoke the mainline so that we can capture results. \"\"\" saved_state = SystemState () std_output = io . StringIO () std_error = io . StringIO () try : returncode = 0 sys . stdout = std_output sys . stderr = std_error if arguments : sys . argv = arguments . copy () else : sys . argv = [] sys . argv . insert ( 0 , self . get_main_name ()) if cwd : os . chdir ( cwd ) self . execute_main () except SystemExit as this_exception : returncode = self . handle_system_exit ( this_exception , std_error ) except Exception : returncode = self . handle_normal_exception () finally : saved_state . restore () return InProcessResult ( returncode , std_output , std_error ) Before changing any of the existing system values, changes that by their very nature are be made across the entire Python interpreter, the original values of those system values are kept safely in an instance of the the SystemState class in the saved_state variable. As I want to ensure that the saved system state is reverted back to regardless of what happens, a try-finally block is used to ensure that the saved_state.restore function is called to restore the system back to it's original state. Once the system state is safely stored away, changes to those system values can be made. Instances of the StringIo class are used to provide alternative streams for stdout and stderr. A new array is assigned to sys.argv , either an empty array if no arguments are provided or a copy of the provided array if provided. To the start of that array is inserted the name of the main script, to ensure that libraries expecting a properly formatted array of system arguments are happy. Finally, if an alternate working directory is provided to the function, the script changes to that directory. To reiterate, the reason it is acceptable to make all of these changes to the system state is that we have a safe copy of the system state stored away that we will revert to when this function completes. After the execute_main function is called to execute the script in the specified manner, there are three possibilities that the function needs to capture the information for. In the case of a normal fall-through execution, the returncode = 0 statement at the start of the try-finally block sets the return code. If a SystemExit exception is thrown, the handle_system_exit function does a bit of process to figure out the return code based on the contents of the exception. Finally, if the execution is terminated for any other exception, the handle_normal_exception makes sure to print out decent debug information and sets the return code to 1. In all three cases, the collected values for stdout and stderr are collected, combined with the return code determined earlier in this paragraph, and a new instance of the InProcessResult class is returned with these values. Verifying Actual Results Against Expected Results When I started with the assert_results function, it was only 3 statements in quick succession: 3 assert statements asserting that the actual values for stdout, stderr and the return code matched the expected values. However, as I started using that function, it was quickly apparent that when something did fail, there was a certain amount of repetitive debugging that I performed to determine why the assert was triggered. At first I added some extra information to the assert statements, and that worked for the return code. But there were still two issues. The first issue was that, in the case where all 3 expected values were different than the actual values, it took 3 iterations of cleaning up the test before it passed. Only when I cleared up the first failure did I see the second failure, and only after the second failure was dealt with did I see the third. While this was workable, it was far from efficient. The second issue was that if there were any differences with the contents of the stdout or stderr stream, the differences between the expected value and the actual value were hard to discern by just looking at them. To address the first issue, I changed the simple assert_results function to the following: def assert_results ( self , stdout = None , stderr = None , error_code = 0 ): \"\"\" Assert the results are as expected in the \"assert\" phase. \"\"\" stdout_error = self . assert_stream_contents ( \"stdout\" , self . std_out , stdout ) stderr_error = self . assert_stream_contents ( \"stderr\" , self . std_err , stderr ) return_code_error = self . assert_return_code ( self . return_code , error_code ) combined_error_msg = \"\" if stdout_error : combined_error_msg = combined_error_msg + \" \\n \" + str ( stdout_error ) if stderr_error : combined_error_msg = combined_error_msg + \" \\n \" + str ( stderr_error ) if return_code_error : combined_error_msg = combined_error_msg + \" \\n \" + str ( return_code_error ) assert not combined_error_msg , ( \"Either stdout, stderr, or the return code was not as expected. \\n \" + combined_error_msg ) The key to resolving the first issue is in capturing the information about all differences that occur, and then asserting only once if any differences are encountered. To accomplish this, several comparison functions are required that capture individual asserts and relay that information back to the assert_results function where they can be aggregated together. It is these comparison functions that are at the heart of the assert_results function. The easiest of these comparison functions is the assert_return_code function, which simply compares the actual return code and the expected return code. If there is any difference, the error message for the assert statement is descriptive enough to provide a clear indication of what the difference is. That raised AssertionError is then captured and returned from the function so the assert_results function can report on it. @classmethod def assert_return_code ( cls , actual_return_code , expected_return_code ): \"\"\" Assert that the actual return code is as expected. \"\"\" result = None try : assert actual_return_code == expected_return_code , ( \"Actual error code (\" + str ( actual_return_code ) + \") and expected error code (\" + str ( expected_return_code ) + \") differ.\" ) except AssertionError as ex : result = ex return result A slightly more complicated function is the assert_stream_contents comparison function. To ensure that helpful information is returned in the assert failure message, it checks to see if the expected_stream is set and calls compare_versus_expected if so. (More about that function in a minute.) If not set, the assert used clearly states that the stream was expected to be empty, and the actual stream is not empty. def assert_stream_contents ( self , stream_name , actual_stream , expected_stream ): \"\"\" Assert that the contents of the given stream are as expected. \"\"\" result = None try : if expected_stream : self . compare_versus_expected ( stream_name , actual_stream , expected_stream ) else : assert not actual_stream . getvalue (), ( \"Expected \" + stream_name + \" to be empty. Not: \\n --- \\n \" + actual_stream . getvalue () + \" \\n --- \\n \" ) except AssertionError as ex : result = ex finally : actual_stream . close () return result Addressing the second issue with the initial assert_results function, the differences between the two streams being difficult to discern, is the compare_versus_expected function. My first variation on this function simply used the statement assert actual_stream.getvalue() != expected_text , producing the same assert result, but lacking in the description of why the assert failed. The second variation of this function added a better assert failure message, but left the task of identifying the difference between the two strings on the reader of the failure message. The final variation of this function uses the difflib module and the difflib.ndiff function to provide a detailed line-by-line comparison between the actual stream contents and the expected stream contents. By using the difflib.ndiff function in this final variation, the assert failure message now contains a very easy to read list of the differences between the two streams. import difflib @classmethod def compare_versus_expected ( cls , stream_name , actual_stream , expected_text ): \"\"\" Do a thorough comparison of the actual stream against the expected text. \"\"\" if actual_stream . getvalue () != expected_text : diff = difflib . ndiff ( expected_text . splitlines (), actual_stream . getvalue () . splitlines () ) diff_values = \" \\n \" . join ( list ( diff )) assert False , ( stream_name + \" not as expected: \\n --- \\n \" + diff_values + \" \\n --- \\n \" ) Using it all together To start using the work that completed in the sections above, a proper subclass of the InProcessExecution class is required. Because that class is an abstract base class, a new class MainlineExecutor is required to resolve the execute_main function and the get_main_name function. class MainlineExecutor ( InProcessExecution ): def __init__ ( self ): super () . __init__ () resource_directory = os . path . join ( os . getcwd (), \"test\" , \"resources\" ) self . resource_directory = resource_directory def execute_main ( self ): PyScan () . main () def get_main_name ( self ): return \"main.py\" The MainlineExecutor class implements those two required functions. The get_main_name function returns the name of the module entry point for the project. This name is inserted into the array of arguments to ensure that any functions based off of the command line sys.argv array resolves properly. The execute_main function implements the actual code to invoke the main entry point for the script. In the case of the PyScan project, the entry point at the end of the main.py script is: if __name__ == \"__main__\" : PyScan () . main () Therefore, the contents of the execute_main function is PyScan().main() . In addition to those two required functions, there is some extra code in the constructor for the class. Instead of recomputing the resource directory in each test that requires it, the MainlineExecutor class computes it in the constructor to keep the test functions as clean as possible. While this is not required when subclassing from InProcessExecution , it has proven very useful in practice. To validate the use of the MainlineExecutor class with the project, I created a simple scenario test to verify that the version of the scanner is correct. This is very simple test, and verifying that the framework passes such a simple test increases the confidence in the framework itself. At the start of the scenario test, the executor variable is created and assigned an instance of our new class MainlineExecutor as well as specify that the arguments to use for the script as [\"--version\"] . in the array suppplied_arguments In keeping with the Arrange-Act-Assert pattern, I then specify the expected behaviors for stdout (in expected_output ), stderr (in expected_error ), and the return code from the script (in expected_return_code ). Having set everything up in the Assert section of the test, the Act section simply invokes the script using the executor.invoke_main function with the suppplied_arguments variable assigned previously, and collect the results. Once collected, the execute_results.assert_results function verifies those actual results against the expected results, asserting if there are differences. def test_get_summarizer_version (): \"\"\" Make sure that we can get information about the version of the summarizer. \"\"\" # Arrange executor = MainlineExecutor () suppplied_arguments = [ \"--version\" ] expected_output = \"\"\" \\ main.py 0.1.0 \"\"\" expected_error = \"\" expected_return_code = 0 # Act execute_results = executor . invoke_main ( arguments = suppplied_arguments , cwd = None ) # Assert execute_results . assert_results ( expected_output , expected_error , expected_return_code ) What Does Using This Look Like? In terms of writing scenario tests, the tests are usually as simple to write as the test_get_summarizer_version function in the last section. If there are parts of the output that have a non-constant value, such as the full path of the directory in which the test is executed in, the expected_output variable would have to be set to compensate for that variability, but that is an expected complexity. For the PyScan project, a quick scan of the PyScan test_scenarios.py file reveals that for this project, the non-constant values most often occur with failure messages, especially ones that relay path information in their failure messages. When that happens, such as with the test_summarize_junit_report_with_bad_source test function, that extra complexity is not overwhelming and does not make the test function unreadable. In terms of the test output for a passing test, there is no difference. If executing pipenv run pytest produced a . for a successful test before, it remains a . now. The big difference is in what is displayed when there is a difference in the test output. In the case where there is a single character difference in the test output, such as changing the expected output for the test_get_summarizer_version test to main.py 0.1.1 , the output below clearly shows where the actual output and expected output differ. Note that in these comparisons, the line that starts with the - character is the expected output and the line that starts with the + character is the actual output. E AssertionError: Either stdout, stderr, or the return code was not as expected. E E stdout not as expected: E --- E - main.py 0.1.1 E ? &#94; E E + main.py 0.1.0 E ? &#94; E E --- In the case where a line in the test output is completely different, such as changing the expected output to This is another line , the output below clearly reflects that difference: E AssertionError: Either stdout, stderr, or the return code was not as expected. E E stdout not as expected: E --- E - This is another line E + main.py 0.1.0 E --- Finally, in the case where the actual output contains either more lines or less lines that the expected output, such as adding the line This is another line to the expected output, the output below clearly shows that difference. In this example, as the first line is at the start of both the actual output and expected output, it is shown without any prefix to the line. E AssertionError: Either stdout, stderr, or the return code was not as expected. E E stdout not as expected: E --- E main.py 0.1.0 E - This is another line E --- Summary While the pytest_execute.py file that I use as the base for my scenario tests isn't rocket science, it is invaluable to me in creating simple, easy-to-read scenario tests. At the heart of the module is the base requirement (as stated above) to execute the Python script independently, capture all relevant information about it's execution, and then verifying that information against expected values. Based on my experience and evolution of this module, I believe that it handily satisfies the requirements with ease. To keep things simple for the article, the additional_error parameter from a number of the functions has been removed. This parameter is used in the PyMarkdown project and will be documented as part of my articles on that project. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/","loc":"https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/"},{"title":"Have a Happy Winter Holiday 2019","text":"I just wanted to take a quick minute and wish everyone a happy winter holiday season as 2019 winds to a close. When I resume posts in the new year, I will be trying to publish weekly posts on Mondays instead of Sundays, and see how that goes. Safe travels, and well wishes.","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/12/29/have-a-happy-winter-holiday-2019/","loc":"https://jackdewinter.github.io/2019/12/29/have-a-happy-winter-holiday-2019/"},{"title":"Markdown Linter - Parser Testing Strategy","text":"Introduction In the previous articles in this series, I discussed the requirements for the Markdown linter that I am writing. From a development point of view, the main requirement is the need for an accurate stream of tokens emitted by the parser. Due to the absence of any Markdown-to-token parsers out there, I need to write a new parser that outputs an accurate stream of tokens instead of a stream of HTML text. With the last article showing the patterns I am using to test the parser, it is now time to figure out a set of good strategies for the project, to ensure I can complete it without losing my confidence (and sanity). Why Is Strategy Important When Testing? When my son was younger, like most boys in his age group, he loved playing with LEGO and he loved the idea of robots. I mean, come on! I am a lot older than him and I still like LEGO and the idea of robots! Anyhow, at his school they advertised for 5th grade students that were interested in participating in a local FIRST Lego League robotics team. From the first mention of it, he was hooked. As they needed some parents to help out, I participated with him as a coach. That position was a very rewarding, very humbling, and very frustrating experience. Rewarding because I got to help 5th graders learn a little taste of what I did everyday at work. Humbling because the look in the kid's eyes when they really understood something reminded me of the benefits of being a coach. Frustrating because of almost all of the rest of the time between those two types of moments. I am not sure which parent, coach, or teacher helped me with a little gem of wisdom, but I remember it as clear as day: People have problems moving boulders, people have success moving pebbles. The idea behind that phrase is that if a team is confronted with a problem, it is like encountering a boulder that you need to move out of the way. Upon seeing a big boulder, many people take a look at it and say something similar to \"Wow! That is too big to move!\" But if you take that boulder and break it down into smaller rocks, such as pebbles, many people will just laugh with ease at moving those rocks, even if they have to do it one at a time. In a similar fashion, breaking down a big problem into smaller problems is a necessity in problem solving a situation. The boulders-to-pebbles phrase is a phrase I still use to this day when coaching people in both my professional and personal lives. Writing a parser that handles anything more significant than a single line of text is definitely \"a boulder\". I have been writing parsers for the better part of 25 years, and those parsers are still boulders to me. However, I know from experience that breaking down that \"boulder-sized\" task into more \"pebble-sized\" tasks works and works well. So here are the various items of my strategy for this project. Strategy 0: Define and Execute Testing, Linting, and Formatting For me this is a strategy that I bring to almost every project, with very few exceptions. I always start with some kind of workflow template that I apply to the project that performs formatting of the source code, linting of the source code, and executes the testing framework. Since I am a stickler for this approach, the setup for this workflow usually takes 5 minutes or less, as I usually have at least one example project lying around. By consistently executing this workflow before committing any changes, I keep the quality reasonably high as I go. Knowing that I had this framework in place for the Markdown parser was a godsend. My preference is to find frequent small break points during the implementation of a feature, and to use those points to run the workflow. For me, it increases my confidence that I am either establishing a new \"last known good point\" or that I need to retrace my steps to the last known good point to address an issue. That confidence helps me go forward with a positive attitude. Strategy 0A: Suppress Major Issues Until Later This may seem like somewhat of a counter to Strategy 0, but I see it more of allowing the project to grow, but being reminded that there is work to do. Minor issues such as stylistics and documentation are handled right away, as they have a direct impact on the maintainability of the code as it moves forward. Major issues usually involve a larger amount of code, and changing that much code usually has a fair amount of side effects unless you work to prevent those side effects. Major issues are usually of the \"too many/much\" type, such as \"too much complexity\", \"too many statements\", or \"too many boolean statements\". When I get to a really good and stable point in the project, I know I will deal with these. If I deal with the issues before I get to such a point, I am taking a chance that I won't have the stability to make the change, while limiting and dealing with any potential side effects in a clean and efficient manner. What is a good and stable point? For me, such a point has to have two dominant characteristics. The first is that I need to have a solid collection of tests in place that I can execute. These tests make sure that any refactoring doesn't negatively affect the quality of the code. The second characteristic is that the source code for the project is at a point where there is a large degree of confidence that the code in the section that I want to refactor is very solid and very well defined. This ensures that I can start looking for commonalities and efficiencies for refactoring that will enhance the source code, but not prematurely. Strategy 1: Break Tests and Development Into Task Groups Following the principle of keeping things at a good size, don't plan the entire project out ahead of time, but make sure to break things down into the groups of tasks that are needed as you need them. Following an agile approach, make sure you have a good idea of what needs to be done for a given task group, and don't worry about any more details of it until you need to. And when you reach that point, reverify the tasks before going forward and flushing out the details. For this parser, the GitHub Flavored Markdown specification delineates it's groups by the features in Markdown that are implemented. Aligning the groups specified in that document with the groups for tests and development was a solid choice from a tracking point of view. One of the reasons that I feel this worked well is because these feature groups have anywhere between 1 and 50 examples in each group. While some of the larger ones were a tiny bit too big, for the most part it was a manageable number of scenarios to handle in each group. Strategy 2: Organize Those Task Groups Themselves Once the task groups have been identified, take a step back and organize those task groups themselves. There are almost always going to be task groups that have a natural affinity to be with similar task groups, so do so. By doing similar tasks in groups, it will help identify refactorings that can be accomplished later, as well as the efficiency benefits from repeating similar processes. Especially with a larger project, those little efficiency benefits can add up quickly. As with the previous strategy, the GitHub Flavored Markdown specification comes to the rescue again. There are some implementation notes near the end of the specification that provide some guidance on grouping. The groups that I recognized were container blocks, normal blocks, and inline parsing. Normal blocks are the foundation of the parsing, so it made sense to schedule those first. Container blocks (lists and block quotes) add nesting requirements, so I scheduled those second. Finally, once all of the block level tasks are done, inline parsing (such as for emphasis) can be performed on text blocks derived at after the processing of the normal and container blocks. After re-reading the end of the specification, the example that they gave seemed to indicate that as well, so I was probably on a decent path. Strategy 3: K.I.S.S. As I mentioned in the last article, I am a big proponent of the K.I.S.S principle . While I usually arrive at an end project that has lots of nice classes and functions, worrying about that at an early stage can often be counter productive. Even if it means doing ugly string manipulations with variable names that you know you will change, that approach can often lead to cleaner code faster. Worry about getting the logic and the algorithms right first, and then worry about making it \"look pretty\". A good example of this is my traditional development practice of giving variables and functions \"garbage names\" until I am finished with a set of functions. Yes, that means during development I have variable names like \"foobar\", \"abc\", \"sdf\", and \"ghi\", just to name a few of them. When I am creating the function, I maintain a good understanding of what the variables are doing, and I want to concentrate on the logic. Once the logic is solid, I can then rename the variables to a descriptive name that accurately reflects it's purpose and use. I am not sure if this process works for everyone, but for me, not focusing on the names helps me focus on the logic itself. I also find that having a \"naming pass\" at the function when I am done with the work helps me to give each variable a more meaningful name before I commit the changes. Once again, this is one of my development practices that helps boost my productivity, and I acknowledge it might not work for everyone. For the parser, I employed this strategy whole-heartedly. The first couple of groups of work on the parser were performed by dealing with strings, with the only class for the parser being the single class containing the parsing logic. Once I got to a good point (see above), I moved a number of the parsing functions and html functions into their own static helper modules. Up until that point, it was just simpler to be creative with the logic in a raw form. After that point, it made more sense to identify and solidify the logic that encapsulated some obvious patterns, moving those algorithms into their own classes for easy identification. As with many things, finding the right points to perform changes like this are difficult to describe. I can only say that \"it felt like the right time for that change\". And as I commit and stage code frequently, if I made a mistake, I could easily rewind and either retry the change, or abandon it altogether. Strategy 4: Use Lots of Debug Output There is a phrase that we use at work called \"TTR\" or Time-To-Resolution. This is usually measured as the time taken from knowing that you have a problem until the time that the problem is resolved and it's solution is published. Added during development and debugging, spurious debug output can help provide a journal or log of what is happening in the project, allowing for a more comprehensive comparison of the output of a passing test with the output of a failing test at the same time. To be clear, using a debugger to load the code and step through it as it executes is another way to debug the code. In fact, in a fairly decent number of situations I recommend that. However, I find that the downside is that I don't get to see the flow through the code in the same way as with lots of debug statements. As with a lot of things, determining the balance between debug output and using a debugger will differ for individual developers and for individual projects. Another benefit of the debug output approach is the transition from debug output to logging. Once the project has been sufficiently stabilized and completed, one of the tasks that arises is usually to output useful log messages at various points throughout the code. I personally find that a certain percentage of the debug output that was good enough to emit during development can become quality log messages with only small changes. The parser development definitely benefitted from this strategy. Within a given task group, there were often two Markdown patterns that were almost the same. Sometimes it looked like they should being parsed differently and sometimes I couldn't figure out why they weren't parsed differently. By examining the debug output for both cases, I was able to verify whether or not the correct paths were followed, and if not, where the divergences occurred. Sure, the debug was cryptic and most of it never made it in the final version of the parser. But when I needed to debug or verify during development, it was invaluable. Strategy 5: Run Tests Frequently Don't only run tests when a set of changes is ready to commit, run those tests frequently during the development of each task. If done properly, most tests are there to verify things are as they should be, and to warn of changes or situations that fall outside of the requirements. If something is wrong, it is better to look through the last feature added to determine what the problem is, rather than trying to determine which of the last 5 features introduced that bad behavior. Therefore, by executing the tests frequently, either the confidence that the project is working properly increases or there are early and frequent indications that something is wrong. During the development of the parser, the tests were instrumental in making sure that I knew what features were \"locked down\" and which features needed work. By keeping track of that when adding a new feature, I could easily see when work on a new feature caused a previously completed feature to fail it's tests. At that point, I knew I didn't have the right solution, but I also had confidence that the changes were small enough to handle. Also, as the specification is large, there were often cases that were present but not always spelled out in the documentation as well as they could have been. However, time and time again, the saving grace for the specification were the examples, now scenarios and scenario tests in my project, sterling examples of what to expect. And as I took care to make sure they ran quickly, I was able to run all of the scenario tests in less than 10 seconds. For me, taking 10 seconds to ensure things were not broken was well worth the cost. Strategy 6: Do Small Refactors Only At Good Points While this strategy may look like a repeat of Strategy 0A: Suppress Major Issues Until Later , the scope for this strategy is on a smaller, more local level. Where Strategy 0A talks about refactoring major issues later, there are often obvious minor refactors that can be done at a local level. These changes are often done right after a function is written to fulfil a feature and rarely includes more than one function. A good example of this is taking a function that performs a given action twice with small variations and rewriting that function by encapsulating that repeated action into it's own well-named function. While such refactors almost always improve the code, care must be taken to strike a good balance between making each method more readable and trying to optimize the function ahead of time. For myself, it is often more efficient for me to see the raw code to recognize patterns from rather than already refactored code. Unless I am the author of the refactored code, I find that I don't see the same patterns as with the raw code. As with many things, \"Your Mileage May Vary\". When implementing the parser, this strategy was effectively applied at the local level to improve readability and maintainability. There were quite a few cases where the logic to detect a given case and the processing of that case were complicated. By assigning the detection of a given case to one function and the processing of that case to another function, the border between the two concepts was enhanced, making the calling function more readable. As this kind of refactoring occurred at the local level, it employed this strategy quite effectively. How Did This Help? For one, I had a plan and a strategy to deal with things. As always, something would happen during development which would require me to re-assess something. Given the above strategy, I had confidence that I would be able to deal with it, adjusting the different parts of the project as I went. Basically, I took a boulder (writing a parser) and not only broke it down into pebbles (tasks needed to write the parser), but came up with a set of rules (strategy) on what to do if I found some rocks that were previously unknown or larger than a pebble. As I mentioned at the start of the article, it's a fairly simple bit of wisdom that I was taught, but what a gem it is! What Comes Next? In the next article, I take the requirements, scenarios, and strategies and put them together to start writing the parser. As one of the test groups that I came up with was normal Markdown blocks, I will describe how I implemented those blocks as well as the issues I had in doing so cleanly.","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/","loc":"https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/"},{"title":"Markdown Linter - Setting Up Parser Tests","text":"Sidebar My apologies for this being a day or two later that usual. My son brought home a cold that knocked the stuffing out of me, I needed to take some personal time to ensure I was feeling better before writing. Thanks for your patience. Introduction As a reminder of the requirements from the last article , the big bullet-point items are: command line driven, GitHub Flavored Markdown (for now), and preserving all tokens. To make sure I have a solid set of goals to work towards, setting these requirements as part of the project was pivotal. Now that I have that as a touchstone, I need to move forward with defining how to progress with the testing of the parser at the core of the linter. Why Write a Parser? In taking a look at the kind of rules that linters support, I have observed that there are typically two categories of rules: general rules and grammar rules. For general rules such as \"tabs should not be used\", it is easy to look at any line in the document being scanned and look for a tab character. For grammatical rules such as \"headings should always be properly capitalized\", that scan is more difficult. The most difficult part of that rule is identifying whether or not any given piece of text is considered part of a header, thus engaging the rest of the rule. From experience, to properly determine which part of grammar maps to which part of text requires a capable parser, written to the specifications of the language to be parsed. Based on my research from the last article , all of the parsers that I found only translated Markdown into HTML, not any intermediate form. Since I need a clean stream of tokens before translation to HTML, the only option is to write my own parser which will output a clean stream of parsed Markdown tokens. As I am writing my own parser, I need to have a good set of tests to ensure that the parser works properly. But where to start? Where To Start With The Tests? Referring back to my article on software reliability , the 2 main types of tests that I need to decide on are scenario tests and unit tests. In a nutshell, the purpose of a scenario test is to test the input and outputs of the project and the purpose of a unit test is to test a specific function of a specific components of the project. Getting a hold of how to balance the quantity of tests that I need to write between the two of these types is my first priority. As one of the initial requirements is to support the GitHub Flavored Markdown specification , it is useful to note that the specification itself has 637 individual examples. Each example provides for the input, in Markdown, and the output, in HTML. While the output is not at the token level needed to satisfy my project's third requirement, it should be close enough. In looking at each of these examples, I need a solid set of rules that I can apply to the tokens to get them from my desired token-based output to a HTML-based output that matches the examples. It is reasonable to collect these rules as I go when I develop the various types of elements to be parsed. If I tried to do them to far ahead of time, it would invariably lead to a lot of rework. Just in time is the way to go for these rules. Taking another looking at the types of tests that I need to write, I realized that this project's test viewpoint was inverted from the usual ratio of scenario tests to unit tests. In most cases, if I have anything more than 20-30 scenario tests, I would think that I have not properly scoped the project. However, with 637 scenarios already defined for me, it would be foolish not to write at least one scenario test for each of those scenarios, adding extra scenario tests and supportive unit tests where needed. In this case, it makes more sense to focus on the scenario tests as the major set of tests to write. The balance of scenario tests to unit tests? Given 637 scenarios ready to go, I need to create at least 637 scenario tests. For those scenario tests, experimenting with the first couple of scenario tests to find a process that worked seemed to be the most efficient way forward. Given a simple and solid template for every scenario test, I had a lot of confidence to then use that template for each scenario test that I tackled. And the unit tests? In implementing any parsing code, I knew that I needed helper functions that parsed a specific type of foundational thing, like a tag in an HTML block or skipping ahead over any whitespace. The unit tests are used to verify that those kind of foundational functions are operating properly, ensuring that the rest of the code can depend on those foundations with confidence. As an added bonus, more combinations of the various sequences to parse could be tested without inflating the number of scenario tests. Ground rules set? Check. On to the first scenario test. Starting With the First Scenario Test While it might not seem correct, starting with example number 189, the first test I did write was for GitHub Flavored Markdown example 189 , the first example included in the specification for the paragraph blocks. After solidly reading the specification, the general rule seemed to be that if it doesn't fit into any other category, it is a paragraph. If everything is going to be a paragraph until the other features are written, I felt that starting with the default case was the right choice. After a number of passes at cleaning up the test for this first case, it boiled down to the following Python code. \"\"\" https://github.github.com/gfm/#paragraphs \"\"\" from pymarkdown.tokenized_markdown import TokenizedMarkdown from .utils import assert_if_lists_different def test_paragraph_blocks_189 (): \"\"\" Test case 189: simple case of paragraphs \"\"\" # Arrange tokenizer = TokenizedMarkdown () source_markdown = \"\"\"aaa bbb\"\"\" expected_tokens = [ \"[para:]\" , \"[text:aaa:]\" , \"[end-para]\" , \"[BLANK:]\" , \"[para:]\" , \"[text:bbb:]\" , \"[end-para]\" , ] # Act actual_tokens = tokenizer . transform ( source_markdown ) # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) Breaking Down the Scenario Test It might be a lot to take in all at once, so let's break it down step by step. Start of the Module The start of the module needs to perform two important tasks: provide useful documentation to someone examining the tests and import any libraries needed. \"\"\" https://github.github.com/gfm/#paragraphs \"\"\" from pymarkdown.tokenized_markdown import TokenizedMarkdown from .utils import assert_if_lists_different The most useful and relevant information about the module that I was able to think of was the actual source for the test cases themselves. That being the case, I felt that including the URI to the specific section in the GitHub Flavored Markdown specification was the right choice for the module documentation. For anyone reading the tests, it provides a solid reference point that answers most of the questions about why the tests are there and whether or not the tests are relevant. Next are the import statements. The first one statement imports the TokenizedMarkdown class, a class that I set up to handle the parsing. Initially this class was a quick and simple skeleton class, especially for the first paragraph case. However, it provided the framework for me to support more use cases while maintaining a uniform interface. The second import statement is used to include a function that provides a good comparison of the contents of the list returned from the transform function of the TokenizedMarkdown class and a simple text list of the expected tokens. Arrange The Data For The Test From all of the useful pieces of information that I have learned about testing, the most useful bits about actually writing tests are the K.I.S.S. principle and the use of the Arrange-Act-Assert pattern. The K.I.S.S principle constantly reminds me to not overcomplicate things, reducing the tests to what is really relevant for that thing or task. The Arrange-Act-Assert pattern reminds me that when writing tests, each test I write breaks down into setup, action, and verification (with cleanup occasionally being added if needed). As such, I always start writing my tests by adding a comment for each of those sections, with the rest of the function blank. Once there, it's easy to remember which parts of the tests go where! def test_paragraph_blocks_189 (): \"\"\" Test case 189: simple case of paragraphs \"\"\" # Arrange tokenizer = TokenizedMarkdown () source_markdown = \"\"\"aaa bbb\"\"\" expected_tokens = [ \"[para:]\" , \"[text:aaa:]\" , \"[end-para]\" , \"[BLANK:]\" , \"[para:]\" , \"[text:bbb:]\" , \"[end-para]\" , ] The Arrange part of this test is simple, consisting mostly of easy-to-read assignments. The object to test needs to be setup in a way that it is completely enclosed within the test function. The tokenizer object with no options is assigned to the tokenizer , so a simple assignment takes care of it's setup. The source_markdown variable is setup within Python's triple-quotes to preserve newlines and provide an accurate look at the string being fed to the tokenizer. This string is copied verbatim from the example represented by the function, in this case example 189 . The final setup, the array assigned to the expected_tokens variable, takes a bit more work. When I wrote these, I sometimes wrote the expect tokens ahead of time, but more often than not used a known \"bad\" set of tokens and adjusted the tokens as I went. Act (Tokenize) and Assert (Verify Results) With all of the work on the setup of the tests, the Act and Assert parts of the test are very anticlimactic. # Act actual_tokens = tokenizer . transform ( source_markdown ) # Assert assert_if_lists_different ( expected_tokens , actual_tokens ) Using the information that was established in the Arrange section of the test, the Act section simply applies the input ( source_markdown ) to the object to test ( tokenizer ) and collects the output in actual_tokens . The Assert section then takes the output tokens and compares them against the expected list of tokens in expected_tokens . Why Not Use Pure Test Driven Development? In a normal project, I usually follow Test Driven Development practices quite diligently, either writing the tests first and code second, or writing both tests and code at the same time. As this was my first version of my first Markdown parser, I was aware that I was going to be adapting the tokens and token formats as I went, eventually arriving at a set of tokens that worked for all scenarios. Knowing that this churn was part of the development process for this project, I decided that a true Test Driven Development process would not be optimal. For this project, it was very useful to adjust the process. The balance that I struck with myself was to make sure that as I coded the parser to respond to a given scenario, I adjusted the tokens assigned to the expected_tokens variable based on the example's HTML output for the equivalent scenario test. This process gave me the confidence to know that as I made tests pass by enabling the code behind the scenario, each individual passing test was both moving towards a fully functioning parser and protecting the work that I had already done in that direction. To be clear, as I copied the template over, I adjusted the function name, the function's doc-string, and the Markdown source text based on the scenario test that I was implementing. The list of tokens in expected_tokens were then populated with a \"best guess\" before I started working on the code to make that scenario pass. In a microscopic sense, as I updated the test and the test tokens before starting on the code, I was still adhering to Test Driven Development on a scenario-by-scenario level. To me, this was a good balance to strike, evaluating the correct tokens as I went instead of trying to work out all 637 sets of tokens ahead of time. How Did This Help? Getting a good process to deal with the large bulk of scenario tests was a welcome relief. While I still needed to create a strategy to deal with that bulk of scenario tests I would need to write (see the next article for details on that), I had a solid template that was simple (see K.I.S.S. principle), easy to follow (see Arrange-Act-Assert pattern), and would scale. This was indeed something that I was able to work with. What About the Unit Tests? Compared to the scenario tests, writing unit tests for the parser's foundation functions was easy. In each case, there is a function to test with a very cleanly specified interface, providing for a clean definition of expected input and output. What Comes Next? In the next article , I look at the work that needs to be done and come up with general strategies that I use to implement the parser required for the linter. With the specification's 637 examples as a base for the scenario tests, good planning is needed to ensure the work can progress forward.","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/12/16/markdown-linter-setting-up-parser-tests/","loc":"https://jackdewinter.github.io/2019/12/16/markdown-linter-setting-up-parser-tests/"},{"title":"Markdown Linter - Collecting Requirements","text":"Introduction My website is now up and running, even though in my mind it took forever. To make sure everything was \"just so\", I went through each article with a fine-toothed comb multiple times, each with a slightly different thing I was looking for. In the end, it worked out, but I wished I could have automated at least some of that work and reduced the time it took to do it. And I also have a lingering question of whether or not I got everything, or did I miss something out? What Is a Linter? A long time ago, when I first heard the term \"lint\", I thought someone was referring to the stuff that you find in the clothes dryer trap that you need to clean out. According to Wikipedia , my guess was close. Similar to the \"undesirable bits of fiber and fluff found in sheep's wool\" from the Wikipedia article, software linters are used to detect undesirable practices and patterns in the objects they scan. Once pointed out, the development team can then decide whether or not to address the issue or ignore the issue. Doing My Research I started looking around, and even though there are a number of Markdown-to-HTML command line programs out there, finding a solid Markdown linter was another story. I did find a couple of starts at making one, but not a finished one that I could use. The only exception was the NPM-based Markdownlint by David Anson. This VSCode plugin is pretty much a standard for anyone creating content in Markdown using VSCode, with well over 1.3 million downloads as of the writing of this article. By default, as you save articles, this linter executes and produces a list of issues in the Problems section at the bottom of the VSCode editor. This tool is indeed handy while writing an article, but the act of verifying multiple articles becomes a bit of chore. My general process was to open a document I wanted to inspect, make a small whitespace changes, save the file, and examine the Problems section to see what the linter came up with. Two things were more annoying about this process that others. The first issue is that any issue for any file that is open is displayed in that section. If I wanted to be efficient, it meant closing every other file and just working on a single file at a time. The second issue is that other plugins write their problems there as well. As a lot of my content is technical, there are a fair number of spelling issues that arise that I need to ignore. Once again, neither one of these issues is a bad thing, just annoying. What Are The Requirements? Doing some thinking about this during the couple of weeks that I worked on the website, a fairly decent set of requirements crystalized: must be able to see an accurate tokenization of the markdown document before translating to HTML working with an accurate tokenization remedies any translation problems instead of translating from HTML all whitespace must be encoded in that token stream as-is for consistency, want an exact interpretation initial tokenization for GitHub Flavored Markdown only, add others later initial tests against the GitHub Flavored Markdown specs plans to later add other flavors of parser must be able to provide a consistent lexical scan of the Markdown document from the command line clean feedback on violations extending the base linting rules should require very little effort clear support for adding custom linting rules. written in Python good cross-platform support same language as Pelican, used as the Static Site Generator for my website While there are only 5 requirements, they are important. The first two requirements speak to reliability: the parsed Markdown tokens should be complete. The third requirement is for stability: write against one specification with a solid set of test cases before moving on to others. The fourth requirement is all about usability: the linter can be run from any appropriate command line. Finally, the fifth requirement is about extensibility: add any needed custom rules. From my point of view, these requirements help me visualize a project that will help me maintain my website by ensuring that any articles that I write conform to a simple set of rules. Those rules can be checked by a script before I commit them, without having to load up a text editor. Simple. Why Is This Important To Me? Writing this section, it took me a couple of tries to word this properly. In the end, I settled on a basic phrase: It is a tool that I can use to make a software project better. In other parts of my professional life, I take a look at things such as a Java project, and try and improve the quality of that project. The input is mainly Java source code and the output is mainly JAR files that are executed by a JVM. My website is no different. Remove Java source code and insert Markdown documents. Remove JAR files executed by a JVM and insert HTML files presented by a browser. There are a few differences between the two types of projects, but in all of the important ways, they are the same. I took the time to manually scan each article for my website multiple times before I did my website's soft release. To me, it just makes sense that there should be an easier way to perform that process. Easier in terms of time, and easier in terms of consistency. Unless I am missing something out there in the Internet, the only project that came close to fulfilling my requirements was Markdownlint , and it still had some things missing. I came to the realization that to be able to lint a Markdown file against a set of rules, I was going to have to write my own Markdown parser. In the last couple of decades of professional life, I have written many parsers, so that part of the project doesn't scare me. Due to the great works of the people at the GFM site , there we a solid number of test cases that I can test the parser against. The extensibility issue would make me look at different ways to integrate code into my tool, so a plus there. All in all, a decent number of things I have to get right, but nothing too far out of my field of experience. Sure it would be hard in places… but also a challenge! Just the kind of thing I like! What Comes Next? In the next article , I start breaking down the requirements for the Markdown parser and document how I will setup the tests for it. As I am parsing a well-known format with varying implementations already available, it is important to stay focused on one implementation and have a solid set of tests to ensure I don't go backwards in my development.","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/","loc":"https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"},{"title":"Website Soft Launch!","text":"Well, it took a lot of work, but the website is finally tuned up the way I want it to be! I still have a number of small things to do to get the website up and running at 100%. However, I am doing this soft-launch to start publishing articles on a regular basis instead of batching them up. In the crawl-walk-run model , this is me moving my website from the crawl stage to the walk stage. The website itself was mostly ready about 2-3 weeks ago, near the start of November. But as this is something public with my name attached to it, I wanted this soft-launch to be a good representation of what I can do and what I can say. As such, I went over the website with a keen eye for detail during the period. There were a number of days where I just went through each individual article, one at a time, looking for something specific. On one day, it was making sure the categories were good. On another, it was making sure the tags I had selected for each article made sense. On others, it was just scanning the articles and making sure they were reflecting my voice. As this is a learning experience for me, please visit the pages on the site and leave constructive comments about the content or the style! Remember that everyone has had help learning things in their live, and pay it forward!","tags":"Website","url":"https://jackdewinter.github.io/2019/12/01/website-soft-launch/","loc":"https://jackdewinter.github.io/2019/12/01/website-soft-launch/"},{"title":"The Inspiration For Jack's Digital Workbench","text":"Growing up in the late 70s and early 80s, the only computers that were available to me at the time were the Commodore line of computers locked up in schools. To feed my growing and inquiring mind, that left electronics and mechanics to experiment with, and expendable examples of both were hard to come by. My only choice was to scrounge around for things in various stages of disrepair, and upon getting one of those uncommon finds, take it downstairs to our unheated garage and trying to \"fix\" it. Far from a stellar place to work, it was not heated, not ventilated and rarely keep clean. And at the head of that garage, on the wall adjoining the basement, was a solitary, ugly workbench fastened to the wall. That workbench was often the birthplace of many far-fetched dreams of what I could do if only I could properly figure the thing laid out before me. I almost never had the right tools, so to be totally honest, more things got \"more broken\" than ever got fixed or reassembled into something else. Even with the right tools, while I believe my ability to make it work again would have increased, the drive and knowledge to do so was probably not there. But even with a veritable graveyard of things that were not completed, I chose not to dwell on the failures or make excuses for why I didn't finish them. Instead, every time I approached that bench, I chose to dream and move forward. Looking back, I fondly remember the times I stood at that bench with the single, bright work light in my eyes. Back then, it was always the journey and the creative imagination that was important. The only pressure to produce something concrete was when my mom asked \"So, what have you been working on?\" However, as I grew older, I realized that my mom's question was equal parts sincere interest on her part and her desire to have me grow past that graveyard of unfinished projects. I believe that she wanted me to be able to channel that creativity into the something tangible. Not so she could show off my work to others, but in a sincere effort to help me grow. Things are a lot different some 40 years later. Even though I longer use that old workbench, I try and capture a lot of that creativity and wonder on a daily basis, both in my professional life and home life. The projects I undertake are no longer far-fetched, but grounded in reality. As a result, I am better at selecting the projects that I want to work on and I am more successful in carrying them to completion. When a project fails, I embrace one of the Mythbuster's idioms, \"Failure is always an option\" and try and learn what failed and how to deal with it next time. While a small number of projects till end up in my project graveyard, but I keep the larger number of successful projects on proud display in my workplace and my house. So why start a blog and call it \"Jack's Digital Workbench\"? The simple answer is that I started blogging to help me keep a digital form of that garage workbench alive and renewed. I want to inspire people in the same way that my mother has done in the past and continues to do so today. I want to inspire people to dream big, following each dream up with a grounding that enables them to work on that project and gain confidence in their skills along the way. I want to inspire people to try and take something on, taking any failures along the way as teaching or learning moments and not as dream killers. Basically, the reason for Jack's Digital Workbench is to communicate with others and to inspire them in a positive manner to allow them to, in turn, inspire the people around them.","tags":"Personal","url":"https://jackdewinter.github.io/2019/11/15/the-inspiration-for-jacks-digital-workbench/","loc":"https://jackdewinter.github.io/2019/11/15/the-inspiration-for-jacks-digital-workbench/"},{"title":"Software Quality: Reliability","text":"In the main article titled What is Software Quality? , I took a high level look at what I believe are the 4 pillars of software quality. This article will focus specifically on the Reliability pillar, with suggestions on how to measure Reliability and how to write good requirements for this pillar. Introduction From the main article on What is Software Quality? , the essence of this pillar can be broken down into two questions: Does the software do the task that it is supposed to do? Does the software execute that task in a consistent manner? This article will take an in-depth look at common types of tests, discussing how those tests can help us gather the information necessary to answer those questions. At the end of this article, the section How To Measure Reliability will use that information to provide a cohesive answer to those questions. How Does Testing Help Measure Reliability? As discussed in the main article's section on Reliability , many articles on testing and reliability refer to a test pyramid that defines the 4 basic types of reliability tests: unit tests, functional/integration tests, scenario tests, and end-to-end tests. While those articles often have slightly different takes on what the pyramid represents, a general reading of most of those articles leaves me with the opinion that each test in each section of the pyramid must pass every time. With tests and reliability being closely related, it is easy for me to draw the conclusion that if tests must pass every time, then reliability is a binary choice: they all pass and the project is reliable, or one or more fail and the project is not reliable. As such, my main question is: Does it have to be a binary choice? Are the only two choices that either all tests did pass or all tests did not pass? If the answer to that question is a binary answer, then the answer is simple: it is either 100% reliable or 0% reliable. More likely, there are other answers that will give use a better understanding of how to measure reliability and how to interpret those measurements. Can We Identify Groups of Tests? Before determining whether or not reliability is a binary choice, I feel that it is important to make some foundational decisions on how to measure reliability based on the types of tests that are already identified. To aid in making those decisions, it helps to examine the four categories of tests, looking for groupings between them. Using the definitions established in the main article, unit tests are used to test the reliability of individual software components and functional tests are used to test the reliability of more than one of those components working together. Both of these categories are used to determine the reliability of the components themselves, and not their objectives. As such, they make for a good grouping as they have a common responsibility: technical reliability. Observing the scenario tests and end-to-end tests through a similar lens, those tests are used to determine whether or not the software project meets its business requirements. The end-to-end tests are often a set of tests that are very narrow and deep of purpose. At a slightly lower level, the scenario tests provide extra support to those end-to-end tests by breaking those \"bulky\" end-to-end tests into more discrete actions matched to the overall business use cases for the project. A good grouping for these tests is by what they: business reliability. Another way to think about it is to view the groups of tests in terms of whether or not they are inside or outside of the black box that is the software project. The first group of tests verify the inside of that black box, ensuring that all of the technical requirements or \"what needs to be done to meet expectations\" are met. The second group of tests verify the outside of that black box, ensuring that all of the business requirements or \"what is expected of the project\" are met. [Add picture of pyramid showing inside and outside?] Give Me an Example For the follow sections, I use the example of a simple project that uses a data store to keep track of contact information. By providing a simple example that most developers have encountered before, my hope is that it will make it easier for the reader to picture the different types of tests and how they will interact with their team's project. As I examine each type of tests, I try and explain my thinking on what I write and how I write it for that group of tests, hoping to guide others on making better decisions for their testing strategy. Note that I do not believe that the definition of \"data store\" is relevant to the example, therefore the definition of \"data store\" is left up to the reader's imagination and experience. End-To-End Tests Starting at the top of test pyramid, each end-to-end test needs to be a solid, representative test of the main focus of the project itself. These tests are usually a small set of tests meant as a solid litmus test on whether the software project is reliably meeting the requirements of the project. In forming the initial end-to-end tests, my intent is to start with a focus on positive cases which occur more than 60% of the time. For the example project, I started with a test to successfully add a new contact. As a nice bonus, starting with that test allowed me to add the remove, list, and update end-to-end tests, as they all need to add a new contact as a foundation of each of those 3 individual tests. Given my experience measuring quality, I believe that all of those tests together provide that check with confidence for the example project. If I had found out that the number of end-to-end tests I needed was more than a handful of tests, I would have then examined the requirements and try to determine if the project had too many responsibilities. Doing this exercise with a new project often helps me figure out if the project is properly scoped and designed, or if it requires further refinement. Having identified the end-to-end tests for a project and assuming that no further refinement is necessary, I rarely write source code for these tests right away. Most of the time I just add some simple documentation to the project outlined in pseudocode to capture that information. I find that the main benefit of doing this in the early stages is to provide a well-defined high level goal that myself and my team can work towards. Even having rough notes on what the test will eventually look like can help the team work towards that goal of a properly reliable project. Scenario Tests Still on the outside of the box, I then add a number of scenario tests to expand on the scope of each of end-to-end tests. For these tests, I focus on use cases that the user of the project will experience in typical scenarios. The intent here is to identify the scenario tests that collectively satisfy 90% or more of the projected business use cases for a given slice of the project. For the example project, adding a test to verify that I can successfully add a contact was the first scenario test that I added. I then added a scenario for the negative use case of adding a contact and being told there are invalid fields in my request and a third for a contact name that already existed. Together, these scenarios met my bar for the \"add a contact\" slice of the scenarios for the project. It is important to remember that these are tests that are facing the user and systems they interact with. Unless there is a very strong reason to, I try and avoid scenario tests that depend on any specific state of the project unless the test explicitly sets that state up. From my experience, such a dependency on external setup of state is very fragile and hard to maintain. It also raises the question on whether or not it is a realistic or valuable test if that setup is not something that the project itself sets up. Why only those 3 scenario tests? Here is a simple table on what types of scenario tests to add that I quickly put together for that project. The estimates are just that, examples, but helped me determine if I hit the 90% mark I was aiming for. Category Percentage Scenario Success 60% Add a contact successfully to the project. Bad/Invalid Data 25% Add an invalid contact name and validate that a ValidateError response is returned. Processing Error 10% Add an contact name for an already existing contact and validate that a ProcessingError response is returned. I sincerely believe that between those 3 scenario tests, I can easily defend that they represent 90%+ of the expected usage of the project for the specific task of adding a contact. While the percentages in the table are swags that seem to be \"plucked out of thing air\", I believe they can be reasonably defended 1 . This defense only needs to be reasonable enough to get the project going. Once the project is going, real data can be obtained by monitoring and more data-driven percentages can be used, if desired. How did I get there? From experience, there are typically 4 groups of action results, and therefore, scenarios: the action succeeded, the action failed due to bad data, the action failed due to a processing error, or the action failed due to a system error. The first scenario test represents the first category. Unless there was a good reason to show another successful \"add\" use case, I will typically stick with a single \"add\" test. As the goal is to achieve 90% of the typical use cases for the project, unless a variant of that success case is justified by it's potential contribution towards the 90% total, it can be better performed by other tests. In addition, tests on variations of the input data are better performed by unit tests and functional tests, where executing those tests have lower setup costs and lower execution costs. The second scenario test is used to satisfy the second group of tests where the data is found to be bad or invalid. In general, I use these to test that there is consistent error handling 2 on the boundary between the user and the project. At this level, I ideally need only one or two tests to verify that any reporting of bad or invalid data is being done consistently. By leaving the bulk of the invalid testing to unit testing and/or functional testing, I can simulate many error conditions and check them for consistent output at a low execution cost. To be clear, if possible I try and verify the general ability that consistent error handling is in place and not that a specific instance of error is being reported properly. The third scenario test is used to verify the third group of tests where data is valid but fails during processing. Similar to the second group of tests, there is an assumption that the reporting of processing errors should be done consistently. However, as most processing errors result due to a sequence of actions originating from the user, representative types of processing errors should be tested individually. The key to this type of scenario tests is to represent processing errors that will help the group of scenario tests hit that 90% mark. Relating this to the example project, getting a \"already add a record with that name\" response from the project is something that would occur with enough frequency to qualify in my books. From experience, the fourth group of tests, testing for system errors, rarely makes it to the level of a scenario test. In this example, unless a system error is so consistent that it was estimated to occur more than 10% of the time, a higher priority is placed on the other types of responses. One of the exceptions to these generic rules are when a business requirement exists to provide extra focus on a given portion of the interface. These requirements are often added to a project based on a past event, either in the project or in a related project. As the business owners have taken the time to add the business requirement due to its perceived priority, it should have a scenario test to verify that requirement is met. In the contact manager example, I made a big assumption that unless there were requirements that stated otherwise, the data store is local and easy to reach. If instead we are talking about a project where the data is being collected on a mobile device and relayed to a server, then a test in this last group of system errors would increase in value. The difference that this context introduces is that it is expected that project will fail to reach the data store on a frequent basis, and hence, providing a scenario for that happening helps us reach that 90% goal. Commonalities between End-to-end tests and scenario tests While I took the long way around describing end-to-end tests and scenario tests, I believe the journey was worth it. These two types of tests test against the external surface of the project, together painting a solid picture of what that external surface will look like once the project is done. For both of those tests, the project needs clear business requirements on what benefit it provides to the user, which will be highlighted by translating the requirements into the various tests. By including either actual data (for existing projects) or projected data (for new projects) on the usage patterns for that project, the requirements can be prioritized to ensure the most frequently used requirements are more fully tested. For each of those requirements and goals, the team can then set goals for the project based on those documented requirements. By codifying those goals and requirements with end-to-end and scenario tests, you firm up those goals into something concrete. Those actions allow the team to present a set of tests or test outlines to the authors of the requirements, validating that things are going in the right direction before writing too much source code or setting up of interfaces with the user. That communication and changing the course before writing code can save a team hours, days, or weeks, depending on any course changes discovered. What happens if the requirements change? The project has a set of tests that explicitly test against the outside of the box, and informs the team on what changes will be needed if that requirement change is applied to the project. At the very least, it starts a conversation with the author of the requirement about what the external surface of the project will look like before and after the change. With that conversation started, the team can have a good understanding of how things will change, with some level of confidence that the change is the change specified by the requirements author. Unit Tests and Functional Tests Transitioning to inside of the black box, unit tests and functional tests are more understood by developers and more frequently used than end-to-end tests or scenario tests. The unit tests isolate a single component (usually a class) and attempt to test that each interface of that component and is functioning properly. The functional tests do the same thing, but with a single group of components that work together as a single component rather than a single component itself. From an implementation point of view, the main difference is in how these tests are created. Unit tests, as they are testing a single component, should only contain a project reference to the one component being tested. If the components are created properly and have a good separation from the rest of the project, this should be achievable for a good number of components for the project, especially the support components. Therefore, the degree to which these tests are successful is determined by the amount of clean division of responsibilities the project has between it's components. Functional tests complete the rest of the inside-of-the-box testing by testing individual components with related components, in the way they are used in a production environment. With these tests, the degree to which these tests are successful is the ability to inject the project dependencies into one or more of the components being tested, coupled with the clean division of responsibilities needed for good unit tests. While using a concept such as the interface concept from Java and C# is not required, it does allow the injection of dependencies to be performed cleanly and with purpose. To enable groups of functional tests to be as independent of the components outside of their group as possible, mock objects are often used to replace concrete classes that are part of your project. If interfaces are used in your project to allow for better dependency injection , your functional tests can create mock objects that reside with your tests. This provides more control and reliability on what changes you are making from the live instance of the interfaces, for the sake of testing. If interfaces are not supplied for better dependency injection, a mocking library such as the Java Mockito are required to replace test dependencies with reliable objects. Back to our example Using the example project as a template, we know from the section on scenario tests that we need to test for valid inputs when adding a new contact. To add coverage for the component containing the \"add a contact\" logic as a unit test, it's success is determined by how much of the handling the external interface is in the one component. If that component contains all of the code needed to handle that external request in one method, it is extremely hard to test that component without bringing in the other components. That is definition of a functional test, not a unit test. As an alternative, if the validation of the input can be condensed into it's own component and removed from that method, that validation component can be unit tested very effectively. Applying that refactoring pattern a couple of more times in the right ways, the project's ability to be functionally tested increases. As an added bonus, depending on how the refactoring is accomplished, new unit tests can be added based on the refactoring, gaining measurable confidence on each additional component tested. Using the adding a contact example again, having refactored the input validation to a validation class could be followed by the following changes: create a new component for the handling of \"add a contact\" and decouple it from logic of the handling of the external interface move the user authentication and authorization logic into it's own component move the persisting of the new contact logic into it's own component From a functional test point of view, each of these refactorings makes it easier to test. For the first refactoring, instead of having to rely on all functional testing going through the external interface, which may include costly setup, we can create a local instance of the new component and test against that. If interfaces are used for the remaining two refactorings, then test objects can be used instead of the \"live\" objects, otherwise a mocking library can be used to replace those objects with more predictable objects. How is each group of Tests Measured? On this winding journey to determine how to measure reliability, I explored the relevant elements of the four main types of tests. I believe that I was successful in showing a clear delineation between the two groups of tests and the benefits each group provides. To recap, the outside-of-the-box group validates the expectations to be met, matched against the requirements set out for the project. The inside-of-the-box group validates how those exceptions are met, matched against the external interfaces for the project. These two distinct foundations are important, as the two distinct groups of tests require two distinct groups of measurements. The first group, scenario tests and end-to-end tests, are measured by scenario coverage. Scenario coverage measures the number of tests that successfully pass against the total number of scenario tests and end-to-end tests for that project. As this group of tests is measuring the business expectations of the project, this measurement is a simple fraction: the number of passing tests as the numerator and the number of defined tests as the denominator. The second group, unit tests and functional tests, are measured by source code coverage, or code coverage for short. Code coverage can be specified along 6 different axes: class, method, line, complexity, blocks, and lines. Different measurement tools will provide different subsets of those measurements, but in the end they are all relaying the same thing: the points in the project's source code that are not properly tested. Back to the original question Does it (the measuring of reliability) have to be a binary choice? It depends. In an ideal world, the answer to that question is yes, but we do not live in an ideal world. In the real world, we have a decision to make for either group of tests on what is good enough for the project and that group of tests. If the suggestions of this article are followed, then a condition of releasing the project to a production state is 100% scenario coverage. Anything less than 100% means that critical use cases for the project are not complete, hence the project itself is not complete. To achieve the 100% coverage without adding new project code, updated requirements are needed from the requirements author, say a project manager, to change the composition of the scenario tests and end-to-end tests. This may include removing some of these tests as the release goals for the project are changed. While changing and removing goals and their tests, may seem like cheating to some people, the other option is very risky. It should be evident that if a project is released without all scenario tests and end-to-end tests passing, that team is taking a gamble with their reputation and the reputation of the project. It is better to adjust the tests and goals, and communicate those changes, than to take a risk on releasing something before it meets those goals. Following the suggestions of this article for code coverage is a more nuanced goal, and really does depend on the project and the situation. If architected and designed to support proper testing from the beginning, I would argue that 95%+ code coverage is easy and desirable. If you are adding testing to an already existing project or do not have the full support of the developers on the project, this number is going to be lower. Another factor is the type of project that is being tested and who will use it. If you are creating this project to support people inside of your company, it is possible that one of the requirements is to have a lower initial code coverage target to allow the project to be used right away and alleviate some internal company pressure. If the project is something that will represent you and your company on the international stage, you will have to balance the time and effort needed to meet a higher bar for code coverage with the need to get the project out where it can be used. As with many things, it is a matter of negotiation and balance between the various requirements. What Is Really Important I want to stress that I believe that the important thing is that each project measures where they are against whatever goals they set for their project. The team doesn't need to always maintain a near-100% code coverage measure, but that team needs to know where they stand. This will influence and inform the people that author the requirements and adjust the priorities for the team. Any negotiations within the team can then cite this information and use it to help with the balancing act of adding new features, fixing existing bugs, and enhancing code quality (in this case, increasing code coverage). How To Measure Reliability To answer the question \"Does the software do the task that it is supposed to do?\", scenario coverage is measured. Scenario coverage for end-to-end tests and scenario tests should always be at 100% when a production release of the project is performed. This measurement is binary. Until that release (or next production release) is performed, adding or changing these tests based on the requirements for the next release will inform the team and any stakeholders of how close the team is to satisfying those requirements for that release. To answer the question \"Does the software execute that task in a consistent manner?\", code coverage is measured. Code coverage for unit tests and functional tests should strive for 95% code coverage along all 6 axes with all active tests completing successfully 100% of the time. The test completion percentage must be non-negotiable, but the code coverage percentage must take into account the maturity of the project and the usage of the project. This measurement is non-binary. However, it is important to know your project's code coverage measurement, and how it trends over time. While the measurement is non-binary, it is suggested to create a binary rule that specifies what the minimum percentage is for each axis, failing the rule if that specific metric falls below the goal percentage. Wrapping It Up By breaking down the types of tests that are expected for a given project, the two different types of measurements of reliably become more evident. Scenario coverage is determined by outlining the major scenarios for using a project and writing end-to-end tests and scenario tests against them. Scenario coverage must be a binary measurement at release time. Code coverage is determined by using tools to measure which parts of the code are executed when running functional tests and unit tests. Code coverage is a non-binary metric that must have a minimum bar for coverage that is met for the project, and determined on the merits of the project itself. By using these two measurements, I hope that I have shown that it is possible to provide a way to empirically measure reliability. By having a project be transparent about how it is reaching those measurements and what they are, any team can provide meaningful and understandable measurements of reliability. If asked, I could easily defend the percentages. For the success case, I would assume that half the 60% number will come from first try successes and half the number will come from success that occurred after people fixed errors returned from the other two tests and resubmitted the data. While the other two categories are somewhat guesswork, from my experience validation errors are 2-3 times more common than an \"existing contact\" processing error. Note that in the absence of real data, these are estimates that do not have to be perfect, just reasonable. ↩ In designing any type of project, you should seek to have clear and consistent interfaces between your project and the users of the project. An extension of that statement is that any responses you return to your user should be grouped with related responses and returned in a common data structure or UI element to avoid confusion. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/11/10/software-quality-reliability/","loc":"https://jackdewinter.github.io/2019/11/10/software-quality-reliability/"},{"title":"Fine Tuning Pelican: Getting Ready For a Soft-Launch","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction Through the previous 11 articles, I have detailed the various fine tuning that I have done on my website as I get it ready to publish. Having completed and tested most of the preparations to get the website ready for a soft-launch, it was time to think long and hard about what categories and tags to use, and the rules going forward for both. For me, having those two concepts right is going to help me shape the voice of my website and really dial in the content I want to deliver. This article details the steps I took to take those final steps towards publishing. Write Some Starter Articles To make sure that I was not presenting a blank website to any prospective readers, I made sure to write a number of articles to have preloaded for the website's soft launch. I am not sure if others would make the same decision, but my reasoning was this: how can I invite readers to my website if there is nothing to read? I am aware that my writing skill is passable and I am frequently reminded that I am somewhat stubborn. Between these two concepts, it took me a long while to find my voice and come up with some ideas on what I wanted to write. For each of the articles published on this website before this date, there were probably 2-3 times as many articles that \"died\" before they were finished, and another 2-3 times that died before the first sentence was written. At first I thought this was a bit of a failure, but then I remembered some words from a talk on photography that encouraged people to take 100 pictures, just to find the 1 or 2 pictures that were good. The speaker went on to mention that as he gained more experience, he still took 100 pictures, but the number of good pictures increased to the point where his \"good picture percentage\" is now about 20%… and he is happy with that. He went on to say that the most important things he learned were to take better pictures and to not waste time on the picture that were not going to turn out. How Does That Relate To Article Writing? Relating that back to my article writing, I believe that I am getting better in writing the articles for three main reasons. And when I say three main reasons, I am not really talking about first, second, and third place – I am talking about three reasons that I feel are almost equal in importance. The first reason is that like the photographer, I needed experience to grow as a writer and gain confidence with my writing. If I waited until after the soft-launch of the website, then all of my learning through articles would be laid out for everyone to see. From my experience, you only get one chance to make a good first impression… after that, the good impression have less and less impact. By gaining that experience before the launch, it enabled me to make all of the failures I wanted to without worrying about any first impressions being lost. The second reason is that I noticed that the more passionate I was about the subject matter, the more passionate I was about writing the article. By reducing the scope of categories and subjects to the ones that I was most passionate about, my drive to write a good article, start to finish, was more intense. While that passion also caused me to take more time to write the articles (and fuss over them like crazy), when each article was done, I sincerely feel like I have done my best. No regrets, no what-ifs, but a strong feeling that I did my best in writing the article the way I did. Properly Scoping My Categories Stemming in part from the second reason above, the third reason that I think I got better with the articles was that I narrowed the scope of the article topics from \"anything I want to say\" to a small number of categories. By focusing on a smaller and more focused number of categories, I can afford to be more passionate about them. I know that these categories may change over time, but I am going to keep it to 5 categories or less. This part was not as easy as I thought, but also not as difficult. As I wrote the articles, I started noticing trends on which articles were making it further along and kept notes and ideas for new articles. If the category was a good one, I was picking up speed in writing the articles. Another good indicator was how easy it was to come up with new articles for that category, along with a 3-4 sentence \"sketch\" on what the article should be about. It did take me a bit of time to recognize it, but in retrospect, it was obvious what the categories should have been from the start. Determining Good Rules for Tags The last thing I needed to figure out before a soft-launch was the tags. Unlike the categories where I have a number of preset categories to create articles in, the tags are present to allow for better identification of what topics or concepts are in an article. This in turn allows a reader to click on one of the tags displayed for an article and see what other articles are listed there. Whereas for my website the categories are more or less fixed, the first rule I came up with is that the tags should be fluid and truly represent the article. If I want readers to be able to find similar articles, I want to build trust that if I say an article is tagged with a given word or phrase, it will directly reference that tag. On my travels through the internet, there is nothing more disappointing that following a link about something you are researching, only to find that that the data you were promised is only 1 sentence in an article. I don't want that for my website. The second rule that I wrote for myself might seem stupid or silly, but a tag should not be the same or similar to an existing category. Before going through my articles for my soft-launch, there were a number of times where that duplication existed. Now, often that was due to me not having the first rule in place when authoring those articles, but it still happened. This should be an easy one to follow. Finally, the last rule that I came up in preparation for the soft-launch was that tags will be descriptive without being too verbose. When I picture this rule in my mind, what I usually think of is 2 words, where one is an adjective and the other is a noun. While this can sometimes get to 3 words, I don't want it to extend past there as that is descending quickly into the verbose category. As an example, \"website\" is a bad tag as it does not describe what kind of website I am talking about. On the other side, \"how to write a better website\" is too verbose, practically being it's own sentence. In between is \"writing better websites\" which seems to strike a good balance between the two for me. What Was Accomplished Using the other articles in this series as a jumping off point, this article focuses on getting the categories and tags right across the articles. I talked briefly about my philosophy about preloading the site with articles, both to gain experience in writing articles and to determine which articles were the right articles for me to write about. As a natural result of that work, I was able to determine a good initial set of categories for my website, encompassing the topics that I am most passionate about, hopefully ensuring that my best work will be put forward. Finally, I came up with a number of initial rules about tags to help readers of my website find related content on my website with a minimum of effort.","tags":"Website","url":"https://jackdewinter.github.io/2019/11/03/fine-tuning-pelican-getting-ready-for-a-soft-launch/","loc":"https://jackdewinter.github.io/2019/11/03/fine-tuning-pelican-getting-ready-for-a-soft-launch/"},{"title":"Fine Tuning Pelican: Enabling Website Crawling","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction Many forms of website crawling are fraught with copyright issues and may be considered unethical, as is discussed in this article on Data Crawling and Ethics . In contrast, there are legal and ethical uses for web crawling, such as providing the data to search engines such as Google and Bing . While Search Engine Registration and Optimization is covered in another article, it is worthwhile to ensure that the website is properly set up to regulate any web crawling that does occur. This article details the setup required to enable this regulation. Why Use A Robots.Txt File? The Robots Exclusion Protocol has been around for almost as long as webservers. As described in the protocol, it manifests itself as a specially formatted robots.txt file located in the base directory of the webserver. While this protocol is not enforceable and remains a suggestion for web crawlers, it does provide for the rules that you have for \"proper\" crawlers accessing your website. For my website, this file exists in the content directory at /extras/robots.txt and has the following configuration: User-agent: * Disallow: Sitemap: https://jackdewinter.github.io/sitemap.xml This instructs any web crawler that is behaving properly of 3 important facts about the website. The first instruction is that the website is okay with crawlers representing any user agents are allowed to access the site. The second instruction is that there are no paths in the webserver that web crawlers are not allowed to access. Finally, the third instruction provides the web crawler with the location of the website's sitemap, detailing the location of each page on the website. These pieces of information are important for different reasons. The first two pieces of information are meant to restrict web crawlers from accessing the site, if so informed. In the case of this configuration, the * value for the user-agent field means that all user agents are allowed, and the empty value for the disallow field means that no parts of the website are disallowed. Between these two instructions, a web crawler can correctly determine that it is allowed to access any webpage on the website, appearing as any type of web browser or web crawler. How To Publish The Robots.txt File Publishing the robots.txt file requires two separate bits of configuration to ensure it is done properly. The first bit of configuration modifies the existing STATIC_PATHS value to add the path extra/robots.txt to the list of directories and files to publish without modification. The second bit of configuration specifies that the file at the path extra/robots.txt , when published without any modifications, will be located at the path /robots.txt at the webserver's root. STATIC_PATHS = [ 'extra/robots.txt' ] EXTRA_PATH_METADATA = { 'extra/robots.txt' : { 'path' : '/robots.txt' } } Publishing the Sitemap Generating a sitemap for Pelican is accomplished by adding the sitemap plugin to the PLUGINS configuration variable as follows: PLUGINS = [ 'sitemap' ] As detailed in the sitemap plugin documentation , while there are defaults for the sitemap, it is always better to specify actual values for each specific website. The values used for my website are as follows: SITEMAP = { 'format' : 'xml' , 'priorities' : { 'articles' : 0.6 , 'indexes' : 0.5 , 'pages' : 0.4 }, 'changefreqs' : { 'articles' : 'weekly' , 'indexes' : 'weekly' , 'pages' : 'monthly' } } In short, the configuration specifies that the format is xml , producing a /sitemap.xml file. The priorities of scanning are articles, then indexes, then pages, with change frequencies roughly backing up the priorities. For my website, the thought behind the values is that articles, and the indices they are part of, will be updated on a weekly frequency while pages will vary rarely changed. What Was Accomplished The purpose of this article was to detail the configuration for my website that supports crawling of the site for information. The first part of this configuration enabled the creation of a robots.txt file and publishing that file as part of the website. The second part of the configuration added the sitemap plugin and tailored the sitemap configuration for the specific balances for my website. Together, this configuration makes me feel confident that the website is well configured for web crawlers, specifically search engines.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/30/fine-tuning-pelican-enabling-website-crawling/","loc":"https://jackdewinter.github.io/2019/10/30/fine-tuning-pelican-enabling-website-crawling/"},{"title":"Fine Tuning Pelican: Connecting with Readers","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction One of the most important things for a website to focus on is to connect to the people that are viewing the website. That focus is usually geared towards the readers that are attracted to the website's content and retaining that focus. To retain this connection, a website needs to employ a number of tools to engage the readers and maintain their attention. Without employing these tools, any readers of the site will be transient and unlikely to return. This article discusses the tools that my website uses to engage and attempt to retain readers. Social Media As social media is so pervasive in today's society, it is pivotal to add links to your social media accounts, as detailed in the Elegant article on displaying social media profiles . For my website, that configuration boiled down to the following configuration: SOCIAL_PROFILE_LABEL = u 'Stay in Touch' SOCIAL = ( ( 'GitHub' , 'https://github.com/jackdewinter' , 'github-alt' ), ( 'linkedin' , 'https://www.linkedin.com/in/jackdewinter/' , \"linkedin\" ), ( 'RSS' , 'https://jackdewinter.github.io/feeds/all.atom.xml' ), ) Together, these configuration settings provide for the Stay in Touch section at the bottom of the right sidebar, containing icons that will take you to social media associated with the website. The one outlier to this is the RSS icon that appears at the end of the other icons. While technically not a form of social media, by following the Elegant steps outlined here , this useful icon is added and placed in a location that is easily accessible for any readers to use in their automation. Allow Readers To Comment To establish a sense of community and utility with the website, it is important to allow readers to make comments on your articles. While some of those comments may not always seem nice, it is important to try and engage with each reader's comment, and figure out a positive manner in which to respond to it. While Pelican does not support any commenting system itself, the Elegant theme has supported Disqus for a long time, and recently added support for Utterances . Going through the obvious features of each of these platforms, I quickly constructed the following table: Disqus Utterances paid tiers free (backed by GitHub) register with Disqus register with GitHub savvy readers may already have GitHub account Based on that information, choosing Utterances was an easy choice for me. Registering with Utterances is also easy, with Elegant having taken care of a number of the steps for us. Enabling Utterances For The Repository Going to the Utterances website , my first pass at trying to figure out how to register was an utter bust, and I confess that I was somewhat confused. Taking a step back, I was able to figure out the following flow: made sure I have a GitHub account went to the Utterance application page clicked on the Configure button at the top right of the page picked the User or Group containing the project where the target repository for the comments is my repository for the website is jackdewinter\\jackdewinter.github.io , so I selected jackdewinter verified my access to setup the account by providing the password this verification may not occur if you have verified your access within the last 10-15 minutes selected the repository jackdewinter.github.io from the drop-down list pressed the Save button to save these changes While I was able to get this flow right on the first try, it did take me an additional time or two to get the flow documented correctly. Other than that, it was really easy to setup, with no misleading steps along the way. Enabling Utterances In Elegant Once I had the GitHub portion of Utterances set up, it was time to setup Elegant to make use of that setup. Following Elegant's article on Comments - Enabling Utterances , I quickly came up with the following configuration changes: UTTERANCES_REPO = \"jackdewinter/jackdewinter.github.io\" UTTERANCES_LABEL = \"Comments\" UTTERANCES_FILTER = False UTTERANCES_THEME = \"github-light\" In order of appearance in the above code block, the comments will appear as issues in the GitHub repository jackdewinter/jackdewinter.github.io tagged with the label Comments . At the bottom of each article, the Comments section is not be filtered out by default, shown to the reader using the github-light theme. With these configuration settings in place, a quick publishing and hosting of the website showed that comments were now enabled for articles! Testing Utterances With the configuration in place, I performed a couple of quick tests of the newly installed comment system. Picking an article at random, I scrolled down to the bottom where a Comments section awaited me. Signing in through GitHub, I was then able to leave a comment that was persisted after a couple of page refreshes. Opening a new tab in the same browser, I navigated over to my jackdewinter.github.io repository and clicked on the Issues button to open the Issues page for the repository. There under the Issues section was the URL of article that I selected with a Comments label on it. Opening up the issue, I was greeted with the test comments that I had entered previously. Tuning Utterances After reading the other articles in that section of the Elegant documentation, two other articles leapt out as being useful. During testing, I had noticed that there weren't any text or images that separated the article from the comments, and I wanted to change that. The article on Comments - Invite Visitors To Comment provided a nice way to do this, so I followed their suggestions and made the following change to the configuration: COMMENTS_INTRO = \"So what do you think? Did I miss something? Is any part unclear? Leave your comments below.\" Another thing that I noticed during testing was that the comments were filed under an issue with the URL of the article. I want to be able to make small changes to the article title if need be, therefore basing the issue on the article's URL is less than optimal. Luckily, the authors of Elegant thought about this problems and have an article on Comments - Thread Id that deals with it. Without any changes, each issue will be attributed to the URL of the article. In the case of this article, that URL ends with: /2019/10/01/fine-tuning-pelican-connecting-with-readers/ By adding the following line to this article's metadata: comment_id : fine-tuning-pelican--connecting-with-readers the issue that is created for the article is instead fine-tuning-pelican--connecting-with-readers . What Was Accomplished At the start of this article, I stressed that a big part about a successful website is the ability to engage readers and maintain their attention. The first tool is to use the social media section of the sidebar that is available with Elegant. The second tool is to have an active comment system for each article that is easy to use. With two small fine-tunings to the Elegant configuration, both of these tools were configured for the website, and working great! While this was one of the aspects of running a website that I was worried about, having great documentation from Elegant ( Comments - Invite Visitors To Comment ) and Utterances made this a snap to setup.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/27/fine-tuning-pelican-connecting-with-readers/","loc":"https://jackdewinter.github.io/2019/10/27/fine-tuning-pelican-connecting-with-readers/"},{"title":"Fine Tuning Pelican: Producing RSS Feeds","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction While it may seem counter-intuitive, the mostly text RSS Feeds have experienced a comeback in the last 5-10 years. Due in large part to a wave of automation, these feeds are mostly set up for computers to read and process new and existing articles, rather than human readers. As such, I felt it was important to provide RSS feeds for my website. This article describes how to set up RSS feeds for a Pelican website. Strictly Speaking… For any purists reading this, note that the output that Pelican provides is actually in the Atom format not pure RSS. This is not a major issue as most RSS readers will accept RSS format or Atom format for their readers. A good article on the differences between the two is presented here . Only Generate Feeds When Publishing During the normal course of writing content, I will often do a simple publish against the pelicanconf.py configuration to see how that content looks when rendered. In my pelicanconf.py configuration, the following lines are present: # Feed generation is usually not desired when developing FEED_ALL_ATOM = None CATEGORY_FEED_ATOM = None TRANSLATION_FEED_ATOM = None AUTHOR_FEED_ATOM = None AUTHOR_FEED_RSS = None Unless you are actively debugging something to do with RSS feeds, there is no need to generate these feeds during development. From a look and feel point of view, each RSS feed contains the same text as the normally viewed article, with all of the theme styling and extras removed. As such, there is usually no benefit to generating the RSS feed until the final publish step. That is why the publishconf.py configuration includes configuration to override the pelicanconf.py which enables RSS feed generation. Generating the Right Types of RSS Feeds To provide the right types of RSS feeds for my website, I provided the following configuration in the publishconf.py files: # Feed Items FEED_MAX_ITEMS = 15 FEED_ALL_ATOM = 'feeds/all.atom.xml' CATEGORY_FEED_ATOM = 'feeds/{slug}.atom.xml' The first line of configuration specifies that none of the feeds should not contain more than 15 items. Without this setting, a website with 200 articles would have all 200 of those articles included in the feed each time the feed was generated. In addition, when each feed was downloaded, it would download all 200 articles. For me, this setting presents a good balance between presenting a decent amount of content and sending too much data. It is very unlikely that I will publish more than 15 articles at a time, so it just seems right. The next two lines of configuration enable the \"all\" feed and the \"category\" feeds. The FEED_ALL_ATOM configuration enables the all.atom.xml feed to be established at the location feeds/all.atom.xml . This feed contains every article is published, in reverse order of publication. The CATEGORY_FEED_ATOM configuration enables the individual category feeds, one for each category that exists. Each on of those feeds is located at feeds/{slug}.atom.xml where {slug} is the category for which the feed is being generated. Based on the above configuration publishconf.py , when this article was written, the feeds produced were: /feeds/all.atom.xml /feeds/github.atom.xml /feeds/markdown.atom.xml /feeds/quality.atom.xml /feeds/technology.atom.xml What Was Accomplished I started with a quick description of why an older protocol such as RSS and Atom are still good things to have in today's website world. I then covered why to not generate RSS feeds until publish time, followed by how to setup and configure the RSS feeds when it was publish time. This effort allowed me to add RSS feeds to my website in a pretty painless manner, and should allow a reader to perform that same task.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/23/fine-tuning-pelican-producing-rss-feeds/","loc":"https://jackdewinter.github.io/2019/10/23/fine-tuning-pelican-producing-rss-feeds/"},{"title":"Fine Tuning Pelican: Custom Error Pages","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction When a webpage is requested that does not exist, it is common for websites to react to the request by displaying a custom error page. While Pelican does not natively ship with this feature, Elegant adds a themed page that does. This article details the changes needed to mark Elegant's custom error page is properly used as \"the\" error page. Configuring Elegant and GitHub Pages Following the instructions from the Elegant Documentation , Elegant can be instructed to generate a properly themed 404 page by adding the 404 string to the DIRECT_TEMPLATES variable in the pelicanconf.py file. After following those instructions, the next time the website is generated, a 404.html file will be generated in the base directory. This file will have all of the trappings of the Elegant theme, and it will display an error page that includes a box to search for what the reader was looking for. The website hosting service that is being used will dictate if there are any extra steps needed to enable the custom 404 page. For GitHub Pages, as long as the file is named 404.html and is in the root directory that is being hosted, GitHub will automatically pick it up and use it as the 404 error page. Note that it seems like the file must exist on the master branch of the GitHub Pages directory in order for that action to take effect. Using Other Themes If you are using a theme other than Elegant, you can replicate some of the behavior that Elegant provides out of the box. In particular, you can define a page called markdown.md somewhere in your contents, and add the following text to it: --- Title : My Custom Page permalink : / 404 . html --- This is my custom error page . The key to this being used as an error page is the permalink: /404.html part of the markdown header. This informs Pelican to always publish the page with output file of /404.html , placing it in the root directory where it will be picked up properly by many site publishers, such as GitHub Pages. What Was Accomplished In this article, I provided some quick information on how to set up a custom 404 page using Elegant, and noted how it will be picked up by GitHub Pages. I also provided some basic information on how to set up a custom page for themes other then Elegant. By using this approach, I was able to have a custom error page that had the theme of my website, allowing the reader to recover in case of a bad URL.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/20/fine-tuning-pelican-custom-error-pages/","loc":"https://jackdewinter.github.io/2019/10/20/fine-tuning-pelican-custom-error-pages/"},{"title":"Fine Tuning Pelican: Markdown Configuration","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction When choosing authoring tools for a website, a primary concern is that the tools are either already used by the authors or easy enough to learn by the authors that it does not allow them to write freely. For me, I regularly author documentation in Markdown using Visual Studio Code for my professional job, so using the same tools for my website was an easy choice. However, unlike the documents I write at work, the Pelican generator utilizes the Python-Markdown generator library which has a few more options than standard Markdown. This article details the Markdown configuration that I have enabled for my website, and why I have enabled the specified configuration. Markdown Configuration The following configuration is the Markdown configuration for my website: MARKDOWN = { 'extension_configs' : { 'markdown.extensions.extra' : {}, 'markdown.extensions.admonition' : {}, 'markdown.extensions.codehilite' : { 'css_class' : 'highlight' }, 'markdown.extensions.meta' : {}, 'smarty' : { 'smart_angled_quotes' : 'true' }, 'markdown.extensions.toc' : { 'permalink' : 'true' , }, } } markdown.extensions.meta This configuration is perhaps the most important extension that I use. This setting enables the Python-Markdown - Metadata feature which enables the processing of the header of Markdown files with metadata about the Markdown that is being authored. While the Python-Markdown processor does not use this metadata itself, the metadata is passed to Pelican and Elegant to allow for configuration of the articles and pages on a one-by-one basis. This choice is actually a requirement for Pelican to work, being provided as one of the defaults for the MARKDOWN configuration element, as documented here . markdown.extensions.extra This configuration enables the Python-Markdown - Extra features which includes support for: abbreviations, attribute lists, definition lists, fenced code blocks, footnotes, and tables. This choice is actually a requirement for Pelican to work, being provided as one of the defaults for the MARKDOWN configuration element, as documented here . markdown.extensions.codehilite This configuration enables the Python-Markdown - Code Hilites feature to provide for special displaying of marked text within given areas of the document. These sections or specially marked text are typically used to display text that represents code for programs or scripts, with more stringent rules on how to display the text. If no specific text format is specified with the text block, such as many of the code blocks in the article on Static Websites: Posting My First Article . If a text format is specified, this feature will try it's best to match it to known highlighters, using color to indicate different parts of the specified text format. This can be seen in a later section of the above article where a code block is used for a sample Markdown article and later in the series where Python configuration is referenced. In both of these examples, the highlighting done to the text is able to be changed according to the type of text being highlighted. This choice is actually a requirement for Pelican to work, being provided as one of the defaults for the MARKDOWN configuration element, as documented here . markdown.extensions.toc This configuration is present as part of the setup for Elegant's Table Of Contents support. This specific value instructs the Python-Markdown - Table of Contents feature to generate permanent links at the end of each header. These links provide the destination URLs that Elegant's Table of Content support use to go directly to a given item in the Table of Contents. I subscribe to Elegant's philosophy on providing a clean reading experience with minimal distractions. By moving the table of contents to the left sidebar and out of the article, I believe the reader can focus more on the article. markdown.extensions.admonition This configuration enables the Python-Markdown - Admonition feature to provide a highlighted box around the indicated text content. These highlighted boxes are themed by Elegant to provide for a good, quick communication of important information to the reader without being too disruptive. An example of admonitions is available in this article on Glanceable Displays . I find that using admonitions in articles allows me to include more useful and descriptive information to the reader. The options of sections of tests in various parentheses and braces, or placing the text in footnotes, doesn't feel right to me, while admonitions, with their colored call outs do. This is a personal preference. smarty This configuration enables the Python-Markdown - SmartyPants feature to provide for more clear representation of various characters and character sequences used in articles and pages. With this feature enabled, the following substitutions are made: the apostrophe character ( ' ) is translated into a left and right single quotes around the words or phrases they surround: ‘this is my phrase' the quotation character ( ' ) is translated into a left and right double quotes around the words or phrases they surround: \"this is my phrase\" double greater than signs ( >> ) and less than signs ( << ) are translated into angled quotes: « and » three period characters in a row ( ... ) are translated into ellipses: and so they said… two consecutive dashes ( -- ) and three consecutive dashes ( --- ) are turned into lengthened dash characters: – and — While I am not 100% sold on this one, I like the effects it has, even though they are small. It just seems to add a bit of a finished touch to the articles. What Was Accomplished This article was created to share the Markdown configuration that I use for my website For each feature that I use I specify what benefit it provides to the articles, along with the reasons that have for using that Markdown feature.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/16/fine-tuning-pelican-markdown-configuration/","loc":"https://jackdewinter.github.io/2019/10/16/fine-tuning-pelican-markdown-configuration/"},{"title":"Fine Tuning Pelican: Pelican Plugins","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction In setting up the website to reflect my decisions on how things should be displayed, I surveyed a number of plugins to use. Some of these plugins were chosen as they are supported by the Elegant theme, and some by Pelican itself. This article details the list of the plugins that I use on my own website and why I decided to use them. Plugins I Use Pelican plugins help shape the look and feel of a website, but they can also get in the way of the content creation. For me, it is important to experiment with plugins to determine whether or not the plugin and the services it presents enhances either the content creation or content display in a manner that I will find positive for my website. If it doesn't meet that metric with an acceptable cost for meeting that metric, then the plugin gets discarded quickly. For the initial setup of my website, I just went for the standard plugins that work well with Elegant, keeping it simple for now. Search The tipue_search plugin provides for a simple yet effect method to catalog the text on the website and to provide for manner in which to search that catalog. Elegant exposes this capability using a search box in the top right corner of each webpage. Instructions on how to configure this plugin are located here . Simply, a website without search would look vary basic, and I wanted to show a finished product. For me, Elegant makes it look nice, while being easy to use. Navigable Table Of Contents Between the extract_toc plugin and some Markdown configuration, Pelican can take a single Markdown tag and generate an accurate table of contents for the article in it's place. Elegant can then take that information out of the main body of the article and display it on the left sidebar in a manner that does not disrupt the reading of the article. Instructions on how to configure this plugin are located here . I subscribe to Elegant's philosophy on providing a clean reading experience with minimal distractions. By moving the table of contents to the left sidebar and out of the article, I believe the reader can focus more on the article. Series Indicators The series plugin provides extra information to Pelican's template engine, to allow themes to show that an article is part of a series. Elegant takes that information and displays the title of the series on the right sidebar, followed by an ordered list of links to each article in the series. Instructions on how to configure this plugin are located here . In a similar vein to how I feel about Elegant displaying the table of contents, having a series navigation on the right sidebar allows for a ready to have a clean reading experience while allowing the reader the freedom to navigate within a series of articles. Previous And Next Article Links The neighbors plugin provides the necessary information to determine the neighbors of the current article, with respect to the time it was written. Elegant displays this information as the links are the bottom of the article, to assist in navigation. Instructions on how to configure this plugin are located here . Quite simply, having the previous and next article links allow a reader to navigate backward or forward through articles, without getting in the way of normal reading of the article. Sharing Article Links The share_post plugin provides information that Elegant than uses to display simple text links at the end of articles. These links allow the reader to share this on their Twitter accounts, through Facebook, or via there email accounts. The main benefit of these links are that they allow the reader to share these posts, hopefully attracting more readers to the website without being tracked. Many of the other \"share\" buttons on other blogs are implement tracking on each link from one website to another, a practice that doesn't sit well with every reader. Instructions on how to configure this plugin are located here . The benefit here is easy for me to quantify. If it is simple and safe to share articles with their friends, they will naturally share the articles. If readers share articles, I will get more readers. Improved Asset Downloads The assets plugin provides for a way for Pelican to take supporting files for the website, such as CSS files and Javascript files, and combine them into a smaller number of files. By performing this ‘minification', the number of downloads for each page or article is reduced, and therefore the pages and articles load time is smaller. Instructions on how to configure this plugin are located here . The benefit here is also easy for me to quantify. From a reader's point of view, I don't want a page taking forever to download… the quicker the better. From a provider's point of view, fewer requests containing fewer bytes means less load on the infrastructure. Reading Time Estimate The post_stats plugin calculates an estimate of the time required to read the article. Elegant displays this estimate at the top of the right sidebar. Instructions on how to configure this plugin are located here . The benefit of this plugin is harder for me to justify, but it falls into my concept of look-and-feel. This plugin provides a good estimate as to how long a reader can expect to take in reading the article, and hence budget enough time for them to read the article without feeling hurried. SiteMap As the sitemap plugin requires a significant amount of configuration, it is detailed along with the configuration for the robots.txt file in the article on Fine Tuning Pelican: Enabling Website Crawling . What Was Accomplished This article walked through the plugins that I currently use for my website, what they are for, and how to install them. To add extra context to each plugin, I also detailed some of the reasons why I selected to use a given plugin, with it's benefits to me.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/13/fine-tuning-pelican-pelican-plugins/","loc":"https://jackdewinter.github.io/2019/10/13/fine-tuning-pelican-pelican-plugins/"},{"title":"Fine Tuning Pelican: Copyright and Creative Commons Notice","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction For anything that you publish, you want to make sure that you clearly state your rights as the author of the publication. In addition, if you want people to use that information in various ways, you want to make sure that you clearly state how they can use that information for their benefit. This article outlines the steps needed to ensure that the website's copyright and any licensing is prominently displayed. Clearly Stating The Copyright and Licensing For copyright and licensing to be effective, you need it to be clearly stated in a place that is visible. One of the reasons I chose the Elegant theme is that it had a solid place for this at the bottom of the page. To add the necessary information for Elegant to display the copyright and licensing, add the following to the pelicancpnf.py file, with your own name instead of mine: # Legal SITE_LICENSE = \"\"\" &copy; Copyright 2019 by Jack De Winter and licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"> <img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /> Creative Commons Attribution 4.0 International License</a>. \"\"\" This configuration accomplished three things: Clearly states that the copyright for any information presented in the article, except where I denote that I am referencing someone else's work, is mine. Clearly states the license under which any information presented can be used, using both the Creative Commons icon and text. Provides a link to the Creative Commons page that clearly states what the licensing agreement is. Selecting the Right Licensing While there are various licenses out there, the Creative Commons licenses have the right balance for me between maintaining my ownership of my content and allowing others to use the information cleanly. The Creative Commons Licenses Page gives a good breakdown of what the particulars of each of their licenses allows and does not allow, with easy to use pictures to allow for quick dissemination of the information. While there are other licenses, such as the MIT License , the GNU General License , and others , for me the Creative Commons licenses are very dry and clean cut on what you can and cannot do. As I want people to use the information I am placing on my site, keeping the license simple and clean is one of my priorities. As one of my goals is to help people, educate people, and inspire people, the most simplistic license that allows people to use the information on the website while protecting my copyright was the best solution. Based on those justifications, the CC-BY license was the right choice for me. This version allows for the sharing and adapting of the information on the website as long as any use of the information is given proper credit, or attribution. Basically, feel free to use it, but give credit where it is due. What Was Accomplished In this article, I outlined a discussion of why you want to make sure that you have copyright and licensing information on your website. I then provided a simple way of adding the copyright and licensing information to each page of a Pelican website themed with Elegant. Finally, I had a quick discussion on what you should think about when deciding on the licensing on your website, and why I chose the CC-BY license for my website.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/09/fine-tuning-pelican-copyright-and-creative-commons-notice/","loc":"https://jackdewinter.github.io/2019/10/09/fine-tuning-pelican-copyright-and-creative-commons-notice/"},{"title":"Fine Tuning Pelican: Publishing, Drafts, and Document Status Defaults","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction When creating content for the website, it is often desired to group articles together that are in various stages of readiness for publication. Prior to publication, many content authors want to see how their content will actually look by preparing a draft of the content that only they can see. This article discusses settings that address both of those concerns. What Does the Document Status do? As part of the standard Pelican feature set, an article or page may have a status of hidden , draft or published . If the status is hidden , then it is ignored by Pelican, and no further action is taken. If the status is draft , then it is published under the drafts directory and not registered with any of the navigation maps. Finally, if the status is published , then it is published using the standard Pelican article and page mapping, and it is registered with the standard navigation maps. While there isn't much information on the status metadata tag, it is included in the main pelican documentation on File Metadata . Without any relevant DEFAULT_METADATA settings being changed (more on that in a minute), the default value for status is published . As such, as long as the article is in the right place to be picked up, it will be published as part of the website. If a draft version of the article is desired, then the status metadata must be set to draft as shown in this example markdown article: --- Title : Some Markdown Article Category : Quality Status : draft --- My markdown article . Unlike the normal publishing process, the rendered version of this file will be placed in the website's /drafts directory, most likely with a name such as some-markdown-article.html . While it is obvious that the page was created by looking at the output from the publishing of the website, this page will not appear on any summaries or searches on the website. This can all be changed by changing the status metadata to publish and re-publishing the website. Setting a Document Status Default This information is largely taken from Pelican's Publishing Drafts section. As is mentioned in that section, to change the metadata default for articles and pages from having a default status of published to a default of draft , the following text much appear in the pelicanconf.py file: DEFAULT_METADATA = { 'status' : 'draft' , } While the Publishing Drafts article does mention that to publish articles and pages, their metadata must now include a Status: published metadata line, I feel it does not stress this enough. When writing articles from that point forward, it may be easy to remember to add that metadata tag to each article. However, to ensure that any previously published page or article is still published, each previous article and page must be revisited and that metadata line must be added to those article's metadata section. What Was Accomplished When authoring content, it is often desired to group articles in a series together, often in different states of readiness. This article started by looking at the document status and how it works for Pelican, and then moving on to how to set a new default for a Pelican-based website. Finally, a note was added to Pelican's own documentation on document status to help any readers avoid \"losing\" any published articles if the default document status is changed.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/06/fine-tuning-pelican-publishing-drafts-and-document-status-defaults/","loc":"https://jackdewinter.github.io/2019/10/06/fine-tuning-pelican-publishing-drafts-and-document-status-defaults/"},{"title":"Fine Tuning Pelican: Article And Page Settings","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the first article in this series contains a Disclaimer section with generic information for each of the articles in the series. Introduction When creating content for a website, it is important to be able to create the content in a way that works for you, changing the configuration to suit your needs. For different reasons, when you publish that content, control of how that content is displayed is important to the character of your website. These settings control these aspects of the content publishing for the website. This article goes over some of the more useful settings for pages and articles and how they affect the website. Where Is the Content Located? I prefer to keep different forms of content in different locations, just to make it easier to keep where things are in my head. As such, I have separate content directories for each type of content: ARTICLE_PATHS = [ 'articles' ] PAGE_PATHS = [ 'pages' ] STATIC_PATHS = [ 'images' , 'theme/images' , 'extra/robots.txt' ] In my website's content directory, I have 4 main directories: articles, pages, images, and extra. The first two are easy to explain, and I set the Pelican configuration values ARTICLE_PATHS and PAGE_PATHS to point to those directories. The static paths specified by the Pelican configuration values for STATIC_PATHS are for folders containing anything that does not need transformation but is part of the website. The most important directories in that category are any images that either I provide or are provided by the theme. The robots.txt file will be covered in a future article, but provides guidance for any robots that are crawling the website. As I intend to keep publishing to this blog for a long time, it was important to me to make sure that I have an organization that can scale with me. By keeping each type of content separate from each other, it allows me to find articles quickly. I take this a step further and create a directory for any series that I am writing, keeping both the draft articles and published articles in that directory. An advantage of this is it allows me to ensure a group of articles has a consistent look and feel within that group with little effort. How is the Content Published? For the publishing of website pages, there are very few options for controlling their location, as Pelican assumes that the collection of pages is pretty constant, while the collection of articles will change frequently. As such, there are two main configuration values that control where the article is published. By default, both of these values are set to the same value {slug}.html , publishing all articles in the root output directory. That didn't seem right to me, so I changed these values to the following: ARTICLE_URL = '{date:%Y}/{date:%m}/{date: %d }/{slug}/' ARTICLE_SAVE_AS = '{date:%Y}/{date:%m}/{date: %d }/{slug}/index.html' When the article is published, it will be published to a directory mostly dictated by the publish date of the article, with the article itself written as an index.html file within that directory. For my tastes, this organizes the articles on the website in a manner in which I can quickly see what I published and when. The default values for ARTICLE_URL and ARTICLE_SAVE_AS just published everything in the root output directory, and that just seemed to cluttered for me. How Many Articles To Show For Lists? One of the other Pelican configuration values that I needed to set was the DEFAULT_PAGINATION value. On the main page, where the list of the most recent articles is displayed, I want to show a decent number of articles, but I don't want them to fall off the bottom too much. As such, I set the following configuration value to provide a default pagination of 10 items: DEFAULT_PAGINATION = 10 I am not sure if this is the right value to start with, but it seems like a decent place to start. I expect to make changes to this as I get more feedback from readers, so just giving it a decent starting value was enough. What Was Accomplished When writing content for a website, it is important to be able to create the content and display content in a manner that works for you, the content author. I described the settings that I have for my website which control where the content is stored as well as where the rendered content is published. In addition, I cover a configuration value that dictates the default for the maximum number of articles to show in a list.","tags":"Website","url":"https://jackdewinter.github.io/2019/10/02/fine-tuning-pelican-article-and-page-settings/","loc":"https://jackdewinter.github.io/2019/10/02/fine-tuning-pelican-article-and-page-settings/"},{"title":"Fine Tuning Pelican: Setting Up The Landing Page","text":"Preface This is one of the articles in a series about how I fine tuned my Pelican+Elegant web site to make it \"more mine\". For other articles in the series, click on the title of the article under the heading \"Fine Tuning Pelican\" on the right side of the webpage. Unlike my series on Choosing and Setting Up Pelican , these articles are intended to be byte sized, addressing specific fine tunings I made to my own website. As such, the Disclaimer section below is referred to by link in the other articles in this series instead of replicating it in each article. Disclaimer I have taken efforts to be clear about whether a given fine tuning was made to the Pelican configuration or the Elegant configuration for my website. Any Pelican configuration that I am documenting should be broadly applicable to any Pelican based site, and any Elegant configuration to a Elegant themed site. If you are using a theme other than Elegant, please consult the documentation for that theme to determine if that theme supports something similar. In addition, I want to be clear that I am a contributor to the Elegant theme that I use for my website, helping with the documentation of the theme. While I decided that it is the best choice for a theme for my website, I leave it to any reader to make their own determination as to which theme is best for their own website. I merely reference Elegant in these notes on Fine Tuning as part of the configuration that I had to learn and set up for my own website. Your mileage may vary. Introduction It is important to engage readers of the website and to give them multiple reasons to come back to the website time and time again. The creation of a good landing page for the website is a large part in making that engagement with the readers happen. This article walks through a number of things that help a landing page increase it's engagement with the readers. One of the most important parts of that is to give the website a good name that will stick in their head and display it prominently on the website. Following that, a well thought out introduction to the website will then give the reader a solid idea of what to expect, allowing them to determine for themselves if it is worth their time and effort to return to the website. Finally, adding a list of projects that are related to the website can be a useful tool in demonstrating to the reader why the website is useful by documenting how the website and it's authors benefit other websites and projects. Giving the Website a good name When a reader first comes to the landing page, it is pivotal to have a name for the website that is front and center on the landing page, easy to identify, and one that will stick in their mind. The Elegant theme partially helps with this by putting the site name at the top left corner of the webpage, with no other text or images right next to it. This Pelican configuration is set with the following configuration in the website's pelicanconf.py file: SITENAME = \"Jack's Digital Workbench\" While the title placement helps, it is up to the website owners to determine the name to be used with the website. I put a lot of thought into the name of my website, as detailed in my article The Inspiration For Jack's Digital Workbench . However, prior to me getting the finishing touches on the website for a soft-launch, the name was Jack's Web Site for a couple of months. I wanted to give it a name that reflected what I wanted to communicate, and I believe that waiting to give it the proper name allowed me to really figure out the right title, and not just a placeholder title. Introducing myself to Readers Now that the website's name was configured, it was time to introduce myself to the people who would hopefully become readers of my blog. The first part of this was to follow Elegant's Home Page - Write Welcome Message article and set the title of the landing page to reflect the new name of the site: LANDING_PAGE_TITLE = \"Welcome to \" + SITENAME With the title of the introduction taken care of, it was time to further follow Elegant's advice, this time from the article titled Home Page — Write About Me . Creating a new document in my content directory called /pages/landing-page-about-hidden.md , I copied their sample document and initially trimmed it down to the following: --- Title : What is Jack ' s Digital Workbench? slug : landing - page - about - hidden status : hidden --- This blog is a place for me to ... After building the website and serving it up locally, to make sure that page was being included properly, I started working on the content. Due to the importance of this page, I started working on it in mid-August and didn't finish it until mid-November. While this may seem a bit of a long time and somewhat silly, I wanted to make sure that this introduction was done right. In the end, after 5-6 drafts, I made notes on what I liked and disliked about each draft, then starting from scratch to write the final version in a couple of hours. During all of those drafts, what I was looking for in an introduction was something that was descriptive, yet not too wordy. In most of my drafts, I nailed one section, but the other two failed this test, so the final version took the best parts of a number of the drafts and wrapped them up as one. I believe it took a long time because I was trying to find the right balance for what I wanted to say, without writing what seemed to be an entire article. In the end, I ended up taking one of the more verbose sections of the introduction and made it into it's own article , replacing it with just two sentences. For me, this struck a good balance between communicating what I wanted in that section and keeping the introduction descriptive, but brief. Adding in related Projects I believe that in any website, it is important to show how that website and it's authors contribute or are related to other projects. Elegant supports this configuration out of the box with the PROJECTS configuration value, documented with the Elegant article on Home Page — Projects List . For my website, this was set up as the following: PROJECTS = [ { 'name' : 'Creating and Maintaining This Website' , 'url' : 'https://jackdewinter.github.io/categories#website-ref' , 'description' : 'Notes and articles about the creation and maintenance of this website.' }, { 'name' : 'Elegant Documentation' , 'url' : 'https://elegant.oncrashreboot.com/' , 'description' : 'Documentation repository for Pelican-Elegant theme.' } ] From my research on different authors and their blogs, the ones that ended up grabbing my attention were the ones that not only solved a problem I had, but showed interest in related projects. Those related projects didn't always have to be related to what I was looking for, but they added \"color\" to the website. Without any extra references to other related websites, I found that I viewed that website as a reference resource rather than a website that I bookmarked and frequented. One thing I thought about for a while was whether or not to include the Creating and Maintaining This Website in the projects list. For the longest time, I was stuck between calling it a category (it is a distinct group of articles) and calling it a project (it is stored as one in GitHub). In the end, the balance I achieved with this was to create a category for it that appears after pressing the Categories button on the title bar and not mentioning it in my introduction where I talk about the other categories. I am not sure if this makes sense to any readers, but I felt that the equilibrium achieved by those two choices allow it to be a category but elevate it a bit to something that is a bit bigger than just another category. As the website is somewhat fluid, I'll see how that goes for now, and perhaps change it later. What Was Accomplished The landing page of any website is almost certainly the most important page or article on that entire website. I documented the process I used to capture a reader's attention by selecting a good title for the website, followed by a good introduction to what the website and it's authors are about. As part of this process, I also talked about how I approached these decisions, and how I figured out that I had accomplished my goal for both of these items. Finally, I talked about how I configured a Projects section to detail some of the other projects I am working on, and why having a section like that is important to a website. The landing page of any website can capture the attention of a reader or lose that attention just as easily. It is important to take your time when developing these parts of your website to ensure that these pieces of your website are an accurate reflection of yourself and what you intend to communicate with the website.","tags":"Website","url":"https://jackdewinter.github.io/2019/09/29/fine-tuning-pelican-setting-up-the-landing-page/","loc":"https://jackdewinter.github.io/2019/09/29/fine-tuning-pelican-setting-up-the-landing-page/"},{"title":"Static Websites: Publishing To GitHub Pages","text":"This is the fifth article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous articles, I used Pelican as my Static Site Generator, generating a nicely themed site with some test articles in it. To make sure I can speak with my own voice, I took my time determining what theming I wanted for the website. By taking these deliberate steps, I was able to arrive at my choice for the site's theme with confidence and clarity. This entire journey has been geared towards generating an externally hosted website that is primarily a blog. This article talks about the final step on that journey: publishing my website to the free GitHub Pages platform. Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Why GitHub Pages? In looking at a number of sites built on various Static Site Generators (SSGs), it became obvious that a majority of the pages were hosted on GitHub Pages . With that in mind, I looked into GitHub pages to figure out if it was best solution for me. The article What is GitHub Pages? goes into a decent amount of detail, but the summary boils down to: it's 100% free only static content can be hosted don't do anything illegal don't create an excessively large site (over 1 GB in size) don't create a site that is incredibly popular (over 100GB per month in bandwidth) For cases where the last two items occur, their website even mentions that they will send \"a polite email from GitHub Support or GitHub Premium Support suggesting strategies for reducing your site's impact on our servers\". To me, this seemed like a good place to start. I already use Git for source management, so familiarity with the website and tooling is already there. Their documentation is good, and it looks relatively easy to implement. Another plus. Most importantly, there are no fees for upload or serving the content, so I can experiment with various things and not worry about incurring extra charges. Branches on GitHub Pages After doing my research on GitHub, specifically about publishing on GitHub pages , I was confused about one point. From my experience with Git, most people and companies do either repository based development or branch based development. Even less frequent is something called monolith based development. The approach for GitHub pages is not one of those. Repository based development uses Git's distributed nature to create changes on your own repository, only merging the changes into the \"main\" repository when you are sure of your changes. Branched based development is similar, except the branching feature of Git is used on a single remote repository, only merging changes into the \"main\" branch when you are sure of your changes. Monolith development is more dangerous, with committing all changes to a single repository with a single branch. For all three type of development, there is one thread going through all of them: you are keeping versions of a single type of thing in your repository. In a number of sites that I researched, it appeared that they were using a tool called ghp-import . This tool allows for the content for the site to be stored in the content branch of the repository, while the produced website is pushed to the master branch of the same repository. While I can wrap my mind around it, to me it didn't seem like a good idea. As this is outside of my normal workflows, I was pretty sure that at some point I would forget and push the wrong thing to the wrong branch. To keep things simple, I wanted my website content in one repository, and my website content in another repository. That itself raised some issues with my current setup, having the output directory at the same level as the content directory. During my research, I came across the statement that Git repositories cannot contain other repositories. If you do need to have this kind of format, a concept called submodules was recommended. The plugins and themes repositories for Pelican make heavy use of submodules, so I knew it could be done. But after some experimentation with some sample repositories, I was unable to make this work reliably. Also, while I can learn to wrap my mind around it, it seemed like a lot of extra work to go through. In the end, I decided that it was best to keep things simple, keeping 2 repositories that were 100% separate from each other. If I do more research and figure out how to make submodules work reliably, I am confident that I can condense these distinct repositories into one physical repository. With that decision made, I needed to create a new output directory outside of the blog-content directory. I decided to call this new directory blog-output and have it at the same level as blog-content . To make sure it was initialized properly with a local repository, I entered the following commands: mkdir ..\\blog-ouptut cd ..\\blog-ouptut git init Once that was complete, I had to ensure that the pelican-* scripts were changed to point to the new location, taking a simple search and replace over all of the script files. That being completed, I executed each of my pelican-* scripts, to verify the changes were correct, with no problems. To further ensure things looked good, I performed a git status -s on both repositories to be sure I didn't miss anything. While this approach wasn't as elegant as the other solution, in my mind it was simpler, and therefore more maintainable. Adding Remotes Repositories Now that I had two local repositories, one for content and one for output, it was time to make remote repositories on GitHub for each of them. I already had a GitHub account for some other projects I was looking at, so no worry there. Even if I didn't have one set up, GitHub makes it simple to set up a new account on their home page . From there, it was a simple matter of clicking the plus icon at the top right of the main window, and selecting New Repository from the drop down list. The first repository I created was for the content, and I simply called it blog-content , which I entered in the edit box under the Repository Name label. As I wanted my content to be private, I changed the selection from Public to Private and clicked on the Create Repository button. For the other repository, I followed the same instructions with two exceptions. The first exception is that, as the output of Pelican needs to be public to be seen, I kept the selection on Public . The second exception was the name of the repository. According to the User Pages page, to publish any committed pages you need to use a site of the form user-name .github.io and push any changes to the master branch. As my user name on GitHub is jackdewinter , this made my repository name jackdewinter.github.io . If you are using this article as a guide, please note that you will need to change the repository name to match your own GitHub user name. Securing The GitHub Access The first time that I added my remote repositories to their local counterparts, I encountered a problem almost right away. When I went to interact with the remotes, I was asked to enter my user id and password for GitHub each time. This was more than annoying. Having faced this issue before on other systems, I knew there were solutions, so back to the research! Now, keep in mind that my main machine is a Windows machine, so of course this is a bit more complicated than when I am working on a Linux machine. If I was on a Linux machine, I would follow the instructions at Connecting to GitHub with SSH and things would probably work with no changes. To start with, I want to make sure that GitHub has it's own private/public key pair, so I would follow the instructions under Generating a New SSH Key and adding it to the ssh-agent . I would then follow the instructions under Adding a new SSH key to your GitHub account to make sure GitHub had the right half of the key. A couple of Git commands later, and it would be tested. In this case, I needed to get it running on windows, and the Win10 instance of SSH takes a bit more finessing. To make sure the service was installed and operational, I followed the instructions on Starting the SSH-Agent . Once that was performed, I was able to execute ssh-agent , and only then could I use ssh-add to add the newly created private key to ssh-agent . In a nutshell, I needed to execute these commands to setup the key on my local machine: ssh-agent ssh-keygen -f %USERPROFILE%\\.ssh\\blog-publish-key -C \"jack.de.winter@outlook.com\" ssh-add %USERPROFILE%\\.ssh\\blog-publish-key Attaching Remote Repositories to Local Repositories This was the real point where I would see if things flowed together properly. First, I needed to specify the remote for the blog-content repository. Looking at my GitHub account, I browsed over to my blog-content repository, and clicked on the clone or download button. Making sure the link began with ssh , I pressed the clipboard icon to copy the link into the clipboard. Back in my shell, and I change directory to blog-content and entered the following: git remote add origin %%%PASTE HERE%%% where %%%PASTE HERE%%% was the text I copied into the clipboard. As my user id is jackdewinter and the repository is blog-content , the actual text was: git remote add origin https://github.com/jackdewinter/blog-content.git This process was then copied for the blog-output directory and the jackdewinter.github.io repository. Publish the Content to Output Until this point, when I wanted to look at the website, I would make sure to have the windows from the pelican-devserver.bat script up and running. Behind the scenes, the pelican-autobuild.bat script and the pelican-server.bat scripts were being run in their own windows, the first script building the site on any changes and the second script serving the newly changed content. As long as I am developing the site or an article, that workflow is a good and efficient workflow. When generating the output for the actual website, I felt that I needed a different workflow. As that act of publishing is a very deliberate act, my feeling is that it should be more controlled than automatically building the entire site on each change. Ideally, I want to be able to proof a group of changes to the website before making those changes public. One of the major reasons for the deliberate workflow is that, from experience, the generation of anything production grade relies on some form of configuration that is specific to the thing you are producing. For my website, this needs extra testing specifically around that production configuration in order for my confidence in those changes to be high enough that I am confident in publishing it. The most immediate example of such configuration is the SITE_URL configuration variable. While it was not obvious in the examples that I researched, this variable must be set to the actual base URL of the hosting site. Using the Elegant theme, if you click on the Categories button in the header, and then the Home button, it will stay on the Categories page. Looking more closely at the source for the base.html page, the Home button contains an url is defaulted to '‘ . Digging into the template for the base.html page, the value being set for the anchor of that button is href=\"{{ SITEURL }}\" . Hence, for the Home button to work properly, SITE_URL needs a proper value. The default configuration in pythonconf.py for SITE_URL is '‘ , so that needed to be changed. For the developer website to work properly, SITE_URL must be set to ‘http://localhost:8000' in pythonconf.py . This however introduces a new issue: how do I make sure this variable is set properly when we publish the output? Luckily, the Pelican developers thought of situations like this. Back in the second article of this series, Step 4: Create a Generic Web Site , I mentioned a file called publishconf.py . This file was generated as part of the output of pelican-quickstart and has not been mentioned since. This file is intended to be used as part of a publish workflow, allowing the variables from publishconf.py to be overridden. Specifically, in that file, the following code imports the settings from publishconf.py before defining alternate values for them: sys . path . append ( os . path . abspath ( os . curdir )) from website.pelicanconf import * Below this part of the configuration, in the same manner as in pythonconf.py , the SITEURL variable in publishconf.py is set to '‘ . Therefore, when I publish the website with the publish configuration, it will use '‘ for the SITE_URL . To make sure the website publishes properly, I needed to change the SITE_URL variable in publishconf.py to reflect the website where we are publishing to, namely https://jackdewinter.github.io . Now that I took care of that, I just needed to come up with a batch script that makes use of publishconf.py . To accomplish that, I simply copied the pelican-build.bat script to pelican-publish.bat , and edited the file removing the –debug flag and referring to publishconf.py instead of pelicanconf.py : pelican --verbose --output ..\\blog-output --settings website\\publishconf.py website\\content To test this out, I stopped the pelican-autobuild.bat script and executed the pelican-publish.bat script. By leaving the pelican-server.bat script running, I was able to double check the published links, verifying that they were based on the jackdewinter.github.io site where I wanted to publish them. Pushing the Content To The Remote At this point, I had two local repositories, one with commits and one without, and two remote repositories with no information. While I wanted to see the results and work on the blog-output repositories first, it was more important to make sure my work was safe in the blog-content repositories. So that one would be first. Changing into the blog-content directory and doing a git status -s , I noticed a couple of changes that were not committed. A quick git add –all and a git commit later, all of the changes were committed to the local repository. At this point, the changes are present in the local repository, but not in the remote repository. The following command will push those changes up to the remote repository's master branch. git push --set-upstream origin master At this point, I did a quick check on the blog-content repository in GitHub and made sure that all of the repository was up there. Now, in the future, I knew I would be more selective than using git add –all most of the time, but for now it was a good start. So I carefully went through the files that GitHub listed and verified them manually against what was in the directory. I didn't expect any issues, but a quick check helped with my confidence that I had set up the repository correctly. Pushing the Output To The Remote Once that was verified, I carefully repeated the same actions with the blog-output directory but with one small change. In the blog-content directory, I want to save any changes. However, with the blog-output directory, I want to commit everything, ever if there are conflicts. This is something that is done with quite a few static sites, so the workflow is decently documented. As this is an action that I am going to repeat every time I publish, I placed in a script file called pelican-upload.bat : pushd ..\\blog-output git add --all . git commit -m \"new files\" ssh-agent git push origin master --force popd In order: switch to the blog-output directory, add all of the files, commit them with a simple reason, ensure the ssh-agent is up and running, push the committed files to remote repository, and go back to our original directory. If that last git push looks weird, it is. It is so weird and destructive that there are a number of posts like git push –force and how to deal with it and GIT: To force-push or not to force-push . However, even after I looked at the manual page for git push , I was still trying to figure it out. It wasn't until I came across The Dark Side of the Force Push , and specifically the Force Push Pitfalls section of that article, that things made sense. Under new script run pelican-upload.bat Viewing the Webpage To make sure things looked right, I wanted to do a side by side comparison of what I could see in my browser both locally and on the new website. To do that, I opened up one tab of my browser and pointed it to http://localhost:8000/ , and another tab beside it and pointed it to https://jackdewinter.github.io/ . To be honest, while I was hoping there would be no issues, I was expecting at least 1-2 items to be different. However, as I went through the comparison, there was 100% parity between the two versions of the website. What Was Accomplished At the beginning of this article, I had most of what I needed to start selecting a theme. It took some small updates to the configuration to make sure I had a good test site available. This was critical to allowing me to go through each theme I was interested in and see if it was for what I was looking for. While one of the themes proved to be a handful, the experience was good in advising me of possible issues I might have in customizing my own site. In the end, I had a strong choice of the elegant theme, which as benefits, is actively being developed and has great documentation. What's Next?","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/09/22/static-websites-publishing-to-github-pages/","loc":"https://jackdewinter.github.io/2019/09/22/static-websites-publishing-to-github-pages/"},{"title":"What is Software Quality?","text":"When introducing myself to someone professionally, I usually start with the normal \"Hi, my name is …\" that is boiler-plated on nametags the world over. Getting past that initial point, if the person is so inclined, they ask that always fun lead off question \"So, what do you?\" For me, I always respond with \"I am an SDET\" 1 , to which anyone not in the software industry replies back with \"Um.... What is that?\" Spewing out \"It means I am a Software Development Engineer in Test.\", I wait for the response that most people use: \"Oh, so you are a tester.\" Often with gritted teeth, I try and explain that testing is only a small part of what I do. If I think they are still listening, I given them my quick elevator pitch that emphasizes that I focus on helping to produce good quality software by helping to increase the quality of the teams, the projects, and the processes that I am tasked to assist with. Approximately 60-70% the time I win people over with the elevator pitch, and a pleasant conversation continues. The next 20-30% of the time, usually with people not in the software field, I get blank stares and they fixate on the \"test\" in the title rather than the \"quality\" in my description. The remaining people are usually Software Development Engineers or SDEs 2 that for one reason or another, start to tune out. For the percentage of people that I win over, they seem to understand that I focus on quality, but the follow up question is almost always: \"What does quality software mean to you?\" Where do we start? For me, I almost always start at the beginning with requirements. Whether they are spoken or written down, each project has a set of requirements. It could be the requirements are to \"explore my ability to use X\" or \"fix the X in the Y project\" or \"create a project that can help me X\", but every project has requirements. In the software development industry, requirements are often presented to teams that are hard to deal with or are left to the developers to write themselves. This practice is so prolific that Scott Adam's Dilbert site has pages and pages of instance where requirements are talked about . One example is when a manager talks to their team and informs them that some process needs to be faster by 5%. Do they have enough information from that manager to understand the context of the requirement? Do they expect that increase by a specific time to meet their own goals? What does that requirement look like? How do they know when they have achieved it? Is it achievable? If it is achievable, how do they measure progress towards that goal? These are some of the core questions that I believe need answering. As those questions are at the front of my mind, when someone asks me how I define software quality, the first thing I immediately think back to is a course that I once took on setting S.M.A.R.T. requirements . In that class, the main focus was on taking unrefined requirements and curating them to a point where they could be more readily be acted upon. The instructor made a very good argument that each requirement must be Specific, Measurable, Assignable, Realistic, and Time-Related. When it comes to software quality, I believe those same questions needs to be asked with regards to any of the requirements teams put on their software. But to ask those questions properly, we need to have some context in which to ask those questions. To establish that context, it is helpful to have some guidelines to provide a framework for the requirements. Establishing Some Guidelines: The Four Pillars A good general article for anyone interested in software quality is the Wikipedia article on Software Quality . In fact, when asked by people where to get started in the software quality area, I often refer them to this article solely because of the excellent diagram in the Measurements section on the right side of the page. 3 The diagram in the Measurements section correlates very closely to what I believe are the four pillars of software quality: Reliability, Maintainability, Efficiency, and Security. The diagram then shows how their pillars relate to other attributes: Application Architecture Standards, Coding Practices, Complexity, Documentation, Portability, and Technical/Functional Volumes. From there, it provides more lists of how to break things down, with many references to other articles. In short, it is a great place to start from. Measuring Software Quality Before proceeding to talk about the pillars themselves, I feel strongly that we need to discuss the categories that I use for measuring the metrics talked about in the Wikipedia article. My intention is that by talking about the metrics before discussing each of the pillars, you can start building a mental model of how to apply them to your projects as you are reading about them. From my point of view, making that mental transition from something abstract that you read about to something concrete that applies to your work is essential to serious forward momentum on software quality. These metrics typically fall into two categories: seldom violated metrics and positive momentum metrics. The seldom violated metrics category contains rules that define rules that are pivotal to the quality of your project. Each rule are a combination of a given metric and a maximum or minimum weighed against that metric. As a guideline, teams should only ignore these rules on a case by case basis after providing a reason that is good, defensible, and documented. Examples of such metrics are Service Level Agreements (SLAs), Static Code Analysis (SCA) results, and Test Failure Rates. Examples of rules are \"the TP99 for the X API is Y millisecond\" or \"all PMD warnings (Java SCA tool) must be following with a minimal of suppressions\". Furthermore, to make these rules useful and to keep your team honest, your team needs to publish the selected metrics, with a description of what the metrics are, how your team measures those metrics, and why your team is measuring them. The positive momentum metrics category is usually reserved for metrics that are being introduced to an already existing project. When introducing software quality metrics into an already existing project, it is not realistic to expect those metrics to be adhered to in an instant. It is more realistic to expect positive momentum towards the goal until the point when your team achieves it, at which point is moves to the desired seldom violated metrics category. As such, a measure of the momentum of these metrics is used, and is hopefully in a positive direction. Similar to the previous category, your team should publish information about the selected metrics, with the added information on when your team feels they will translate it from the positive momentum category to the seldom violated category. Being consistent on these chosen metrics is very important. While dropping a metric looks better on any reporting in the short term, it usually negatively impacts the software quality, perhaps in a way that is not obvious until later. Adding a new metric will show lower the measured quality in the short term, but increases the measured quality in the long term. Your team can negate the short term impact by paying the immediate cost of making the new metric a seldom violated metric, but that has to be weighed against the other priorities for your project. As with everything, it is a balancing act that needs to be negotiated with your team. Exploring The Four Pillars Having established that S.M.A.R.T. requirements and the two categories for metrics from the previous sections are useful in measuring software quality, the focus of the article can return to the guidelines: the four pillars. Each one of these pillars will look at your software project from a different angle, with the goal of providing a set of data points to formulate a coherent measurement of software quality for that project. In the following sections, I strive to describe each of the four pillars, providing a jumping off point to another article that describes that pillar in a more comprehensive manner. I firmly believe that by providing metrics for each pillar that are specific to your project, with each of those metrics properly categorized into the two measurement categories documented above, that your team will take a decent step forward in clearly defining software quality for your project. Reliability The essence of this pillar can be broken down into two questions: Does the software do the task that it is supposed to do? Does the software execute that task in a consistent manner? Reliability is one of the areas in which \"pure\" testing shines. A lot of the tests that SDEs, SDETs, and testers are asked to write specifically verify if a given object does what it is supposed to do. Unit tests determine whether an individual software unit, such as a class, performs they way it is supposed to. Functional tests or integration tests take that a step higher, determining whether a group of related software units do what they are supposed to do. Another step higher are the scenario tests, which determine whether the software project, as a whole, responds properly to various use cases or scenarios that are considered critical to its operation. Finally, end-to-end tests or acceptance tests determine whether or not a group of projects respond properly from an end user's perspective. This pattern is so widely used, any search for test pyramid , will find many variations of the same theme. Different articles on the subject will stress different points about the pyramid, but they will all generally look like this: This pyramid, or other similar pyramids, are interpreted by authors to indicate a specific things about the tests, to highlight the position of their article. Some of these interpretations are: An article on test volume will typically stress that ~70-80% of the tests should be at the unit test level, ~10-15% at the functional test level, ~5-10% at the scenario level, and ~1-3% at the end-to-end level. An article on test frequency will typically stress that tests near the bottom of the pyramid should complete within 60 seconds and be executed every time the source code is checked in. Tests near the top of the pyramid may take minutes or hours and should be executed once a week. An article on test fragility will typically stress that tests near the bottom of the pyramid are closer to their components, the expectation is that they will not fail. Tests near the top of the pyramid require more orchestration between projects and teams, and therefore, are more likely to failure do to environmental or other reasons. While all of these interpretations have merit, the critical point for me is the issue of boiling down that information to a small number of bite sized observations that can be easily measured and communicated. In the upcoming article Software Quality: Reliability , I will delve more into breaking the Reliability pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. Maintainability The essence of this pillar can be broken down into one question: If you are asked to change the software to fix a bug or introduce a new feature, how easy is it to change the software, how many surprises do you expect to encounter, and how confident will you be about the change afterwards? The best, and most comedic, form of asking this question is captured by this cartoon from OSNews : Maintainability is a single pillar that encompasses the most diverse types of processes and measurements of any of the pillars. The reason for this is that maintainability is often a word that is used without a lot of provided context. For me, a good way to think about maintainability is that it is the cleanliness of your project. Different people will have different experiences, and after asking different people about how \"clean\" the project is, the collected answers will almost certainly by different. Try this in your office with your colleagues. Point to a given area of your office and ask 2-5 people how clean a given area, such as your desk is. Instead of accepting a single answer, dig in a bit as to why they answered the way they did. Most likely, you will get as many distinct answers as people that you talk to. This exercise illustrates how hard it is to give a good answer to how maintainable software a given piece of software is. The best way to provide metrics for maintainability is usually with various Static Code Analysis tools. Almost every mature language has at least one tool to do this, and each tool usually measures a fair number of metrics. These metrics will use established (and sometimes experimental) industry practices to look at the source code of your project and determine if there are issues that can be addressed. In addition to those metrics, those same tools often look for \"problematic\" and \"sloppy\" code. Problematic code is usually some manner of pattern that a fair number of experts have agreed is a bad thing, such as appending to a string within a loop. Sloppy code is usually things like having a variable or a parameter that is not being used, or a forgotten comment on a public method. In addition to Static Code Analysis, teams must continue to strive to have a good set of documentation on what the project is doing, and regularly maintain that documentation. While the \"correctness\" of the documentation is harder to measure than source code, it is pivotal for a project. How much of the information on the various projects that your team supports is in the head of one or two individuals? What is going to happen if they leave the team or leave the company. Your team should not need volumes of information on every decision that was made, but as a team, it is imperative to document the major decisions that affect the flow of the project. It is also a good idea to have solid documentation on building, deploying, and executing the project. Imagine yourself as a new team member looking at the software project and any documentation, and honestly ask yourself \"How much would I want to run away from that project?\" If the honest answer from each member of the team is something similar to \"I'm good\", you probably have a decent level of documentation. A Note On Static Code Analysis Before delving deeper into maintainability, I want to take a minute to talk about Static Code Analysis. Typically, Static Code Analysis is used as a gatekeeper for maintainability, and as such, any suggestions should be strictly followed. However, Static Code Analysis tends to be an outlier to the gatekeeper rule in that the metrics need to be \"bent\" every so often. This \"bending\" is accomplished using some form of suppression specified by the Analyzer itself. Static Code Analyzers tend to fall into two main categories: style and correctness. Any warnings that are generated by a style analyzer should be addressed without fail. In terms of stylistics, there are very few times where deviating from a common style are beneficial, and as such should be avoided. As stylistics can vary from person to person when writing code, it is useful to supplement the style analyzer with an IDE plugin that will reformat the source code to meet the team's stylistics, with the Static Code Analyzer acting as a backstop in case the IDE formatting fails. Warnings generated by correctness analyzers are more likely to require bending. Most correctness analyzers are based on rules that are normally correct, but do have exceptions. As such, your team should deal with these exception by having a follow up rule on when it is acceptable to suppress the exceptions, and specifically on a case-by-case basis. It is also acceptable to suppress the exception after generating a future requirement to address the exception, if your team is diligent on following up with these requests. In both cases, it is important to remember that SCAs are used to help your team keep the project's maintainability at a healthy level. Back to Maintainability In the upcoming article Software Quality: Maintainability , I will delve more into breaking the Maintainability pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. I will do this by presenting the 4-5 metrics that I consider to be useful as well as both patterns and anti-patterns to avoid. [ED: Need to rephrase that last sentence.] Efficiency The essence of this pillar can be broken down into one question: Does the software execute that task in a timely manner? Similar to my analogy of maintainability being the cleanliness of your software, efficiency is whether or not your software is executing \"fast enough\". Coming up with an answer to a question on whether or not something is \"fast enough\" is usually pretty easy. But when you ask for a definition of what \"fast enough\" means, that is when people start to have issues coming up with a solid answer. In my experience, a large part of the reason for that vagueness is usually not having a good set of requirements. As an example, let's figure out what \"fast enough\" means for two different video games that my family plays: Civilization and Rocket League. For the game Civilization (in multiplayer mode), the big delays in the game are the human interactions and decisions required before a player ends their turn. It is also very important that all of the information get conveyed between turns so that the multiplayer server can accurately record actions in a fair and just manner. For this game, \"fast enough\" for the software is largely dwarfed by the delays that the players introduce. However, if we have a game with 12 players, 2 of them human and the other 10 using the game's AI players, then we can start to formulate what \"fast enough\" is for the AI players. It really depends on the context. Rocket League is a different story. Rocket League is a sequel to the game \"Supersonic Acrobatic Rocket-Powered Battle-Cars\" released in 2008. In this game, you play a game of arena soccer using rocket powered cars, each match consisting of a series of games between teams of 1-3 players. Unless there is a LAN tournament between professional teams, it is very rare for more than one player to be in the immediate vicinity of their teammates, and often players are from different states/provinces and even countries. For the client software on the player's computers, \"fast enough\" is measured by latency and packet loss. With each player's action being relayed to the server and then back out to the other players, any packet loss or increase in latency will impact the server's reaction to various inputs from the player's controllers. For this type of game, \"fast enough\" depends on a good network connection and a server that is able to process many actions per second. As you can see from the video game example, efficiency greatly depends on what the requirements of the software are. In the upcoming article Software Quality: Efficiency , I will delve more into breaking the Efficiency pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. Security The essence of this pillar can be broken down into one question: How easy is it for a third party to perform malicious actions with your software? That is one dramatic question. \"Perform malicious actions.\" Wow! I have read all sorts of articles on various news sites about those, but surely they cannot affect my software? That is usually one of the first reactions of a lot of software developers. Just 10 minutes with a security researcher can open your eyes to what is possible. To understand this better, pretend that your software project is on a slide, being viewed through a microscope. If you look at the slide without the microscope, you just see your software on the slide, pretty much the same as any other slide. However, if you increase your magnification by one order of magnitude, you see that your project includes your source code and components developed by other people. You may be following proper security practices, but did they? Another order of magnitude down, and you are looking at the low level instructions for your project and any included components. Once the component was assembled, could a third party have added some malicious code to that component, executing normally until they activate it? Was that malicious code in their from the beginning? Or maybe it is a vulnerability at the source code, machine code, or machine levels? Someone can make a small change to a component to utilize that vulnerability with little effort if they know what they are doing. Reversing our direction, if we expand outwards instead of inwards, we have containerization. Containerization solutions, such as Docker , provides a complete computing environment to execute your software within. Popular with back end development, you encapsulate your software with it's intended operating system platform, reducing the number of platform's you need to design your software for to 1. But with containerization, we also have to ask the same questions of the platform as we did with the software. How secure is the operating system that the container uses as it's base? In today's world of software development, where componentization is key, the software you write is not the only place where security issues can be introduced. However, there are proactive steps you can take to reduce the vectors than users can follow to use your software maliciously. In the upcoming article Software Quality: Security , I will delve more into breaking the Security pillar into S.M.A.R.T. requirements and I provide suggestions on how they it be measured. Back To Requirements Having explored the 4 pillars, it is important to bring the discussion back to the definition of good requirements. Using the information from each of the individual pillar articles in concert with the information on S.M.A.R.T. terminology, your team can request requirements that are more focused. As any focused requirements will be Specific (the S. in S.M.A.R.T.), it is reasonable to expect that any impact on our 4 pillars will be noted. Asking for this change will almost guarantee some negotiations with the team's stakeholders. In my experience, when your team asks for more focused goals from your stakeholders, there will typically be some pushback from those stakeholders at the beginning. If your team has had some requirements mishaps in the past, highlight each mishap and how the ensuing loss of time and focus could have been avoided usually sways stakeholders. Don't point fingers, but simply point out something like: Hey, when we did the X requirement, we all had a different idea on what to fix, and as such,it took X hours of meeting and Y hours of coding and testing to figure out it was the wrong thing. We just want to help tune the requirements process a bit to help everyone try and avoid that waste.\" Most stakeholders are being asked to have their teams do the maximum amount of work possible in the shortest amount of time. By asking that question in such simple manner, you are asking if you can spend a small amount of time up front to hopefully eliminate any such missteps. Most stakeholders will grab on to that as a way for them to look good and for the team to look good, a win-win. What will these requirements look like? The requirements will typically come in two main categories. The first category, requirements focused on fixing bugs or adding features, will typically be the bulk of the requirements. Each requirement should outline any negative impact it will have on any of the metrics. If nothing is added on negative impacts, the assumption is that the impact will be neutral or positive. A good example of this is a requirement to add a new feature to the project. The requirement should be clearly stated using S.M.A.R.T. terminology, because it will remove any ambiguity in the requirements. As any source code added without tests would impact any reliability metrics, reliability tests should be added to meet any seldom violated metrics for your project. In similar ways for the other 3 pillars, it is assumed that any source code added will be a step forward or neutral in terms of quality, not backward. At some point in your project, you should expect that at least a few of the requirements will appear in the the second category: requirements specifically targeted at one or more of the pillars. These requirements allow your team to focus on some aspect of your project where your team feels that the quality can be improved. The big caveat with these requirements is to be mindful of the Achievable and Time-Related aspects of S.M.A.R.T. requirements. Make sure that whatever the goal of these requirements are, they are things that won't go on forever and are not pipe dreams. A good example of this is wanting to improve the efficiency of your project or processes. Without a good requirements that is Specific, Achievable and Time-Related, this can go on forever. A bad requirement would state something like \"Make the project build faster\". A good requirement might state something like \"Reduce the unit test time from 20 seconds to under 15 seconds\", timeboxed to 4 hours. The good requirement has good guard rails on it to keep it from exploding on someone who picks up that work. Publishing Software Quality Having gone through the previous sections and any related articles, you should have a better idea on: how to write better requirements to ask for software quality to be improved what metrics I recommend to use for each of the four pillars how to measure those metrics and integrate them into your projects Using this information as tools, your team can improve the quality of the project at it's own pace, be that either an immediate focus or a long term focus for your project. For any metrics that are in the seldom violated category, the best way to approach them is to make them gatekeeper metrics for your project. It should be possible to execute a great many of the gatekeeper metrics before a commit happens, which is optimal. For the remaining metrics in the seldom violated category and metrics in the the positive momentum category, your team should publish those metrics with every commit or push, giving the submitter that needed feedback. In addition, publishing the metrics to some kind of data store allows your team to determine how the project quality is trending over time, allowing any stakeholders or project members to observe any potential software quality issues and take steps to deal with them. Even for certain seldom violated metrics, it can be useful to track how they are trending, even if they are trending above the gatekeeper lines set for the project. If your team does not publish those metrics in some form, the only data point they have for the project is a binary one: it passes or it does not. From my experience, that binary metric is often a false positive that burns teams due to a lack of information. What Does Software Quality Mean To Me? Software quality means each software project has a plan. When requirements come in to the project, they are detailed using the S.M.A.R.T. terminology. If not specifically geared towards a given software quality pillar, each requirement may specify what kind of impact it has on one or more of the pillars. If not specified, it is assumed that it has a neutral or positive effect on all of the software quality pillars. The goals are also specific, not overly broad, and realistically achieved within a given time frame. Software quality means that metrics are well thought out for each project. Each metric is both defensible and reasonable for that project and that team. Any metrics that are not being used as gatekeepers are published so they can be tracked over time. For additional benefit, non-binary gatekeeper metrics are also published, to further improve the project and the quality of the project. Software quality means ensuring that software projects are reliable. Projects have well thought out tests that are performed at many levels to ensure that the project's components work together to meet the project requirements as well as verify the correctness of the components themselves. These tests are executed frequently, and a large number of them are used as gatekeepers, trying to ensure that only reliable changes are made to the project. When a project is released, the scenario coverage is 100% and the code coverage is either at 100% or whatever percentage the team has negotiated and documented for their project. Software quality means ensuring that software projects are maintainable. This entails sufficient documentation of project goals, architecture, design, and current state. The documentation is coupled with Static Code Analysis to measure a number of maintainability metrics and to gatekeep on most of them, ensuring that the project moves in a positive direction to a higher quality project. Software quality means ensuring that software projects and their processes are efficient. Team process to administrate and maintain the software and the software itself do not have to be blindingly fast, but they need to be as efficient as they need to be for that project and for that team. They do not need to be fast as lightning, only fast enough for the software project itself. Software quality means ensuring that software projects are secure. If third party components are used for the project, those components need to be monitored for vulnerabilities, and any issues that arise must be addressed quickly. Steps are taken, at a level that is appropriate for the type of software project, to reduce the possible ways that an user can use the software project do something malicious. To me, software quality is about the journey, continuously improving quality and showing that progress, while adding new features and fixing bugs at the same time. Wrapping It Up To put it succinctly, software quality for a project is about having a common nomenclature describing the various pillars of quality, having a common way of measuring against each of those pillars, and the publishing of those measures. Therefore, from my point of view, software quality is not a single metric but a collection of metrics and a philosophy. That philosophy is that your team can only really answer that question by having clearly defined goals for your project and it's quality metrics, and steering the project towards those goals. Does every project need to be super high quality? No, not even close. But I firmly believe that each project needs to have a solid understanding of what level of software quality they have in order to negotiate the definition of \"good enough\" for each project. In the United States, where I currently live, I am a Software Development Engineer in Test or SDET. I do not have an engineering degree. In any other country, including my native Canada, I am a Software Developer in Test or SDT. ↩ In the United States, where I currently live, a Software Development Engineer or SDE is the same as a Software Developer in any other country. ↩ Based on my experience, where the article breaks out Size as it's own pillar, I would place it in the Maintainability section. Similarly, while I can understand why they place Indentifying Critical Programming Errors in its own section, I would most likely fold half of the items into the Maintainability section and half of them into the Reliability section. To be clear, I agree with the content they present, it is just the organization that I disagree with on two small points. ↩","tags":"Software Quality","url":"https://jackdewinter.github.io/2019/09/15/what-is-software-quality/","loc":"https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"},{"title":"Static Websites: Getting Ready For Publishing - Themes and Minutiae","text":"This is the fourth article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous articles, I was able to verify my setup of Pelican, posting a test article that I was able to view in a browser. I was then able to improve the fidelity of that article by using Lorem Ipsum to make sure it looked more like a real article in terms of content and length. Almost as important, I was able to come up with a more efficient workflow for publishing changes as I work on them. To make the jump from authoring to publishing, there are a number of things that I needed to finish: Fixing The Build Warning File Types and File Paths Better Values For Defaults A Default About Page Selecting a Theme Once all of that was completed, I should be ready to publish… so let's proceed! Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Task 1: Fixing The Build Warning When the site was regenerated using the pelican-build.bat file, I noticed a warning at the top of the output. WARNING: Docutils has no localization for 'english'. Using 'en' instead. This is an easy one to handle. I went to the pelicanconf.py and changed the value for DEFAULT_LANG from english to en . Running pelican-build.bat again, the warning went away. This was simply changing the value from a human readable format to the ISO 2 letter code for the language, so it was an easy fix all around. Task 2: File Types and File Paths Even though I am creating a static website, there are various categories of content that I feel should be kept separate, for various reasons. Luckily, the contributors to Pelican though of this too, and the following change to pelicanconf.py separated the different forms of content: ARTICLE_PATHS = [ 'articles' ] PAGE_PATHS = [ 'pages' ] STATIC_PATHS = [ 'images' ] This configuration informs Pelican that articles will be contained within the content\\articles directory, pages within the content\\pages directory, and static content, such as images, in the content\\images directory. While this isn't 100% necessary at the moment, I feel that the organization will pay off later in the website's history. To complete this change, I moved the content\\test.md file into the content\\articles\\test.md directory to follow this paradigm. To make the paradigm more complete, I wanted to make sure that the articles that I write have the date they were created as part of their published path. Searching around the Pelican site itself, this was easily accomplished with the following change: ARTICLE_URL = '{date:%Y}/{date:%m}/{date: %d }/{slug}/' ARTICLE_SAVE_AS = '{date:%Y}/{date:%m}/{date: %d }/{slug}/index.html' Task 3: Better Values For Defaults The pelicanconf.py file contains two variables, LINKS and SOCIAL, which are still at their default values. Giving each a better value will give me a better idea of what things will look like with various themes, so it makes sense to change them now to: LINKS = ( ( 'Pelican' , 'http://getpelican.com/' ), ) SOCIAL = ( ( 'github' , 'https://github.com/jackdewinter' ), ) Task 4: A Default About Page In trying to determine what I needed before looking at themes, I noticed one small gap: I had no static pages. While I expect most of my content to by blog posts, there will be the occasional times where I want some kind of page on my site that isn't an article. The first type of page that came to mind was an About page, so I quickly created a new file content\\pages\\about.md with the contents: # About Me This is me . While it is just a placeholder, the intent was that it would give me a good idea of what to expect going forward. Sidebar 1: What Are Themes? In Pelican, extensions are performed using a combination of configuration, pip installs, and git repositories. The configuration changes and pip installs felt natural to me, as they are common paradigms in Python. However, I found the repositories as source took a bit of getting used to. Not too bad, but a bit of change that was good! Themes themselves are each an individual git repository, containing all of the asserts that the theme needs. Luckily there is a site that shows samples of approximately 80% of the themes. While it is a bit to process in one viewing, Pelican Themes currently contains 126 themes, of which 100 of them have images of themed pages. The better themes have 3-4 pages shown in the index, whereas some of the themes only have 1 image. Regardless, it is a lot better than downloading 100 themes and trying each one out! Task 5: Selecting a Theme At this point, I was pretty sure I had all of the assets I needed before I looked at themes. Sample articles. Check. Sample page. Check. Sample Links. Check. Sample social links. Check. It was time to start! Seeing as I hadn't actually read anything in people's blog posts about how hard it was to select a theme, I wasn't sure what to expect. As such, I budgeted a decent amount of time to the effort. In fact, I actually budgeted a whole week for this next section. For me, the choice of theme was pivotal in being able to communicate effectively with readers. To be clear, I didn't need it to be the right choice. I did need it to be the right choice for me and my voice. It was a clear proposition in my mind: come out of the gate with a theme that wasn't me, or take the time and try and nail it. Even if I missed by a bit, I wouldn't regret taking that time. If you are following these articles as a guide, remember that. Give yourself the time to make a solid choice that you believe in. Which Themes to Try? As I was trying to find a theme for myself, I went through the complete list of themes 3 or 4 times, just taking a look and seeing which ones appealed to me and which ones had ideas that I liked. I started a list on the first pass, and with each pass through the list, I whittled that list down based on what I saw. On the final pass, I focused on getting the number of themes down to a manageable 3 themes. For that pass, I found it important, yet difficult, to pare down the choices to 3 themes. It was important, because I didn't want to be stuck analyzing the themes for a long time. It was difficult because there are a lot of good options for themes, and to come up with only 3 options wasn't easy. However, I found that by focusing on my primary goal of ease of use from my first article , both ease of writing and ease of reading, it helped me narrow things down. The use of an actual list, in my case a simple list written in Markdown, was pivotal. At first it contained the URLs of each of the repositories I wanted to look at more. With each pass through the list, it contained more information about individual themes and a smaller number of themes, This approach helped to quickly whittle the selection down to the real contenders. This approach, while pedantic, saved me a number of times, as it became more of a struggle to remember which theme had which look and which features. With my final pass through the list, I arrived at my top pick of the nice-blog theme, with alternates of the blueprint theme and the elegant theme. Nice-blog is simple and uncomplicated, with a decent looking sidebar. blueprint is the theme for the site of Duncan Lock which I found during my research. blueprint had a bit more of a finished techy feel, with a nice layout on the sidebar. Each article had \"X min read\" text with the clock icon really grabbed me, which really appealed to me. Finally, the elegant theme seemed to keep things simple but elegant, with lots of room for creativity. Trying Out The Themes Each of the themes, Nice-Blog , Blueprint , and Elegant , exists in it's own GitHub repository. As such, one approach to downloading the themes was to create a blog-themes directory at the same level as the base project directory , creating a directory for each theme. As Nice-Blog and Elegant are in the main Pelican themes repository, the other approach for those two themes was to clone the Pelican Themes Repository into the blog-themes directory using: git clone --recursive https://github.com/getpelican/pelican-themes For the first approach, I would have to individually add each theme, whereas with the second approach, I can get most of the themes all at once. The was also the concern that regardless of which way I chose for nice-blog and elegant , I would have to use the first approach for blueprint . Was it worth it to have two paradigms? After thinking about it a bit, I decided to go with the first approach, as I only had 3 themes I was interested in. So, on the command line, I entered: mkdir ..\\blog-themes git clone https://github.com/guilherme-toti/nice-blog ..\\blog-themes\\nice-blog git clone https://github.com/dflock/blueprint ..\\blog-themes\\blueprint git clone https://github.com/Pelican-Elegant/elegant ..\\blog-themes\\elegant The plan was to try each of the candidates, writing down notes about what aspects I found good and bad about each. Following the instructions on the Pelican home page, I modified pelicanconf.py to refer to the first theme as follows: THEME = '%%%MY_DIRECTORY%%%\\\\blog-themes\\\\nice-blog' Save the file, switch to the browser and refresh. Check to make sure it changed properly. Look around the site a bit and write down some notes. Easy. While I would end up coming back to this theme later for more information, the first pass was a solid 5 minutes with no issues. Expecting similar behavior, I did the similar change to make the THEME variable point to the blueprint directory. I switched to the browser and refreshed and it was the same as before. A quick examination of the Builder command window, and I got the following notification: CRITICAL: TemplateSyntaxError: Encountered unknown tag 'assets'. Critical error… encountering that was foreboding. I stopped the windows started by pelican-devserver.bat , and buckled down to do more research. This was the start of a long diversion. Off The Beaten Path: Getting Blueprint to Work Note: I have not contacted the developer of the Blueprint theme, and his blog and his theme have not had any recent changes. When I decided to try it out, it was with the knowledge that it would probably require more effort to get it to work. I had hoped to wait for a bit before exploring plugins, as there are many plugins listed on the Pelican Plugins website. In addition, unless you know what you are looking for with a plugin, it's effects are either invisible or difficult to spot. For those reasons, I wanted to wait until the more major variables regarding the website were set before tackling these more minor ones. Adding Required Plugins After doing my research, it appeared that blueprint was dependent upon the assets plugin. While installing Plugins faces the same issue as how to install Themes, I chose to do the \"all at once\" approach for the plugins. The main reason for this was to allow me in the future to try each plugin, figuring out that plugins is worth the impact. As such, having all of the common plugins together made a lot more sense. Similar to the way described above to install all the themes, I created a blog-plugins directory at the same level as the base project directory . Changing into that directory, I issued the following command to pull the contents down to my system. git clone --recursive https://github.com/getpelican/pelican-plugins ..\\blog-plugins Once I had the contents of the plugin repository on my machine, I added configuration to Pelican to point to the new plugin directory. Then I needed to add the assets plugin to satisfy the blueprint theme. This was done by adding the following to pelcianconf.py : PLUGIN_PATHS = [ '../../blog-plugins/' ] PLUGINS = [ 'assets' ] Running the pelican-build.bat script this time, I received the following error, buried deep within the output: WARNING: `assets` failed to load dependency `webassets`.`assets` plugin not loaded. Plugins With Python Package Requirements Luckily, as part of the previous research, this warning was mentioned, and it was because the assets plugin requires the webassets Python package. A quick pip install webassets later, it's time to build the website again. This time, after running the build script, the output ended with the following lines: ... CRITICAL: ValueError: not enough values to unpack (expected 3, got 2) ... File \"XXX\\blog-themes\\blueprint\\templates\\base.html\", line 37, in top-level template code {% for anchor_text, name, link in SOCIAL %} ValueError: not enough values to unpack (expected 3, got 2) Configuration Changes For Plugins Once again, research to the rescue, this being a relatively easy issue. Different plugins and themes have different configuration requirements, and this one is no different. By looking at the error message, mixed with my knowledge of Python, I saw that the plugin is expecting 3 values for the SOCIAL configuration variable: anchor_text, name, and link. A quick look at my current configuration, and I see the default settings of: SOCIAL = ( ( 'github' , 'https://github.com/jackdewinter' ), ) represent the 2 values that the theme is expecting. Needing a third, I simply cloned the first value into the second position: SOCIAL = ( ( 'github' , 'github' , 'https://github.com/jackdewinter' ), ) Wash. Rinse. Repeat. Running the script again, I received the following critical error: ... CRITICAL: TemplateAssertionError: no filter named 'sidebar_date_format' ... File \"XXX\\blog-content\\virtualenv\\lib\\site-packages\\jinja2\\compiler.py\", line 315, in fail raise TemplateAssertionError(msg, lineno, self.name, self.filename) jinja2.exceptions.TemplateAssertionError: no filter named 'sidebar_date_format' More Configuration Changes This time, it took a lot of looking. I did scans over the entire blog-themes directory as well as the blog-plugins and blog-content directories. Just in case. In fact, I had almost given up hope when I started to look at the pelicanconf.py file that Duncan himself used. Down near the bottom were a number of configuration entries, including one for the missing configuration item. from datetime import date ... def month_name ( month_number ): import calendar return calendar . month_name [ month_number ] def custom_strftime ( format , t ): return t . strftime ( format ) . replace ( '{S}' , str ( t . day ) + suffix ( t . day )) def archive_date_format ( date ): return custom_strftime ( '{S} %B, %Y' , date ) def sidebar_date_format ( date ): return custom_strftime ( '%a {S} %B, %Y' , date ) def suffix ( d , wrap = True ): tmp = 'th' if 11 <= d <= 13 else { 1 : 'st' , 2 : 'nd' , 3 : 'rd' } . get ( d % 10 , 'th' ) if wrap : return '<span class=\"day_suffix\">' + tmp + '</span>' else : return tmp # Which custom Jinja filters to enable JINJA_FILTERS = { \"month_name\" : month_name , \"archive_date_format\" : archive_date_format , \"sidebar_date_format\" : sidebar_date_format , } Copying this into my own pelicanconf.py file, it was time to run the build script again. To be honest, for this section of configuration, I started only copying the sidebar_date_format function. Then I realized it needed custom_strftime . Then I realized it needed… It was at that time that I figured it was easier to just copy all of this code over, and if I stayed with the theme, I would see about cleaning it up. With the complete section copied over, running the build script produced the following error: CRITICAL: UndefinedError: 'pelican.contents.Article object' has no attribute 'stats' More Build Iterations Realizing that more of the configuration may be in the pelicanconf.py file, with the above error in mind, I scanned the configuration and noticed a plugin called post_stats . Looking at the documentation for post_stats , it seemed like it would expose that attribute. So, adding ‘post_stats' to the plugins, I re-ran the build, with the attribute error disappearing, only to be replaced with: ModuleNotFoundError: No module named 'bs4' Having looked at the documentation for post_stats , I was able to solve this one right away. To get the proper word count for the stats, the plugin uses the Python Beautiful Soup package (version 4) to scrape the HTML output. Executing pip install beautifulsoup4 , and then rebuilding again, we get the following output: File \"%%%MY_DIRECTORY%%%\\blog-themes\\blueprint\\templates\\footer.html\", line 41, in top-level template code {% for anchor_text, name, link in LINKS %} ValueError: not enough values to unpack (expected 3, got 2) This is the same error as with the SOCIAL variable, and the same solution will work. Changing the LINKS variable to: LINKS = ( ( 'Pelican' , 'Pelican' , 'http://getpelican.com/' ), ) And recompile and… all errors gone. Switch back to the browser and refresh the page. Repeat the entire process as with the nice-blog theme, taking notes. Back to the Final Theme Bracing for the worst, I changed the theme to Elegant and it… just worked. After blueprint , I expected a lot more effort, but it just worked. Counting my blessings, I did the usual playing around and taking notes, then I sat back. From then on, I went back to the three themes frequently and took more detailed notes and observations until I got to a point where I wasn't adding any meaningful notes. It was then that I was sure that I had the information I needed to make my decision. Lessons Learned Before I go on to my decision process, I wanted to go over some things I learned or remembered along the way of finding a theme. Some are obvious, some are not. The more standard things are, the less you are going to have to do to make them work. Blueprint was not in the main collection of themes, and it took a lot of effort to make it work in it's out-of-the-box mode. Elegant and Nice-Blog both worked out-of-the-box with 0-2 modifications to configuration and no extra packages or plugins. The auto-build feature of the generator can hang. There were a number of times I had to Ctrl-C and restart the generator. This seemed to happen more when I was changing the themes or plugins. It also happened when I introduced something that required a new Python package. I didn't regret putting in the work. I wanted to put the work in to each theme on the short list to have a balanced comparison. I wanted to have a good understanding of what it would take to maintain each of the themes. The work I put in gave me the understanding that blueprint would take a lot more work than the others to maintain. As old school as it is, a simple pros and cons list help me figure out the best theme. The first two themes were easy to keep in my head at first. The more themes that I saw, the harder it was to remember any points about each of the themes. By the time I got down to the last 2 themes, I had so many parts of so many themes going through my head. Without the list, I would have been lost. The Final Selection Due to the amount of work required for blueprint , including a number of limitations I encountered clicking around, it was out of consideration right away. One of my core issues from the first article was ease of use, and I didn't get the feeling that blueprint would fall into that category. Looking at the notes a number of times, it was hard to distinguish the two remaining themes from each other. They both had a simple and elegant look-and-feel, and that was making the decision even more difficult. There were a couple of small features that were different, but those missing from one were balanced by ones missing from the other one. It was pretty much a stalemate in my head. After going back and forth a number of times to try and resolve the stalemate, I decided I needed more criteria to help. The first two that came to mind were the documentation and active maintenance of the theme. This was returning to the ease-of-use consideration that was a driving force in my first article, so I knew it would be relevant. It was then that the elegant theme became the winner by a fair amount. So, after about a weeks worth of shifting between themes in \"spare time\", I landed on the elegant theme for my website. It took a couple of days for it to sink in while I was writing this article, but with each passing day, my confidence that I made the right choice increased. I took the time, examined the themes, wrote the notes, and in the end, it resulted in a single, clear choice. With finality I went to the configuration file, and near the top added: # Current Theme THEME = ' %% %MY_DIRECTORY %% % \\\\ blog-other-themes \\\\ elegant' What Was Accomplished At the beginning of this article, I had most of what I needed to start selecting a theme. It took some small updates to the configuration to make sure I had a good test site available. This was critical to allowing me to go through each theme I was interested in and see if it was for what I was looking for. While one of the themes proved to be a handful, the experience was good in advising me of possible issues I might have in customizing my own site. In the end, I made a strong, confident choice of the elegant theme, which as benefits, is actively being developed and has great documentation. What's Next? Now that I have completed all of the major tasks, the next step is to publish the website to an external host. For choices that will become evident, I will be publishing the website to GitHub pages. Completing this task will be covered in the next post Publishing To GitHub Pages . Epilogue: Jack's Notes on Themes In the previous sections, I mentioned taking notes on each themes. To do this, I used the main page, the about page, and the two sample articles as a litmus test. I clicked around the website, looking for things that worked and didn't work, as well as the feel of the website. I also went to the home repository of each theme and checked for how often it was updated, and what kind of support the repository offered for customization. In the end, I came up with three lists of items, with pros and cons, as follows: nice-blog pros starting point minimal but elegant look and feel categories support metadata for articles subtitle, cover image, gallery can change color easily cons doesn't seem to handle TOC properly no tags support simple tracking blueprint pros tags by name and frequency like \"4 min read\" nice blue date format Fri 3rd December, 2010 category and tag support origin of better figures plugin cons too limited otherwise no archives by default documentation for theme very limited elegant pros nice layout very elegant and simple feel category and tag support extensive documentation lots of metadata actively being maintained cons no \"4 min read\" support no links support","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/09/08/static-websites-getting-ready-for-publishing-themes-and-minutiae/","loc":"https://jackdewinter.github.io/2019/09/08/static-websites-getting-ready-for-publishing-themes-and-minutiae/"},{"title":"Static Websites: Posting My First Article","text":"This is the third article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction With Pelican installed, it was time to start creating stuff. The first I thing I wanted to do was to figure out if the base site was good by creating a default site and viewing it in a browser. Then I wanted to come up with a very simple test article, publishing it and viewing it to verify my ability to publish and exercise that workflow. Finally, I wanted to take some time to come up with a more representative test article and improve the publishing workflows. Similar to the last article, this is not about leaping forward, but to take a decent step forward in understanding the tools and processes. Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Step 1: Verify Our Basic Web Site The first thing I wanted to do is to make sure I didn't mess anything up in the last article, Setting Up the Pelican Static Site Generator . Therefore, it made the most sense that I verify that my website was up and running before I added my first article. After doing some research, it was evident that there are two phases that I need to take care of: building the content and viewing the content. Step 1a: Building The Web Site Content The first article's section on Static Site Generators (SSGs) hopefully made it clear that the focus of SSGs is to render the site content when updated. In the second article, Setting Up the Pelican Static Site Generator , I created the base configuration for a generic website, but did nothing to build that content. To build that content, I needed to find out how take the empty content and generate the output files from it. Looking at some articles, I was drawn to the output of the pelican-quickstart command that generated the base configuration. In particular, I noticed the prompt: > Do you want to generate a tasks.py/Makefile to automate generation and publishing? (Y/n) n I specifically answered No because of my research. If you answer Yes , the template will ask a number of additional questions related to publishing, and it will place the Makefile and tasks.py in the website directory. As my primary machine is a Windows machine, these two files are mostly useless. Their primary value would be to instruct me on what I needed to do to replicate their behavior on the command line or in Python. Thankfully, between the command pelican –help , using the Makefile for reference, and experimentation, I was able to arrive that this command for building the website: pelican --debug --output website\\output --settings website\\pelicanconf.py website\\content Nothing too fancy. I started Pelican in debug mode to have more information about what was going on. Using the above command, the output was placed in the website\\output directory, the settings were retrieved from the website\\pelcianconf.py file, and the content for the website is located in the website\\content directory. I double checked to make sure these were all correct, and it ran once without issue. To make sure that I didn't lose this command, I created a simple script pelican-build.bat in my base project directory, with the above command being the only line in that file. Pelican generated a lot of output, but in the end, it looked like everything was okay. I was excited to check out the new website, so I used my web browser to open the file website\\output\\index.html and got this result: I must admit… it was kind of underwhelming. And then I thought about it a bit. A lot of the stuff that makes web pages powerful are other parts that included by the web page, and may not be loaded if the page is loaded directly from the file system. To properly view the content, I was going to have to host it somewhere. Step 1b: Viewing The Web Site Content If you are like me, hearing or thinking the phrase \"I need to install my own web server\" fills me with read almost right away. Even for a pared down web server, there are usually 3-6 directories to set up, ports to clear with firewalls, and other little things. I must admit, when I started looking around, I was not exactly in a good mood. However, the first 3 websites that talked about Pelican were from the Pelican project's various versions. Looking deeper into the documentation, I found the part of the documentation titled Preview Your Site . Without taking too much away from the documentation, it specified that Python 3 includes a pared down web server that is available for simple uses, like I needed. After a bit of experimentation and fiddling, and I came up with the following lines and placed them in pelican-server.bat : pushd website\\output python -m pelican.server popd Executing that script, I then saw the following output: XXX\\python\\python37-32\\Lib\\runpy.py:125: RuntimeWarning: 'pelican.server' found in sys.modules after import of package 'pelican', but prior to execution of 'pelican.server'; this may result in unpredictable behaviour warn(RuntimeWarning(msg)) WARNING: 'python -m pelican.server' is deprecated. | The Pelican development server should be run via 'pelican --listen' or 'pelican -l'. | This can be combined with regeneration as 'pelican -lr'. | Rerun 'pelican-quickstart' to get new Makefile and tasks.py files. -> Serving at port 8000, server . ... While this is what the official documentation suggests, it does look like it is out of date. Using the output from above, the installed Makefile for guidance, and more experimentation, I replaced the contents of the pelican-server.bat file with the following: pelican -l -p 8000 --debug --output website\\output --settings website\\pelicanconf.py website\\content This time, the output of the script was: DEBUG: Pelican version: 4.0.1 DEBUG: Python version: 3.7.3 DEBUG: Adding current directory to system path DEBUG: Temporarily adding PLUGIN_PATHS to system path DEBUG: Restoring system path WARNING: Docutils has no localization for 'english'. Using 'en' instead. -> Serving at port 8000, server . To me, it looked like the web pages were served up properly, or at least Pelican didn't report any errors to the screen. The only thing left was to actually see if it would allow me to load the generated website to my browser. Instead of loading the file directly into the web browser as before, I entered \"localhost:8000\" in the address bar. To be honest, I took a guess that server . meant the localhost. This time, I got the following result: Unstyled, no unique content, and only locally visible. It's not a great result… but it's a start! Step 1c: Commit Changes Before I went forward, I wanted to save the current state of the website, so back to Git and the git status -s command, whose output was now: ?? pelican-build.bat ?? pelican-server.bat ?? website/__pycache__/ ?? website/output/ In my head, I compared these results to the changes I made: I added 2 script files to help me build and serve the content both file are detected, and I want them as source in my project I built the website, placing the output into website/output the directory was detected, but I want to generate this output, and not persist it from previous experience, the website/ pycache / contains pyc files that are built when the python scripts are interpreted the directory containing these files was detected, but these should never be persisted Using thought processes derived from what I documented in the section Step 5: Committing the Changes from the previous article, it was clear that I needed to git add pelican-build.bat and git add pelican-server.bat to persist those changes. However, the two items that I did not want to persist would require different tacks for each one. The first case, website/output/ , is a directory like the virtualenv directory in the section # Step 2: Create a Project Directory For The Site from the previous article. Therefore, I edited the .gitignore file to include the directory by name. That was the simple one. The second case was more complex, the website/ pycache / directory. This directory only exists to contain compiled Python byte code designed to speed up execution on subsequent passes. If I add the directory, as with the first case, it only takes care of that one directory. If I run any Python scripts from other locations, I will have to add those directories too. This was not an efficient option, hence I edited the .gitignore file to ignore the files themselves, by specifying &ast.pyc as the pattern to ignore. Therefore, following those changes, the .gitignore file looked like: virtualenv/ website/output/ *.pyc Using git status -s , I verified that only the 2 scripts were being added, as well as the .gitignore file itself being changed. Quickly adding git add .gitignore , I then used git commit -m \"my message\" to commit these changes to the repository. So let's take inventory. We have a basic website up and running, we have tested it in our browser, and we have committed any changes to our local repository. It's definitely time to write our first article. Step 2: The First Article Looking through the Pelican Documentation, I found another good section on Writing Content . From here, I learned that certain Markdown processors, such as the Python implementation, support metadata on files. As a base example, they specify this as a sample post: --- Title : My super title Date : 2010 - 12 - 03 10 : 20 Modified : 2010 - 12 - 05 19 : 30 Category : Python Tags : pelican , publishing Slug : my - super - post Authors : Alexis Metaireau , Conan Doyle Summary : Short version for index and feeds --- This is the content of my super blog post . Reading further, a lot of the metadata fields have defaults, but the Title metadata is required for the page to be picked up and processed by Pelican. That got some wheels turning in my head, but I put it aside for a later article. First I wanted to have a solid way of writing the articles before adding to that. Taking the markdown content from above, I created a file in the website/content directory called test.md and placed the content in there and saved it. Following the same pattern I used for the base content, I ran pelican-build.bat and then pelican-server.bat to view the content, providing the following: Granted, its a stock article, but I now had a way to publish my articles! Along the way, I noticed a couple of things I noticed needing improvement. Step 3a: A More Streamlined Build/Preview Workflow In the previous section, the workflow I used took 4 actions to go from something I wrote to something I could see and validate: save the file, run pelican-build.bat , run pelican-server.bat , and refresh the web page. Wondering if there was a more efficient way to do that, I checked back on the Makefile that I used as the inspiration for the two scripts, and noticed a devserver entry. As the devserver entry only differed from the server entry by the introduction of the -r flag, I quickly created a pelican-devserver.bat with that simple change, and executed it. Within seconds, I got the feedback: CRITICAL: TypeError: can't pickle generator objects Weird error, but understandable. In Python, the default serialization of objects to be passed outside of executables is referred to as pickling . If Pelican was already hosting the server and wanted to spawn a process to rebuild the website, it would make sense that it pass the current configuration to that new process. Doing a bit more searching, I came across this issue logged against the Pelican project in GitHub. There are a fair number of entries for this issue, offering various pieces of advice. The low down for me is that due to the way pickling works, I cannot use it on my Windows machine with that given program. Reading further in the comments for the issue, a number of people did mention that the -r flag works fine when applied to the command I am using for building the pages, just not for building the pages from the server. Trying it out, I renamed pelican-devserver.bat to pelican-autobuild.bat , and switched the file contents to: pelican -r --output website\\output --settings website\\pelicanconf.py website\\content Based on what I read, this should have my desired effect, and a quick modification of the test.md file confirmed it. When I saved the file, the generator detected it and rebuilt the contents. However, thinking through things a bit more, I wondered if the Windows start command would help build a better solution. When I write a Windows batch file and I need to execute another batch file, I use the call primitive. This allows me to run the new batch file within the current batch file's shell. When that new batch file completes, I can check the return code from that script, and determine what action, if any, to do next. By structuring batch files and scripts this way, I find that I assemble things together more quickly, keeping the batch files and scripts more readable and more maintainable. The start primitive is almost identical to the call primitive, with one small exception: the new program is started in a newly created shell spawned from the current shell. As the current shell and the new shell are separate processes, by default, both shells operate independently of each other, unless the /wait flag is specified. While this is not ideal for composing batch files, this behavior seemed to fit what I was trying to achieve. Creating a new version of the pelican-devserver.bat file, I placed within it: @echo off start pelican-autobuild.bat start pelican-server.bat And executed the batch file. Two full size shell windows popped up, each one running one of the scripts, with the original shell window left alone. A new modification to the test.md file resulted in the rebuild happening in the window running pelican-autobuild.bat . A quick refresh of the browser page, and the newly changed article appeared in the browser. I was mostly where I needed the script to be. Further tweaking the contents of pelican-devserver.bat to: @echo off start \"Builder-Pelican\" /min pelican-autobuild.bat start \"Server-Pelican\" /min pelican-server.bat got me to where I wanted to be. To keep things clean, I wanted to specify a title for each shell window and I wanted to start them minimized. Thus, by default both windows are out of the way, but if I need them, I can find them quickly. Side Note: If you are using this as a guide, note that this only seems to be an issue on Windows machines. In quick tests I did using an Ubuntu image running under Docker, I did not see this issue, and the served pages updated properly. Step 3b: A More Representative Test Article The first thing I noticed about my test article is that it was short. Really short. When I posted it, it was hard to notice where the test article was being summarized, and where it was just the article. I needed to come up with a test article that would be a better representation of an actual article. For that, I dug back into my testing experience. When coming up with test information for test projects, I have used the site Lorem Ipsum and sites like it for years. The site goes into more details, but by using this text it allows someone viewing the text to focus on what is around the text instead of the text itself. This is also better than using random words, as someone viewing that random text may try and find patterns in the paragraphs, once again distracting from what is around the text. After a couple of quick clicks on the site, and it generated 5 decent sized paragraphs of random text that started with: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc eget velit porta, efficitur justo at, sagittis nulla. Donec neque arcu, condimentum sed massa a, elementum rhoncus justo. ... I didn't include all 5 paragraphs here, as it was randomly generated. If you are using this as a guide, you can easily generate your own test paragraphs and insert them into your test message. However, when the page was regenerated, this is what it looked like: Also, to give a bit of contrast, I created a copy of the page, called it test-2.md , changed the title a bit, and removed all except the first two paragraphs. My reasoning behind this was to make sure I had a sample that was long and a sample that was short. Step 3c: Commit The Changes As always, before moving on, I needed to make sure I committed the changes. This one was easy, simply performing the git add action on the files test.md , pelican-autobuild.bat , and pelican-devserver.bat , followed by a git commit . What Was Accomplished At the beginning of this article, I only had the foundation for a basic website. The first thing I did was to make sure I had a good workflow for generating the website and viewing it in a browser as a website. After that, I added a sample article, and also improved the original build/preview workflow. Finally, I created more realistic data for the test article, so I could see what a normal article would look like. As for the goal of creating a test article that I could use to start fine-tuning the site, I believe I am in a good place. So next, fine-tuning and selecting a theme! What's Next? Next, I need to clean up a couple of small things before selecting a theme for the website. Getting a solid choice for a theme is the last major task I have to complete before publishing the website itself. Completing this last major task will be covered in the next post Getting Ready For Publishing - Themes and Minutiae .","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/09/01/static-websites-posting-my-first-article/","loc":"https://jackdewinter.github.io/2019/09/01/static-websites-posting-my-first-article/"},{"title":"Static Websites: Setting Up the Pelican Static Site Generator","text":"This is the second article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous article, I decided that the Pelican Static Site Generator (SSG) was the right choice for me. This article goes through the steps that I took in setting up Pelican on my system, providing a play-by-play for anyone thinking of doing the same thing. Feel free to use this as a guide for setting up your own website to experiment with. If you are not using this as a guide, I hope it provides you with any details that you require for regarding Pelican and it's setup. From my experience, it is more important for me to make a small step forward and lay a good foundation for what comes next than it is to take leaps and bounds and miss things. Hence, I feel that focusing on the setup of Pelican is a good and properly scoped step. Why Another Pelican How-To-Guide? In looking around, there was a real mish-mash of articles out there: Making a Static Blog with Pelican Using pelican to generate static sites on windows Creating a Blog on GitHub.io with Python Creating your blog with Pelican How to Create Your First Static Site with Pelican and Jinja2 The first thing that was obvious to me was that all of the Pelican posts I found were written in 2017 or earlier. This means that these articles refer to versions of Pelican before the current 4.0.1 release that I am running, so they are either out of date or inaccurate. The second thing that was obvious was there were very few posts written about using Pelican on a Windows machine. According to the site NetMarketShare.com , Windows machines account for over 87% of the desktop machines surveyed. While it is true (from experience) that developers prefer Macs over Windows, projects like WSL are starting to chip away at those reasons. And it still remains that for a non-developer, Windows is by far the most common machine type. As it is my primary environment, I want to make sure it is represented. Component Versions If you are using this as a guide, you may try versions of any listed component other than those specified, but note that Your Mileage May Vary . This article was written from my detailed notes of how I set up my website, using the component versions listed. As such, if you experience any issues, I would fall back to those versions as an established baseline. If you would like to try other versions of components, I strongly encourage you to go to the baseline established in this article and commit it to your local repository (see Step 5: Committing the Changes below). Once you have that point set and committed, you can then try and use other versions of the components, having a known state to return to. Regardless, keep detailed notes about what you try, and if you find yourself in a bad state, fall back to one of your previous known states and try again. Operating System Paths From long experience as a developer, there is virtually no sematic difference between pathing that is meaningful. Windows uses a format of C:\\this\\is\\my\\directory and Linux systems use /my-mount/this/is/my/directory . I personally work on both types of systems equally and do not have any issues switching back and forth between them. One reason that I enjoy using Python over PowerShell, Bat/Cmd, and *Sh shells, is that I can properly obfuscate any such issues in my code. Python scripts can easily be written that are platform agnostic, eliminating duplicated scripts to handle platform issues. Add in to that additional support from editors such as VSCode and PyCharm, and it becomes a powerful scripting language with some real muscle behind it. While I realize others may feel differently, I expect the reader to be able to perform the same translation task while reading, with practice if required. Step 1: Install Pre-requisites There is one pre-requisite for Pelican itself, and that is having Python 3.7 installed with a current version of Pip. The information in this post was generated with Python 3.7.3 and Pip 19.1.1 . As I have a great habit of fat fingering commands, I prefer to keep most of my files in a version control system, specifically Git . While I use Source Tree as a graphical interface to Git, I realize most people use the command line. Therefore, for the purpose of any examples, I will use the Git command line interface, assuming that anyone reading this who uses a Git GUI can find the appropriate command in their GUI if needed. The commands I will be using in this article are as follows: git init - Create an empty git repository. git status - Working tree status of the current repository. git add - Add a new file to the working tree. git commit - Commit any changes to the working tree to the repository. If you are new to Git or need a refresher, please go to the links about and do some searching online for examples. These are some of the base concepts of Git, and should be understood before proceeding. With respect to how to install Python and Git/SourceTree, there is plenty of information on how to install those programs for each platform. Please google Python install , Git install , and SourceTree install for the best installation instructions for a given platform. For the Windows system I am using, I simply downloaded the installations from the websites linked to in the previous 2 paragraphs. After installing Python on my system, the installation of required packages was very simple. At the command prompt, I entered the following line: pip install pip==19.1.1 virtualenv==16.6.0 The two packages installed were the Pip Package Manager itself and the virtualenv package. The first installed package, pip, makes sure that Python's own package manager is at the specified version. While pip does not often change meaningfully and I am not planning on using any new features, it usually pays to keep things current. The second one is a virtual environment system for Python. Virtualenv is a Python tool that allows you to isolate a given Python project. While it is not portable between different systems, it does provide for a manner in which to isolate different versions of Python and different versions of Python packages from each other. Using virtualenv will allow me to install a specific versions of Python and each package with no fear of any global changes affecting this project. To verify that I have the correct version of pip and virtualenv installed, I executed each tool with the –version parameter, expecting to see output similar to the following: XXX> pip --version pip 19.1.1 from XXX\\lib\\site-packages\\pip (python 3.7) XXX> virtualenv --version 16.6.0 After these pre-requisites were installed, I was ready to create a directory to host the content for the website. Step 2: Create a Project Directory For The Site Before generating content, I needed to create a place to install Pelican that was isolated and self-contained. As mentioned in the previous section, that is exactly the use case that the Virtualenv program was created for. That is the first of the two tools that I needed to set up for the new directory. The other tool is version control, to ensure I can replicate and version the website. For this purpose, I use Git. The first thing this accomplishes is to ensure that if the computer hosting the website content gets destroyed, I still have the latest information about the website. The other thing that this accomplishes is to ensure that if I make a change (or two, or three) to the website that I don't like, I can always return back to previous versions of any of the documents. That out of the way, I selected a directory as a location of the website. In my case, I keep all of my local Git repositories in a directory c:\\enlistments , so I created the directory I wanted to keep the website in was c:\\enlistments\\blog-content . The location is totally arbitrary, so for the sake of clarity, if I refer to this directory indirectly, I will use the term base project directory . If I refer to this directory directly in a script, I will use the pattern %%%MY_DIRECTORY%%%. To create the base project directory, I executed the following commands in order: mkdir %%%MY_DIRECTORY%%% cd %%%MY_DIRECTORY%%% git init virtualenv virtualenv virtualenv\\scripts\\activate.bat In order of execution, the commands first create a directory and then changed the current directory to that directory. Once the base project directory was created, git init was used to create an empty Git repository with nothing in it, ready for the project to populate. Next, virtualenv virtualenv was used to create a virtual environment for the website, housing that environment in the virtualenv directory of the project. Finally, the activate script of virtualenv was executed to enable the virtual environment. The script activate.bat on my Windows platform ( activate.sh on Linux platforms) performs two simple tasks: change the shell's path to use the virtual environment AND change the path to make sure that change is evident. To be sure, I checked the PATH environment variable to make sure it starts with the Python path of the project's virtual environment and that the prompt started with (virtualenv) . Note that while I used git init to create a local repository, I was still getting started with the project. As such, I didn't need to worry about ensuring that the local repository is reflected in a remote repository. At that point, the purpose of having the local repository was to ensure that I could see what changed and revert back to previous versions if needed. If you are using this as a guide, please note that from this point forward, any commands that I entered were entered in the virtual environment shell. If for some reason you close your shell and need to restore the shell to where you were, you will need to open a new shell and submit the following commands: cd %%%MY_DIRECTORY%%% virtualenv\\scripts\\activate.bat I had a good directory ready to go, but I had one small issue to fix. When I submitted the git status -s command, I encounter the output: ?? virtualenv/ As mentioned above, the virtual environment is specific to a given operating system and version of Python. Because of this, committing the virtualenv directory to Git didn't make sense, as it contains system specific information. Luckily, this issue was easily addressed by creating a file in the base directory called .gitignore with the contents: virtualenv/ The format of .gitignore is pretty simple to understand. In my case, I only wanted to ignore the virtualenv directory off the base project directory, so I just needed to add that directory to the .gitignore file. Submitting the git status -s command again, I then saw the output of: ?? .gitignore This showed me that Git is ignoring the entire virtualenv directory, instead showing the .gitignore file that I just created. Since I have only done limited setup in the base project directory, having only the .gitignore file showing up as untracked is what I expected. To be safe, I used the git add and git commit commands to save these changes as follows: git add .gitignore git commit -m \"initial commit\" The directory was ready, time to focus on Pelican. Step 3: Install Pelican Pelican itself is installed as a package using Python's Pip program. Based on information from Pelican's Installing page , both the markdown and typogrify package are useful, so I installed them as well. The markdown package allows for content authoring with Pelican using using Markdown. Using a simple text file, special annotations can be placed in the text that alter how it will look when rendered with a Markdown processor. The full range of Markdown annotations and their effects are shown here . As this is one of the requirements I established in the previous page , this support was critical. The Typogrify package \"cleans\" up the text to make it look more professional. It accomplishes this by wrapping certain blocks of text in HTML span tags to allow for CSS styling. In addition, it replaces certain sequences of characters with other sequences that add polish to the finished document. While not required to get the site up and running, I figured it would be of use later. To install these three packages, I submitted the command: pip install pelican==4.0.1 markdown==3.1.1 typogrify==2.0.7 This resulted in the installation of any dependent packages, is essence, going from the stock packages (displayed using pip list ) of: Package Version ---------- ------- pip 19.1.1 setuptools 41.0.1 wheel 0.33.4 to: Package Version --------------- ------- blinker 1.4 docutils 0.14 feedgenerator 1.9 Jinja2 2.10.1 Markdown 3.1.1 MarkupSafe 1.1.1 pelican 4.0.1 pip 19.1.1 Pygments 2.4.2 python-dateutil 2.8.0 pytz 2019.1 setuptools 41.0.1 six 1.12.0 smartypants 2.0.1 typogrify 2.0.7 Unidecode 1.0.23 wheel 0.33.4 To make sure that I would be able to keep these libraries at their current versions in the future, I needed to take a snapshot and save it with the repository. Thankfully, this is a scenario that the Pip contributors though of. On the command line, I typed in: pip freeze > requirements.txt This command produces a terse list of each package and it's version, which I redirected into the file requirements.txt . The benefit to doing this is that, at any time, I can execute the following command to restore the packages and versions: pip install -r requirements.txt Step 4: Create a Generic Web Site Now that Pelican is installed and ready to go, I needed to enact the templating system of Pelican to form the basis of my website. The authors of Pelican have kept this simple. All I needed to do is run the command: pelican-quickstart During the installation, I was asked a number of questions: > Where do you want to create your new website? [.] website > What will be the title of this website? Jack's Web Site > Who will be the author of this website? Jack De Winter > What will be the default language of this website? [English] > Do you want to specify a URL prefix? e.g., https://example.com (Y/n) n > Do you want to enable article pagination? (Y/n) > How many articles per page do you want? [10] > What is your time zone? [Europe/Paris] America/Los Angeles > Do you want to generate a tasks.py/Makefile to automate generation and publishing? (Y/n) n Done. Your new project is available at %%%MY_DIRECTORY%%%\\website For a decent number of the questions, the defaults were sufficient. The questions that I answered specifically were: website directory keep the website isolated in a subdirectory, for future use title something simple for now author my name here url prefix No, we can change this later time zone I didn't know this, so I entered in something silly, and it gave me an URL to a web page where I looked it up generate no, I will provide simple scripts for that When this was completed, my base directory had a new directory website/ which contained 2 files and 2 directories. The first file pelicanconf.py had the settings that I entered using pelican-quickstart . The second file publishconf.py has any remaining settings that Pelican will use when publishing. If things change with the website, I just need to change the settings in these files, and the next time I publish, they will be in effect. The 2 directories are the key here. The content directory was created to contain any files that are the source parts of the website. During the publishing action (covered in a subsequent post), Pelican will translate that content into output and place it in the other directory that was created, output directory. Step 5: Committing the Changes At this point, it was useful to use git add and git commit to commit what I did to the local Git repository, as there was useful progress. Entering the git status -s command, it reported that the only meaningful changes were that the website directory and the requirements.txt file were added. As both of these objects are considered source and configuration, I added them as follows: git add requirements.txt git add website git commit -m \"Base website\" If there was something that was added that was not part of source, configuration, or documentation for the website, I would have edited the .gitignore file to include a pattern that cleanly removed those changes from Git's preview. When this comes up in future articles in this series, I will point out what I added and why. What Was Accomplished Having decided that SSGs were the correct paradigm for my website, and Pelican the correct SSG for me to use, it was time to set it up. I documented how I installed various components, as well as how I set up the base project directory for the website itself. Finally, I created a default website as a solid foundation for my purposes, and made sure that I committed the base project directory to Git for version control. In my professional career, most of the time it is advantageous to build a foundation for your application, committing it to version control often. Having provided that foundation in this article, I can now proceed with the actual building of the website. What's Next? Next, I will start with publishing a simple file to the website and make sure I check it online for any errors. By providing realistic samples and getting experience with the publishing workflows, I will get invaluable information on using Pelican. This task will be covered in the next post Posting My First Article .","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/","loc":"https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/"},{"title":"Embracing Something Hard","text":"If you look at my LinkedIn profile , you'll see that I have been around for a while. I was a Software Developer for many years beginning in 1991 at a small 2 person company in Waterloo, until a stint in Denmark gave me a bit of a wakeup call. When I came back to the United States in January 2011, I changed from a Software Developer to a Software Developer in Test. 1 While I started my career \"making things that work\", after 20 years I was more focused on \"making things work better.\" That career change was a wonderful and happy change for myself that I have never looked back. To be specific, I am not a tester. I do know a lot about testing, but that is not my primary focus. My focus is looking at interconnected systems and figuring out how to improve them. It is not about breaking things, like a lot of people assume, but making the team around me better. It is about standing up and mentioning that having 5 developers struggle through the same setup process for 4 days each is just plain inefficient. It is about standing up and mentioning that having 5 developers struggle through the same setup process for 4 days each is just plain inefficient. Solution: Add a responsibility for documenting the setup process for the first person, with each subsequent person responsible for the process of updating the document to current standards. Benefits: The team owns the process and its updating, with a clear path of responsibility if it doesn't work the next time it is used. It is about looking holistically at a group of systems and, while developers enjoy being creative, focusing that creativity on the parts of the systems that will benefit them and the team the most. Solution: Use templates and common libraries where possible to reduce the amount of \"creativity\" needed for anything remotely common to as close to zero as possible. Benefits: The team spends time on stuff that needs to be solved and not stuff that has already been solved. It is about asking each member of the team what they expect of documentation from the projects that their projects rely on. Solution: Setup a set of \"how-to\" guides that documents common practices for the documentation of projects for the team, hopefully with a \"dummy\" project that gives a live interpretation of those guides. Benefits: The team writes a set of documentation for their systems that looks like they were written by the same team, instead of a mish-mash of people thrown together. My job is as much about asking the simple but tough questions, as it is about having an idea on how to respond to both the questions and answers that follow those tough questions. The actual job is almost always about automation or process, it almost always involves changing the way people look at things, and unfortunately it almost always includes having someone's ego bruised along the way. Partially due to me having Autism, I can see certain patterns very easily, almost as if I was reading a book. Changing the way developers look at things almost always brings around the bruised egos. A lot of developers associate code they write with themselves. As such, saying that the code or process can be improved becomes confused with saying that the developers themselves can be improved. And yes, when that happens, it is often the people asking the questions and making suggestions on how to make things better that take the brunt of the anger that results. I still remember my daughter asking me one time why I liked being a software developer in test, as I am often frustrated with people over a perceived lack of momentum. Thinking about it quickly, the immediate answer was easy: I am good at a decent portion of it. If you are in a box and looking around you, all you see is the box. I am able to elevate my perspective to a higher level and not only see the one box, but the boxes around it. I can see how they are stacked, and if they are looking like they will tip over. That came second nature to me. But it wasn't the complete answer. Even as I responded with that answer to my daughter, there was something missing in that answer, and it bothered me when I thought about that conversation over the next couple of years. It was years later during one of those teaching moments we as parents have with our children that it occurred to me. I was reminding one of my children that we have a saying in our family: being knocked flat on your ass is not the last step, it's just the step before we pick ourselves up, brush ourselves off, and try again. Yeah, having issues and making mistakes sucks, but they helped make us who we are, and it's how we stand up again that defines us. It was then that I realized: I became a software developer in test because it was hard. I wanted the challenge to make things better, and to help others get better at what they were doing. I knew I was going to encounter stubborn people along the way, but I was determined that I would try and figure out a way to get through to them. Sure, I get knocked flat on my rear end a fair number of times, but I always get back up and try again. And it wasn't just the other people, it was myself. I had to learn when to strive for perfection and when to strive for \"just good enough\". I had to learn to find the balance between what I felt was the right decision and what the right decision was for the business right now. I had to learn that while my own passion and vision were wonderful, unless I was able to share those things in a way that others were receptive to, they meant nothing. I had to learn to get in a state of continuous learning. After all that time, I finally had my answer: I liked being a Software Developer in Test because I was good at it and because it was a hard challenge that forced me to learn and grow. That takes me to the last couple of months. For a long time I have wanted to start my own blog and help out an open source project. I was under no illusion that either objective was going to be easy, I just didn't have a clue about how different it would evolve into from what I thought it originally was. As I was going through and picking out a platform for my blog, I kept notes and started to turn them into articles. That was relatively easy. Or at least the first pass was. I found out that when it comes to articles, I want to make sure I am saying the right thing in the right way, and can literally spend 45 minutes working on one sentence. Shortly after that, I also learned that I can spend 5 minutes getting said sentence in a way that makes sense, add text a marker like **TBD** before it, and then come back to it at the end of the article. And yes, following that, I realized that about half the time, going downstairs and doing something totally unrelated caused me to think of THE EXACT phrase that I needed within seconds of coming back after the break. Yup, learning is fun, and hindsight is perfect! This blog isn't hard in terms of writing for me, but the production of it sometimes gets me. If you want to stay on track, you have to give yourself some kind of schedule. For me it was to publish once a week on something technical. It is a challenge to always make sure you have a couple of articles on the go at any time, and that you can polish one up and publish it on a weekly basis. I also have to balance the writing with exploring stuff so that I can write about it in my blog. And I realized I have to extend that out 4-6 weeks to give me time to go through a good ideation process. In picking a theme for my website, my attention was drawn to the Elegant theme for its simplicity and crispness. Looking into the documentation a bit, I noticed that some things were close, but not spot on. I wanted to get one of those features working for my website, so asking for some help getting it working, I changed the document to better document what needed to be done. The change was welcomed, and I volunteered to help out with any other articles. That is how I started contributing to the Elegant theme. What does it entail? Take the work I am doing for my blog articles, subtract the subject matter research, in certain cases add some localized research, and supplement that with making sure I write the articles in a more professional and international. 2 On top of that, apply a bit of my developer in test training and try and make sure I have a solid theme, and that I am making the process easier for me and other users of Elegant in the process. For sure, doing these things at the same time can be nuts, but I am thoroughly enjoying the challenge. I am growing both personally and professionally as I work though things on each project, some inside of my expertise and some outside of it. Sometimes I wish there were more hours in the day, but I wouldn't trade the learning I am doing for the world. Yeah, it's quite often hard, but it wouldn't be worth it if it wasn't hard, would it? In the United States, where I currently live, I am a Software Development Engineer in Test or SDET. I do not have an engineering degree. In any other country, including my native Canada, I am a Software Developer in Test or SDT. ↩ I will probably write an upcoming article about this to explain it fully. In essence, if you are writing in English for an international audience, you have to remember that a fair percentage of your readers are not native English speakers. Until they reach a point in their English proficiency, they will typically think in their native language and translate what they are reading from English to that native language. As such, you want to keep your language as free from idioms and imprecision as possible. ↩","tags":"Personal","url":"https://jackdewinter.github.io/2019/08/18/embracing-something-hard/","loc":"https://jackdewinter.github.io/2019/08/18/embracing-something-hard/"},{"title":"Static Websites: Choosing a Static (Web) Site Generator","text":"This is the first article in a series about setting up my own website using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction Why do I want a static website? It has been on my mind for a couple of years that I would like to talk about some of the things that I find cool and other things that I feel strongly about. As a big proponent of servant leadership , I want to help and inspire people to do more, be more, and help more. Basically, I want a low-friction, easy to use platform that allows me to communicate with and help others in my own way. I also want tooling and workflows to be efficient so that I can spend more time communicating and less time fidgeting on the website. To this end, I want to focus on something that is simple, but extensible. I want to be able to have a good mix of my own pages and a blog, allowing for flexibility on what I am doing. I want to be able to focus on the message and my voice, rather than the medium. From my research, Static Site Generators fulfils those requirements in spades. But as with a number of things I do, I want to take the time to determine if they are the right choice for my website, and if so, select the right one for me. I would rather take a small amount of time now to ensure it is a good fit, than to take time reworking things because it wasn't. What are Static Site Generators? Static Site Generators (SSGs) are programs that take a certain type of website and shift the heavy loading of the website from the content-request point to the content-update point. Put another way, the decision of what content to show the viewer of the web page is changed from when the web page is requested by the browser to when the web page content is updated by the author. Therefore, to be eligible to use a SSG for a given website, that website must have all its content available when the content is updated, with no real time content present. There are small ways to get around that restriction, but those present their own challenges. As such, I am going to assume that 95% of the website is going to be static, and that all the important information is within that 95%. SSGs themselves usually combine different technologies to achieve their results: writing, templating/styling, metadata, and publishing. The pages for the site need to be written in either a supplied WYSIWYG editor or in some format like Markdown. Once the pages are written, generic styling and templating are applied to the collection of pages to make sure that the branding on the pages is consistent. To help organize the pages, most SSGs include some form of metadata that is attached to the pages, used by the SSG to group pages and provide other guidance to the SSG itself. Finally, when the pages are checked over, there needs to be a simple way to publish them to the intended website. Reducing the barrier to entry and keeping it low is essential to a SSGs success. Therefore providing multiple technology choices for parts of the SSG is common. All the SSGs that I looked at had a variety of options for most of these technologies, with templating and styling being the exception. In the case of templating, most of the SSGs support a single default templating engine that is used on all the pages. With respect to styling, common HTML technologies such as Cascading Style Sheets (CSSs) are most commonly used. Besides keeping the cost of the templating and styling low, using a common technology allows for lots of examples to be provided by the Internet at a low cost. Is a Static Site Generator The Right Choice For Me? In determining whether SSGs are the right technology for my site, I started making a list of the things I was primarily worried about with a website: Security As I deal with security concerns at work, this one was top of mind. How often have I heard about various platforms and websites being hacked in the last month? To avoid this, I want to keep my website as simple as possible. Static pages created by a SSG leave a single attack vector: the repository where my website's HTML pages are being stored. Mitigating attacks by only having a single attack vector is very attractive solution for me. Ease Of Use I don't want to be messing with the website more than I have to. I either want to be updating something on the site in response to something I am working on, or working on that thing. I am already used to writing documentation at work in Markdown, so writing web pages in Markdown is already very efficient for me. In a similar manner, at work I keep documentation in a version control system, so keeping the pages in their various stages of completion in a readable form in version control is also efficient. Using my already existing workflows, with minor modifications, is an acceptable solution that keeps ease of use high. Real-Time Contents and User Input A lot of my focus at work is making sure that we can anticipate and plan for events that happen to change our system's state. If a user or another system sends a request to our system, can we validate the request, verify the system is in a good state for the request, and enforce the correct behavior for the response? Any good SSG takes care of this issue by eliminating any change of state. Resolving the concerns of state management by removing all state information seems like an ideal solution for the limited scope of my personal website. Once those were out of the way, only the big question remained: Do I have any plans for a web site that would require it to support dynamic content? My primary purpose is to allow me to communicate to interested readers. As such, the sites's content is largely going to change only when I write something new and publish it. Based on the 95% barrier I set for myself above, such content appears to be will within that barrier. To handle the remaining 5%, I am fine with any dynamic content for my site being generated using JavaScript. A good example of that is using something like Disqus for dynamic comments. By handling such things with a Javascript approach, I can keep the simple things simple, only making things more complex when they need to be. To me, that seems to be a solid way to handle the few exceptions that may arise. For those reasons, I believe an SSG is an ideal choice for a personal website with a blog. Which One to Choose? In case I need or want to do any extension work, I want the SSG to be in a language I am comfortable with, maintaining the ease of use concern from the previous section. At the moment, that limits the SSG language to C#, Java, and Python. While I can work effectively in other languages, the comfort and enjoyment levels are not there. As I am doing this for myself, I want the writing of any extensions to be easy and fun to increase the chances that I will stick with the writing and the SSG choice. Looking at a number of sites, these are the SSGs that appear the most. Generator Language Website Last Updated Hugo Go https://gohugo.io/ May 30, 2019 Pelican Python http://blog.getpelican.com/ May 13, 2019 Hexo Node.js https://hexo.io/ Jun 6, 2019 Jekyll Ruby http://jekyllrb.com Jun 9, 2019 Gatsby NodeJs https://github.com/gatsbyjs Jun 9, 2019 (Note that the last updated column is current as of 2019-Jun-09.) Given these goals in mind, I looked through the choices for static site generators and decided to go with Pelican. It's written in Python, has been maintained recently, any extensions are written in Python, and seems to be easy to use. This choice supports my desire to write pages and articles in Markdown, and any needed extensions can be tweaked if needed. What Was Accomplished At the beginning of this article, I was intrigued by Static Site Generators and whether or not SSGs would be an effective tool for me to communicate through. During the article, I not only determined that it was the right fit, but selected the Pelican as the SSG to use, based on its Python coding and recent updating. I feel that this determination to use SSGs, and Pelican specifically, puts me in a good position to start building my website with confidence. If You Are Trying To Decide… If you are trying to decide if SSGs, or a specific SSG, will work for you and your site, I would encourage you to walk through a similar process to what I did. Figure out your own concerns for your own website, and determine whether or not an SSG will address those concerns. If you think you may write extension for it, take a look at the language of the SSG and make sure you are familiar with the extension language. Most importantly, figure out whether your choice of SSGs in general or a specific SSG will serve as a tool for you to publish your information, or whether it will be an impediment to you publishing. From the point where I was at when I made the decision, Pelican appeared to be a good choice for me. I did some research to make sure this was the right choice for me. Make sure you ask yourself those questions, get those answers through research, and make sure your choice is going to work for you! What's Next? Next, I need to start exploring Pelican and how to set it up with default content. This will be covered in the article Setting Up the Pelican Static Site Generator .","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/08/18/static-websites-choosing-a-static-web-site-generator/","loc":"https://jackdewinter.github.io/2019/08/18/static-websites-choosing-a-static-web-site-generator/"},{"title":"Glanceable Displays: What Do I Want To Display?","text":"Preface This is the final article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In the previous article Glanceable Displays: Setting Up Our Display , I added the finishing touches to allow a Raspberry Pi to display a single web page on my Glanceable Display. It took a bit of time to get there, but the work that I put in over the previous articles made sure that the foundation for the Glanceable Display is solid, a necessity for a device that I want to be as \"hands-off\" as it can be. This article takes a look at the options and work I did to put a series of web pages on that display. I have tried to document the various processes that I went through with my family's display to help any readers come up with a good way to help them come up with their own requirements for their Glanceable Display. What Are My Options? As of the writing of this article, there are 3 major options available: roll your own use Dakboard use Magic Mirror To walk through each of these issues, I have shared my \"Pros and Cons\" lists, as well as some general discussion on those lists. Option 1: Roll Your Own Pros Cons write it yourself write it yourself Writing down a list of pros and cons for this option, I came up with the same thing on both sides: write it yourself. On the pros side, you have the freedom to do what you want, however you want to do it. On the cons side, you have the responsibility to learn and create what you want, as well as maintaining it, debugging it, and hosting it. While this is the option with the greatest ability to be creative, it is also the option with the greatest costs. Taking a hard look at this option, while I am capable of doing that work, it would be slightly outside of my normal comfort zone. Any changes would likely be code changes, and not configuration changes. The web pages would have to be hosted somewhere, either on a local machine or in the cloud, and the monetary and time costs associated with that. Taking all of this into account, the maintenance cost seemed to be the primary cost, and it just seemed to continue to get higher with this option. For the most part, this was immediately discarded by me, and that decision was backed up by my family. It seems that I get grumpy if I have to do too much maintenance on our systems at home, and they would prefer to avoid that, is possible. Go figure! Option 2: Use Dakboard Pros Cons professional solution locked in to their pricing scheme they do the dev and testing locked in to their feature set multiple screens available not in control of new features loops of screens available simple WYSIWYG editors This option uses the frequently sited DAKboard site to host and maintain the webpages to display. As their pricing indicates, if you want to do something custom, you are at least looking at their Essential package. However, for that monthly fee, you do get a number of neat features that just work . The first feature is the WYSIWYG editor, with which you create custom screens. You get seamless integration with different types of calendars and both provided and custom data sources. Comparing this option to the previous option was like comparing night to dark. Yes, there is a monetary cost to consider. But for that cost, any screens I created would be hosted on their server. The maintenance cost for me is the configuration of the screens, with no development or debugging cost. And that maintenance cost would be limited to look-and-feel issues with the displays, not debugging why something is not working. Taking an honest look at the cost-to-benefit ratio for this option, the primary cost was the monthly fee and there were multiple features that just worked on the benefit side, making this a promising option. Option 3: Use Magic Mirror Pros Cons lots of third party plugins oriented to sharing a single screen can customized output you handle the installation of plugins you handle the configuration This option uses an open source solution that is often cited, Magic Mirror . As this solution is open source, you will be putting your own time into this project to download and configure things. That configuration is mostly done with a file editor and the testing of changes, re-iterating with reboots of the system. If there is something that you need this to do, if worse comes to worse, you can write the plugin and get it into Magic Mirror's ecosystem. From my viewpoint, this option landed somewhere between the Dakboard option and the Roll Your Own option. While there is a lot of existing choices and a framework to host it, there is still a bunch of experimentation and maintenance that is needed to get this working. The big issue here is that MagicMirror is intended to be a single screen display that shows a collection of information, not a series of screens with different information. What Did I Decide? To satisfy my family's requirements, Dakboard's solution was the best fit for what I wanted to do. As this is a family project, an implicit requirement is that if we decided that we want to change something, I need to be able to affect that change pretty quickly. Between Dakboard's features and their custom screen editor, I had confidence that I would be able to make these changes quickly. The decision came down to an easy calculation of cost versus benefit. For Dakboard's monthly pricing, the ease of which our glanceable display would display information and require almost no maintenance is still worth it. While spending time on side projects is fun for me, this project's main goal is to use a glanceable display to make an appliance that we all use. In my mind, spending hours laboring over the maintenance of the appliance defeats the purpose of calling it an appliance. Starting to Use Dakboard Using my family's requirements as a guideline, starting to use Dakboard was a pretty easy journey. As I knew we needed around 2-3 pages in rotation, the default templates were not satisfactory and I registered for a premium account. At the time of this article, that premium account was 5.95 USD per month and included 3 screens that can be added to a single loop. Depending on what my family's future plans are, we might revisit the pricing, but for now that works for us. Logging in to my Dakboard account, I had a screen called My Predefined Screen ready to go in my account. Taking a quick look at it, from my point of view, it was too cluttered and busy for our family. Based on prior discussions, we distinctly wanted 2 screens: one for a calendar and one for the weather. Everything after that was extra. Selecting the Add a Custom Screen or Template button under Custom Screens , I was left at a screen asking me to put things together for the screen. Where to start? Thinking through a theme Before I came back to write about this point in the article, I tried a number of times to create screens that took care of our requirements for calendars and weather… but they didn't seem right. It just didn't seem to me like the pages where cohesive and their purpose clear. It wasn't until I took a couple of days away from the project that I realized my problem: the pages didn't have a common theme. Now being aware that I needed to find a theme, I went to Dakboard's Templates display for inspiration. After a bit of thinking, I figured that the folks at Dakboard wanted to put their best foot forward, so they probably had someone with artistic experience help them with their templates. As such, using them as meta-templates or inspiration for our screens seemed like a straight forward thing to do. Similar to my approach for deciding on the software to use to generate the pages, I went through each of the template screens, writing down good and bad things about each one. Right off the mark, I was able to eliminate some screens as too busy for my taste, which helped out. In the end, the list of things I wanted for the display boiled down to the following items: a horizontal layout instead of a vertical layout the left quarter of each screen will be very simple text on background of a rotating set of images good examples of this are the Simple Agenda and Big Calendar templates the current date and time will appear at the top of that quarter only current weather warnings will appears the bottom of that quarter the remaining three quarters of each screen will have a singular focus the calendar screen will have a calendar that takes up the entire screen, due to it's importance to our family the Big Calendar screen is a good example, but must support a different color for each family member the weather screen will have specific items related to the weather, including a storm map if possible the Beach screen's left side is a good example the final screen will have other things we want to display and test out Keeping with my desire to keep my family on board with the project, I talked to them about this after dinner one night, and with the exception of some small squabbles (about which family member got represented with which color on the calendar) everything was good to proceed. Doing the work - The Left Quarter Panels To make the transitions between screens seamless, it was important to me to ensure that the size of the left image panel and the date and time panels we consistent through all three screens. While it might have appeared to be a very pedantic behavior on my part, I wanted to reduce any barriers to adoption by my family. From doing some reading on user interfaces for my job, choppy or inconsistent interfaces can often turn off users. Based on that experience, the extra time used to get it \"just right\" was a worthwhile investment for me. To carry the theming further, I wanted the images displayed in this left image panel to have a common theme. Looking through stock images from Unsplash , I went through and started downloading 15-20 images that were 634 pixels by 951 pixels, a common size available on the site. Looking at the download directory, I noticed that those pictures appeared to have 3 common groups: pictures of night skies, pictures of nature, and pictures of various paths. I then created three new folders to hold each group of pictures, moved the relevant pictures into their own directory, and pointed the image panels of each screen to a specific directory. Before moving on to the next part of the screens, I added these screens to a loop and looked over and over again at various parts of the display, making sure that the display was reflecting what I wanted the look and feel to be. After a number of nitpicky hours of fiddling with panel size and location, I brought my family in and took some creative criticism on the screens. I took their feedback and applied it to the screens, repeating the vetting process with myself and then the other members of my family until it was just right. Once I had everyone onboard, I moved to the content of the other screens. Doing the work - The Calendar Screen Dragging the Calendar panel to the screen was the easiest part of this screen. While the biggest decision was to decide whether or not to use the Monthly calendar type or the Big Monthly calendar type, it was the small details that really needed tweaking. The first part was to add each of our personal calendars to the panel. The people at Dakboard facilitated this by having a button called Show iCal help that displays easy to follow instructions. I had no problems adding each of our personal calendars to the screen, and assigning unique colors to each of those calendars. The fine tuning and nitpicking came over the next 2 weeks as me and my family took a look at the display and tried various options to get it dialed in to what we needed. The reason that this screen is so important to my family is that we all use our phones to record when we have things to do and where we need to be to do them. Having a separate whiteboard that we had to update with those schedules wasn't working for us, so having something that was able to use the same calendar source as our phones was pivotal to it's acceptance. Doing the work - The Weather Screen In approaching this screen's design, it was important for me to think of how my family intended to use this screen. In asking my family, there were three main use cases: What will the weather be like today? (for dressing) What will the weather be like in a few days? (for planning) Are there any weather alerts for the area? (for implementing plan-B to avoid the bad weather) For the first two, it has been hard to figure out which two weather sources are more accurate, Darksky Weather or OpenWeatherMap, so instead I just show both. In the future, I am going to have to change that, but for now my family is still deciding. To increase our information though, we have a precipitation radar map courtesy of The Weather Channel . To embed it in the panel, I added an image panel, went to the weather map I wanted to add, right clicked on the image and then selected Copy Image Address and placed that in the configuration for the image panel. It took a couple of tries to get it looking the way I wanted, but was pretty easy. Adding the weather alerts was even easier for me as Dakboard already had a separate panel for that. It was as simple as adding the panel, setting the location, and saving the panel. One note for all of these weather panels was that they worked better when I used the closest major city instead of the small town that I live in. Results were weird when I didn't do this, and if I figured it out correctly, were showing the weather for New York City! Doing the work - The Left-over Screen Yes, the left-overs. Where to put the fun stuff that really didn't fit in anywhere else? For fun, I used the News/Rss Panel to add feeds for Brainy Quotes and Word of the Day . The panels were easy to configure and get them to display in a way that was easy to read. My wife's family loves Christmas, so I added a X Days Until Christmas display using the Countdown panel, setting the date to December 25, 2019 and to show only the days. The really fun panel was to show the number of astronauts currently in space as counted by NASA. Searching online, I found NASA's open api which includes the number of astronauts in space complete with instructions on how to interpret the data . Adding it into a new Fetch panel, I set the panel to extract the number of people in space from the number field and use the user-astronaut icon from font-awesome to display beside the number. I am sure there will be more fun things to add, but confident that this was a good start, it was time to wrap it up. Wrapping The Screens in a Loop To facilitate the showing of multiple screens in succession, Dakboard provides a wrapper called a loop. A loop can change the current screen at a various intervals from every 15 seconds to every hour. To add a loop, I went to the main screen for my account, clicked on the loops button and followed the simple instructions provided on the popup that appeared. For my family, I chose a change frequency of 15 seconds and added all 3 of the screens I created into the loop. To view the loop, I clicked on the View Loop button that was visible once I clicked on the ellipses (…) at the bottom of the panel in the loops display. Once things looked right on my computer, it was time to add them to the glanceable display. When I pressed the View Loop button, the browser changed to an URL that begins with https://dakboard.com/loop/uuid/ . This URL is the actual URL you need to use to view the loop from any browser without having to go through the configuration screen. SSHing into our glanceable display, I went back to my notes in the article on Setting Up Our Display - Starting Chromium In Kiosk Mode and replaced the placeholder URL of https://bingwallpaper.anerg.com/ with the URL that I found from the View Loop window. Double checking my work, I used my SSH session to enter sudo reboot and reboot the glanceable display. After a couple of minutes of boot up time, the glanceable display finished it's startup and began showing the web page from Dakboard, rotating through the screens! How Did It Work? I started the first draft of this article in late July, just after I got the glanceable display working for my family. As of October, when I am adding this section to the article, we are using the display on a daily basis, mostly in the way it was intended. We continue to tweak little bits on the \"Left-over\" screen, but the other screens are dialed in the way we want them to be. One of the things that I believe is crucial to it's success is the placement in our kitchen. We have a nice corner where we have it plugged in, out of the way of various devices, tucked under the cupboards where we store our dinnerware and our glasses. This ensures that when we go to get some food or drink, we almost instinctivly glance down at the display as we are getting something out of one of those cupboards. For me, this eliminates the \"but I didn't look at the display\" factor for each of us, as we each know that getting something from those cupboards means we saw the display, if even for a second. It also may have seemed like a small thing to include my family in the process, but I firmly believe that engaging with them at various points and incorporating their feedback into the project had a large bearing in their acceptance of the display. It was important to all of us to be able to talk to the others and say \"hey, can we add X to the display\" and talk through whether there was a benefit to all of us from including that idea on the display. By including each family member in the creation process and design process, they continue to have an investment in what is being displayed, and ensuring it is current and relevant. And yes, we have all had to ask or remind each other to add various meetings on to our own calendars so they show up on the main calendar. By their very act of doing that, I know they are using the calendar. By the number of times my wife has mentioned that there is snow at the nearby Snoqualmie Pass, I know the weather screen is getting used, especially the weather alerts section. The harder one to figure out is the \"Left-Overs\" screen, but I am sure that I have had my son try hard to seamlessly work the word of the day into casual conversations, sometimes with hilarious results. But it's the thought that counts! What Was Accomplished This article detailed the work that was needed to take my family's glanceable display from an appliance that was just sitting around to one that we use every day. I shared with readers a number of different notes I made for this project at various points, along with my rationale for those notes. Finally, I walked through the various setup that I performed on the various screens to get them on the display. If you have followed this process through all 5 articles, thanks for your support, and I hope you take the time to look at some of the other articles on this site!","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/08/11/glanceable-displays-what-do-i-want-to-display/","loc":"https://jackdewinter.github.io/2019/08/11/glanceable-displays-what-do-i-want-to-display/"},{"title":"Glanceable Displays: Setting Up Our Display","text":"Preface This is the fourth article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In the first article, Glanceable Displays: What Are They? , I introduced the concept of a Glanceable Display, describing a number of things to consider if you decided to embark on a project like this. Assuming you decided to proceed, the article Glanceable Displays: Installing Raspbian on a Raspberry Pi detailed how I went from an unformatted Raspberry Pi to one with the Raspbian operating system installed. As I prefer working from my own workstation, I also detailed the setup of the SSH service on the Raspberry Pi to allow for remote connections. Finally, the article Glanceable Displays: Fine Tuning A Raspberry Pi Installation showed how I filled in a number of gaps that I encountered with the normal installation, namely setting up a wireless connection with my network and ensuring the Raspberry Pi has a solid understanding of the current time. Having taken all of those steps to be confident that setting up the actual display will work, it is time to jump right in and set the display up. But what is it actually that I was setting up? What Are Our Display's Implementation Requirements? When it comes down to it, the articles in the series have been building up to this point. I now have a glanceable display is a Raspberry Pi. Upon boot, it will start a web browser pointing at a specific webpage. It's that simple. However, the devil is in the requirements. The high level requirements for the display were covered in the first article in this series, Glanceable Displays: What Are They? . What is left are the specific requirements that will realize those high level requirements into an actual display: the implementation requirements. The first set of implementation requirements is that any administration of the machine can be performed from my desktop. With the exception of the Raspberry Pi seizing up, which I have noticed from time to time, I should not have to touch the Raspberry Pi itself. For the most part, I will be running sudo reboot to restart the machine, but all of that should be done without the need to plug a keyboard into the Raspberry Pi. Thanks to the documentation in the previous articles, these requirements have been completed. The second set of implementation requirements is that after a reboot of the machine, I shouldn't need to plug in a keyboard and type some commands to get it started. After the machine starts, it should open a browser and display the page or pages that are required. To be clear, on reboot with no keyboard and no mouse, the display should start by itself. The physical cost of plugging a keyboard into the machine kind of defeats the \"appliance\" feel that I want it to have. The final set of implementation requirements is that, to the best extent possible, any processing of what to show on the display should be performed on a machine other than the Raspberry Pi. While some of the more recent machines have more power on them, I want to be able to use lower cost Raspberry Pi machines for the display. If I must also run scripts to pull information to generate information for the display, it means I need a heftier machine. While this may change later, I believe that starting with the lower machine requirements is the right thing to do. Installing the Right Tools To make sure that I had the right browser and other utilities I needed, I ran the following command: sudo apt-get install chromium-browser x11-xserver-utils unclutter A lot of Raspbian installations will come with chromium-browser package already installed, but I included it just to make sure it is there. The x11-xserver-utils package has one or two small utilities that will make the display look cleaner. The unclutter package allows me to hide the mouse cursor after inactivity, perfect for a display where I know there will be no mouse attached. Note When I was testing out the installation instructions, one of the things that made me include instructions on setting up a time server is the apt-get command. In certain cases, if your Raspberry Pi's clock is too far in the past, you will not be able to access the right packages with apt-get . Please make sure your Raspberry Pi's clock is current before using the apt-get command. Creating a Local Startup file By default, Raspbian comes with a heavily modified version of the LXDE or \"Lightweight X-11 Desktop Environment\". According to the documentation , the startup configuration file for the a given user needs to be located at the path /home/$user/.config/lxsession/LXDE-pi/autostart . If it is not there, it will default to the generic file located at the path /etc/xdg/lxsession/LXDE-pi/autostart . As this setup uses the default pi user, the startup configuration file needs to reside at /home/pi/.config/lxsession/LXDE-pi/autostart Following the advice of various articles, I elected to create a copy of the autostart file under the local /home/pi/ directory. That way, if something bad happens, I can always restart the configuration by copying the default file into the /home/pi/ directory again. To accomplish this, I executed the following commands: mkdir -p /home/pi/.config/lxsession/LXDE-pi/ cp /etc/xdg/lxsession/LXDE-pi/autostart /home/pi/.config/lxsession/LXDE-pi/autostart To place the file in the proper directory, I performed a mkdir command as the LXDE-pi directory did not exist with the clean Raspbian installation I was using. Once I had the directory created, I used the cp command to copy the default version of the autostart file into that directory. At that time, the file looked like this: @lxpanel --profile LXDE-pi @pcmanfm --desktop --profile LXDE-pi @xscreensaver -no-splash @point-rpi Starting Chromium In Kiosk Mode Now that the local startup file was present, I edited the file with the command nano /home/pi/.config/lxsession/LXDE-pi/autostart in order to add my own startup commands. To be honest, I tried a number of different startup commands recommended by different articles, and each one had good points and bad points. After much experimentation, I ended up with a simple set of startup commands which was as follows: @unclutter @xset s off @xset s noblank @xset -dpms @chromium-browser --incognito --start-maximized --disable-notifications --disable-extensions --disable-hang-monitor --disable-infobars --kiosk https://bingwallpaper.anerg.com/ As mentioned above, the unclutter tool makes the mouse disappear when not used, which is perfect for a display that is never going to have a mouse. The xset tools allows for the setting of various XWindows related settings. Specifically, the s setting is for the screen saver and the -dpms setting is for the monitor's Energy Start features. Finally, the Chromium browser is the browser I chose to start with for displaying the webpages as it has the most documentation on command line switches . Note When I say the most documentation on command line switches, look at the link. The list is way too large to confidently comprehend. As such, I had to take guesses as to which of the –disable switches I needed. In order, the changes I made to the configuration file: hide the mouse turn the screen saver off don't blank the screen turn off any Energy Star power save monitor features start the browser (window maximized, incognito, in kiosk mode) pointing at the Bing wallpaper page After I finished editing the file, I made sure to save the file (ctrl-X, yes , enter), and then double checked all of my changes. When I confident I had all of them entered correctly, I proceeded to the next step. Verifying things are Working Properly I issued a sudo reboot to reboot the Raspberry Pi. And waited. And waited. And waited. If the repetition doesn't make it clear, it felt like forever. I was sure I had followed my own instructions properly. Even going off of my own notes, there was the anticipation of seeing whether or not it would work. The Raspberry Pi I was using seemed slower than usual, but after a while, everything started up and it was displaying the website https://bingwallpaper.anerg.com/ in the browser. As I checked the display, I saw the mouse pointer disappear after a few minutes. Check. After a couple of hours, the screen saver had not kicked in. Check. After a couple of hours, the monitor was still displaying the website. Check. For the most part, due to some good notes that I kept, everything was up and running the way it was supposed to. But What if it Is Not? Let's be honest. The first 5-10 times that I performed this setup, I didn't get it right and it was only my notetaking that helped me figure out which things worked and which didn't. It took a lot of notetaking and a lot of debugging and looking at log files to figure out what was working. The first log file that I used to debug things was the /home/pi/.xsession-errors file. After executing the cat /home/pi/.xsession-errors command, I noticed that while it didn't have a lot of useful information, it had two important pieces of information: ** Message: 17:00:13.893: main.vala:101: Session is LXDE-pi ** Message: 17:00:13.894: main.vala:102: DE is LXDE ** Message: 17:00:16.375: main.vala:133: log directory: /home/pi/.cache/lxsession/LXDE-pi ** Message: 17:00:16.376: main.vala:134: log path: /home/pi/.cache/lxsession/LXDE-pi/run.log This information let me know I was putting the changes in the right place, and where to look for the log for the current user's session: /home/pi/.cache/lxsession/LXDE-pi/run.log . Also, the fact that it was putting the logs in the /home/pi/.cache/lxsession/LXDE-pi directory meant that it noticed the autostart file that I added, as was using it. That was a useful piece of verification. When I started looking at that file, I was at first overwhelmed before I found a couple of tricks to help me out. The first trick was to look for a file that looks like this: ** Message: 17:00:16.917: autostart.vala:42: Autostart path : /home/pi/.config/lxsession/LXDE-pi/autostart Everything before that point are just the other parts of LXDE getting set up, and it really didn't have any effect on what I was trying to do. After that line were a series of lines that began with Launching , corresponding with each line of the autostart file. The next section of lines complemented those lines, providing for the exit codes of each of the lines in that file. Finally, there is a section starting with Connecting ... that signifies the section of the log where the wired and wireless link status is logged. While the link status is important, the fact that it gets to this point successfully generally means that the display is ready to go! What Was Accomplished This article detailed the work that was needed to take my Raspberry Pi from a sufficiently set up machine to fulfilling 2 out of the 3 requirements of my a glanceable display. At this point, I have setup a Raspberry Pi that is remotely administered from a machine other than the Raspberry Pi itself, one of our requirements. Another of the requirements is to have the machine start displaying a web browser automatically, without any requirement for a keyboard or mouse to be attached to the machine, which has also been accomplished. Along the way of documenting that setup, I also provided some useful tools to clean up the display and some places to look for debug logs for the setup process. Specifically, this article took the big step forward from a Raspberry Pi that was remotely administered, to a Raspberry Pi that automatically launches a web browser pointing to a specific webpage on reboot. The next step is to provide that web browser with some content that is not generated on the Raspberry Pi itself. What's Next? In the final article in this series, Glanceable Displays: What Do I Want To Display? I walk through the steps I took to determine where to get finalized webpages to display in my glanceable display, while adhering to the final implementation requirement of my glanceable display.","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/08/04/glanceable-displays-setting-up-our-display/","loc":"https://jackdewinter.github.io/2019/08/04/glanceable-displays-setting-up-our-display/"},{"title":"Glanceable Displays: Fine Tuning A Raspberry Pi Installation","text":"Preface This is the third article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In the previous article , I detailed the steps required to get the Raspbian (Linux) operating system installed on a Raspberry Pi with remote access through SSH enabled. This article attempts to move the needle forward by tackling a number of the issues that I had in getting necessary foundation services up and running on my Raspberry Pi. The two services that I had issues with were: ensuring that Wi-Fi was working properly and that the Raspberry Pi clock was being set properly. Until I was confident that these two issues were resolved, I was not confident that I would be able to use my glanceable display in it's intended location, as that location does not have any wired network access. Note As the steps in the previous article concluded with providing SSH access to the Raspberry Pi, I performed all of the following configuration using an SSH client. While you are welcome to enter them through a keyboard directly connected to the Raspberry Pi, I did not test any of the steps in that manner. Warning From the experience of reviewing and retrying these steps, sudo reboot is a good friend. If it looks like something didn't work and you think it should, consider using sudo reboot to reboot the machine and carry on from there. This is very handy when changing configuration. Step 1: Wi-Fi Access Unless you have the fortune of having a wired network outlet available near your display AND enjoy the aesthetic of having a network cable going into your display, you most likely want to enable Wi-Fi access to your display. Here are the steps I went though to get wireless access enabled. Step 1a: Searching for Existing Wireless Access The first command that I used to check for wireless network access was the ifconfig command. This is a general command used to determine what network connections are available. I had a network cable running into the machine, but nothing else, so the following response 1 was expected: eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 ... While it looks like gibberish to some, it was what I expected. Linux systems often use the tag eth0 for the first wired connection and the tag lo for the loopback connection. The important thing I noticed was that I didn't see a wlan0 tag, typically used for the first wireless connection. The clear observation I had was that the system did not currently have a capability to have a wireless connection. This observation on a normal computer running Linux might be farfetched, but from information gained researching the Raspberry Pi on various forums, it is relatively normal for a Raspberry Pi to not have any wireless capability built-in. As wireless components have recently become cheaper, it seems like it is only the latest versions of the Raspberry Pi 3 have wireless access built in. Note I have not repeated these steps on a newer Raspberry Pi, but I would expect that doing so would allow me to skip the next section on installing the adapter. I will update the article once I have tested that scenario. Step 1b: Installing a Wireless Adapter In my instance, the machine I was configuring did not have any wireless capabilities built in. That was resolved in short order by noting down the connection requirements for the household router (WPA2-PSK (AES)) and purchasing an inexpensive USB Wi-Fi adapter from a nearby store. Note If possible, use that store's online presence to inspect the possible adapters, verifying 1-3 choices for adapters that meet the router's specifications. While not necessary, it can avoid round trips to the store to try and find the right adapter. Doing this, I found that my local store had a TP-Link TL WN823N USB Adapter that was right for the job for $15. Returning home from the store with an inexpensive TP-Link TL WN823N adapter, I powered off the Raspberry Pi, installed the Wi-Fi adapter in one of the open USB slots, and powered up the Raspberry Pi. Once Raspbian had booted up, I reconnected to the machine using SSH and entered the lsusb command. This command is similar to the ls command to list files, but is used to list various USB components that the machine has recognized. The response I received from that command was: Bus 001 Device 006: ID 2357:0109 TP-Link TL WN823N RTL8192EU Bus 001 Device 005: ID 0461:4e26 Primax Electronics, Ltd Bus 001 Device 004: ID 0461:4d64 Primax Electronics, Ltd Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. SMC9514 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Seeing as the adapter I bought was a TP-Link TL WN823N, I was relieved to see it listed as device 06. At this point it was clear to me that the Raspberry Pi recognized the adapter, and it was time to install the required drivers. Step 1c: Installing the Wi-Fi Drivers From a research point of view, this was the most frustrating part of getting wireless access working. In most of the articles I researched, the authors seemed to assume that the drivers for a given adapter would already be installed in the Raspbian distribution. The few articles I found that handled missing drivers were very specific, and not very helpful. They often used the Raspbian apt-get family of commands to look for an adapter, and each adapter I found seemed to have a slightly different way of making sure it worked properly. As I was writing down notes for this article to help other people, that experience was frustrating and was far from helpful. Everything changed when my research led me to a reference to the Fars Robotics driver install script. This breaks down the process into the following three commands: sudo wget http://www.fars-robotics.net/install-wifi -O /usr/bin/install-wifi sudo chmod +x /usr/bin/install-wifi sudo /usr/bin/install-wifi Instead of playing around with apt-get commands and looking for a specific driver, this script automated the process, finding the correct driver and installing it. The commands first downloaded the script into the /usr/bin directory, set it to be executable, and then executed the downloaded script. Within 60 seconds, it had completed, and it's output suggested that the driver for my TP-Link TL WN823N had installed successfully. To double check that it worked properly, I resubmitted the ifconfig command and got the following: eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.2.3 netmask 255.255.255.0 broadcast 192.168.2.255 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 ... wlan0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 ... Great! Progress! The new Wi-Fi adapter was installed, the driver was connected, and Raspbian was reporting that it could talk with it! From experience, I now had to configure the machine to connect to the wireless network. From the output, this was somewhat obvious. For the eth0 and lo adapters, there was a line beginning with inet that described the current IPv4 setup for that adapter. As the wlan0 adapter was missing the inet line (and also the inet6 line), Raspbian could talk to the adapter, but the adapter was not connected to the local Wi-Fi network. Next stop, configuring Wi-Fi access. Step 1d: Configuring Wi-Fi Access Similar to the previous section on Installing the Wi-Fi Drivers, I found many articles on how to do this by hand, each with its own little twist on how to set things up better. In my case, I wanted to go for repeatable and easy, and the sudo raspi-config command used to set up SSH access in a prvious article proved to be the best solution. From the main menu, I selected 2 network options and then N2 WiFi . As it was my first time setting up the network on this machine, I was prompted for my country, which I entered. Next I was prompted with Please enter SSID , and I responded with my router's SSID. This was followed with the inevitable Please enter passphrase. Leave it empty if none. , to which I responded with my router's password. Hoping everything would work out, I pressed the Next button. After waiting for approximately 30 seconds, the focus was returned to the main configuration screen. Exiting out of that screen and back to the main screen, executing the ifconfig now had the following response: eth0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500 inet 192.168.2.3 netmask 255.255.255.0 broadcast 192.168.2.255 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 ... wlan0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.2.67 netmask 255.255.255.0 broadcast 192.168.2.255 ... This was what I was waiting for! At the moment, the eth0 wired connection had an address of 192.168.2.3 and the wlan0 wireless connection had an address of 192.168.2.67. The address 192.168.2.3 is the one I had been SSHing into while fixing up the Wi-Fi, so that lined up. Based on my router's configuration, 192.168.2.67 was a likely address for a new machine, so that lined up. Warning If you have your network secured, make sure you follow the normal steps for adding a new machine to your network. This may include asking your local network person to add your machine to the network. In my case, forgetting that step cost me over an hour trying to figure out why the Raspberry Pi would not connect to the network! To verify things were working, I repeated the relevant portions of Step 5: Setting Up SSH Access from the previous article, using the new address 192.168.2.67, instead of the old one 192.168.2.3. Once this succeeded, the final test was to disconnect the physical connection and see if the wireless connection worked. It did take a lengthy bit of time, and one trip to a local store for hardware, but it was very gratifying being able to talk to the glanceable display over a wireless connection! Step 2: System Clock Synchronization While the Raspberry Pi is a useful little machine, it's small form causes it to not have a component that many machine take for granted: a system clock. Not having this set up for your glanceable display can cause any display of time to be extremely out of sync. In addition, if the Raspberry Pi's time is not decently close to the actual time, the downloading of extra components through mechanisms such as the apt-get commands may fail. To avoid these issues, setting up proper system clock synchronization is a must. Pre-Requisite The list of servers and download locations for the apt family of commands has most likely changed since the Raspbian image was constructed. If this is the case, any apt commands that require any kind of package updates will most likely fail. To solve this before it becomes an issue, I issued the following command to update those tables. sudo apt-get update Step 2a: Installing the NTP Service The quickest and most efficient solution to solve the synchronization issue was to install the NTP time service. In the distribution of Raspbian that I was using, the NTP service was not installed. When I entered the command sudo systemctl status ntp , I saw the following output: Unit ntp.service could not be found. Based on various articles, the clear solution was to install the NTP service using apt-get with the following command: sudo apt-get install ntp . Once that finished, when I repeated the sudo systemctl status ntp command, I then received the following output: ● ntp.service - Network Time Service Loaded: loaded (/lib/systemd/system/ntp.service; enabled; vendor preset: enabled) Active: active (running) since ... At this point, I was satisfied that the NTP service was up and running, but not that it was working correctly. Step 2b: Diagnosing the NTP Service Configuration Once the service was loaded, I needed to confirm that the NTP service was working properly. The easy way to do this was to reboot the machine using sudo reboot and then use the date command to check the date. When I followed that pattern, the date was over 2 weeks off from the actual time. Time to go into debug mode. The documentation on the NTP service mentions a useful command: ntpq -p . This command lists information about what the current NTP service is doing. Presented as a series of columns, the important column was the number of seconds since the service successfully contacted a specific NTP server. When I checked the output of the command, I was greeted with the following table: remote refid st t when poll reach delay offset jitter ============================================================================== 0.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 1.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 2.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 3.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 Looking at the when column, the issue seemed to be that it was not able to connect to those servers and get information about them. Doing a bit of research, it turned out that other people have had problems with the default servers for the NTP server, and there were ways to address that. Step 2c: Changing the NTP Service Configuration Now that I had the NTP service installed and had issues with the default configuration, it was time to look at it. The configuration for the service is located in the file /etc/ntp.conf . As I had found issues with the default configuration, I needed to learn more about this configuration file to fix the problem. Looking at the file using a command like more /etc/ntp.conf was daunting. There were a number of 1-2 line sections with multiple lines of comments before them. When I saw that, I was concerned. In my experience, a large comment to useful line ratio means they are commenting the heck out of it, because people have done stupid things in the past. Learning more about the configuration, it turned out that I only needed to look at one particular section. The most important part of the configuration is a section that started with the follow text: # pool.ntp.org maps to about 1000 low-stratum NTP servers. Your server will # pick a different set every time it starts up. Please consider joining the # pool: <http://www.pool.ntp.org/join.html> Right after this text, there was a group of servers specified where the first part of the server name was a number between 0 and 3, and the rest of the server name was the same. Many of the articles I found didn't touch the rest of the file, but focused on that small set of lines. A very useful page I found during my research was the NTP Pool Time Servers page. Near the bottom of that page is a table that lists the various geographic areas, from which I selected the North America selection. At the top of the next page was a clearly marked section of text, with clear instruction to add that text to the ntp.conf file. Given that information, I went back to the Raspbian machine, entered sudo nano /etc/ntp.conf to edit the /etc/ntp.conf file, and replaced the default debian servers in that section with the information from the clearly marked section of the NTP Pool Time Servers page. Followed up with Ctrl-X to save, y to save the file, enter to use the provided file name, and it was changed. Just to make sure, I did another sudo reboot after verifying my changes, and the date command now returned the right date. Step 2d: Verify that the NTP Service is Working Warning As a number of Denial-Of-Service attacks have used the NTP port to return bad information, a number of routers come pre-configured with their firewalls set to block port 119 and 123, the NTP ports. If you follow these instructions and are still having issues, check the firewall settings for your computer and your router. Previously, when I had checked the status of the NTP service using the ntpq -p command, I did not seeing anything other than - in the when column of the output. If things were working properly, I would expect that column to change. Submitting the ntpq -p command once the system rebooted, I got the following output: remote refid st t when poll reach delay offset jitter ============================================================================== -grom.polpo.org 127.67.113.92 2 u 21 64 1 59.302 9.614 6.053 +199.180.133.100 140.142.234.133 3 u 17 64 1 26.382 5.225 3.611 +time1-inapsales 216.218.254.202 2 u 17 64 1 41.043 -2.463 4.417 *vps5.ctyme.com 216.218.254.202 2 u 18 64 1 36.759 -5.673 2.870 Submitting the command multiple times, I observed that the when column values increased to a certain point before they started over with a low single digit number. From the documentation, this was a good indication that each of those servers was being queried successfully, then registering itself for the next time to check against that server. Seeing the successful connections with the NTP servers, the time synchronization issues were cleared up! Just to be sure, I did a couple of cycles of sudo reboot and checking the time, with no errors. What Was Accomplished This article detailed the steps taken to fix two of the major issues I had after installing Raspbian on my Raspberry Pi: the Wi-Fi access and the system clock being out of sync. I worked through the various steps taken to ensure that the wireless access was up and running, including the various checks I did throughout the process. I then walked through the steps I took to ensure that the time on the Raspberry Pi was reflecting actual time, despite not having an onboard system clock. With this accomplished, I had confidence in having firm foundations on which to start building the display part of my glanceable display. What's Next? In the next article in this series, Glanceable Displays: Setting Up Our Display I walk through the steps I took to setup the Raspberry Pi to start the display upon boot for the glanceable display. In any of these examples, a set of ellipses (\"…\") are used to denote that content was removed that was either sensitive or not relevant to the example being presented. ↩","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/07/28/glanceable-displays-fine-tuning-a-raspberry-pi-installation/","loc":"https://jackdewinter.github.io/2019/07/28/glanceable-displays-fine-tuning-a-raspberry-pi-installation/"},{"title":"Glanceable Displays: Installing Raspbian on a Raspberry Pi","text":"Preface This is the second article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction The methods detailed in this article provide for simple installation of the Raspbian operating system using the New Out Of Box Software (NOOBS) installation method, as suggested for beginners by the Raspberry Pi site . While there are more direct methods for experienced users, the NOOBs installation was selected for it's simplicity and ease of installation. Using the NOOBs installation, this article details the first steps I took in setting up one of my Raspberry Pi systems to be a glanceable display for my family. Those steps start with the formatting of a MicroSD card and installation of the NOOBs installer on to that MicroSD card. After installing that card into the Raspberry Pi, the steps continue with the installation of a stock Raspbian distribution, detailing the a couple of questions that need to be answered to complete the installation. Finally, to enable remote access, the last step is to ensure that I can access the Raspberry Pi using SSH for later configuration and control. Requirements Raspberry Pi, version 3.0 or later Power Supply for Raspberry Pi Keyboard and Mouse HDMI cable connected to monitor Cat5 Ethernet cable Notepad (electronic or paper) to write down notes on specifics of the installation Note Please keep your own notes as you go, and refer back to them. While I have tested the steps on my own Raspberry Pi machines, they were by no means exhaustive tests. Step 1: Interfacing With a MicroSD Card The configuration and main drive for a Raspberry Pi is a MicroSD card. To get the card ready for use, your computer must be able to interface with the card. Most computers do not come with MicroSD slots, but there are a fair number with SD slots. To make things easier for MicroSD buyers, some of the more high end MicroSD makers include a MicroSD-to-SD adapter in their packaging, such as this 32GB MicroSD card from Amazon . I started out using this, but found that the adapter was only good for 3-4 uses, not for continual use. An alternative is a more multi-purpose adaptor, such as this multi-adapter from Amazon . As it is made from a more durable material, it will survive more uses. The one that I bought from Amazon at 7.00 USD is still working after about 70+ uses, so at 0.10 USD per use, it has already paid for itself. Also, as it has a USB adapter, I can plug it into a USB extension cable that I already have on my desk. Whichever way you decide to go, make sure to add the MicroSD card to the adapter before plugging the adaptor into you computer. Once it is securely in the adapter, make sure to apply it to the relevant slot on your computer firmly, and make sure the connection is there. On my Windows 10 machine, I can tell this happens as it will acknowledge the connection by opening up an Explorer window, with either a \"please format\" instruction or a list of any files in the directory. Step 2: Getting the MicroSD Card Ready Note Note that the steps that follow are for my Windows 10 machine. The NOOBs site , has sections for installing on Mac and Linux, but I did not test them. If they do not work,please Google/Bing linux microsd card format and linux microsd card mount . Feel free to replace the generic linux in the searches with the name of the Linux distribution that you are using. Step 2a: Reformatting a Used MicroSD Card Reusing old hardware is important, for important reasons such as the environment and cost to build. To make sure that is possible, it took me a number of tries to create a solid recipe for reformatting the MicroSD cards. As I mention in the Requirements section, keep good notes of what you do, or if following a recipe like this one, what changes you made to the recipe. While there is a command line only tool that will also do the job, I found it clunky and hard to use. Instead, the Disk Management applet for the control panel was the tool I settled on. This can be invoked by typing partition in the search window on the tool bar and selecting the create and format hard disk partitions item. Selecting that item brought up the Disk Management window, showing a break down of every drive connected to my computer. When the MicroSD card was properly connected to the computer, it showed up as Removable media after all of my permanent drives. Using the right mouse button, I clicked on each of the blocks on my MicroSD card and selected the Delete Volume menu item until all of the volumes were gone. When that was accomplished, I was left with two blocks, and right clicking on the rightmost block presented me with a Delete Partition menu item, which consolidated all of the partitions into a single unallocated block. From there, I was able to right click on the Unallocated partition to select the Create Volume menu item. This started a simple wizard that quickly walked me through the options for creating a new volume. I used all defaults that were presented with the exception of the file system and quick format settings. I changed the file system setting to FAT32 and unchecked the Use Quick Format checkbox, before clicking on finish and waiting for about 30 minutes before the format was complete. Step 2b: Formatting a New MicroSD Card From a Windows 10 point of view, this was easy. When the MicroSD card was properly connected to my computer, it prompted me to format the card, presenting me with the format dialog itself. When formatting the MicroSD card, it was important to select FAT32 as the type of format and to unselect Quick Format on the dialog. Once I clicked the format button, it took a good while before it was completed. As a rough estimate, I guessed that it was roughly 1 minute per gigabyte on the MicroSD card, regardless of computer speed. Step 3: Install Raspbian Lite Using NOOBS Note When the format is finished from the previous step, it is important to go to your taskbar and eject the media from your computer. I accomplished this by right clicking on the USB stick icon and selecting \"Eject Mass Storage Device\" from the menu. At that point, I cleanly removed the adaptor and the MicroSD card from the computer to ensure the ejection was complete. When I tested various scenarios, any time that I forgot to eject the media at this point, it did not take later on. The people behind the Raspberry PI made sure there is a simple to use installation system that simplifies the task of installing operating systems on to the Raspberry Pi. The New Out Of Box Software (NOOBS) site aims to allow a fairly basic installation of Raspberry Pi operating systems with little effort. Unless you are familiar with Linux systems, their installation can be very daunting, so it is best to keep the installation as simple as possible. To start that process, I downloaded the NOOBS zip file from their website to my computer. After reinserting the MicroSD card and adapter to my computer, I then unzipped the contents of the NOOBS_V3_2_0.zip file to the root of the drive for the MicroSD card. I had to take care to ensure that the contents were in the root of the drive, not in a subdirectory of the drive. This happened enough times that I actually unzipped the files to a local directory and just used XCOPY to copy the files over, solving the placement problem for myself. As with the note at the start of this section, once this action was done, I once again ejected the USB device before disconnecting it from the computer, for the same reasons. Taking the MicroSD card, I found the MicroSD port on the Raspberry Pi. The port is flat with the motherboard of the Raspberry Pi, and the cases I have all have a hole in the case to make it easy to find. Inserting the card into the port, I then attached the other cables for monitor (HDMI), ethernet (Cat5), keyboard (USB), and mouse (USB), with the 5V adapter cable being last. Two minutes later, I was presented with a screen which prompted me to select the operating system to install. I tried a number of times to get the Raspbian Lite install to work, but encountered a number of issues, so I defaulted to the stock Raspbian [RECOMMENDED] install. Once I made this choice, I selected Raspbian [RECOMMENDED] from the top of the list in the NOOBs installation dialog, followed by pressing the Install button at the top. From there, it took about 30 minutes or so before I was prompted with a dialog box that said: OS(es) installed successfully When I pressed the OK button on that dialog, the system rebooted with first a rainbow screen, then a screen with a big raspberry on it, then lots of text scrolling by quickly. After a relatively small amount of action and a lot of waiting, it was now time to set up the operating system for simple uses! Step 4: Initial System Setup There was a lot of text that scrolled by so quickly, I was unable to read it. From what I could see, there were a lot of green OK texts on the left side, so I guessed that the installation had succeeded. After a nice round of blinking lights from the Raspberry Pi, the desktop showed up and proceeded to walk me through the setup configuration. The first dialog was pretty simple, with the title Welcome to Raspberry Pi . The important thing to note off of this dialog is at the bottom right of the dialog is the IP address that the system currently has assigned to it. As this was important, I wrote it down, and proceeded with the rest of the configuration. The configuration is a series of simple dialogs, each giving a clear indication of what is required. Whenever I pressed the Next button, it wrote the information to the system configuration files. As such, I expect delays between when I pressed the Next button and when the next dialog showed up. Turns out that was a rather healthy expectation. Some of the things that were setup were: country language time zone the default user pi setting a new password for the user pi take care of black border update software Having tested this workflow, I knew that the next workflow for my glanceable display would include updating existing packages and installing new packages. As such, I skipped the update software, knowing I would do it later. Both paths produce the same results, so feel free to skip it like I did, or update at a later point. Warning If you forget the password for the user pi , there is no easy way to recover what you changed the password to. Consider creating a fake entry in a password manager, like LastPass, and storing the password there for later use. Step 5: Setting Up SSH Access That being accomplished, the last thing to complete before stopping the installation of the bare bones system was to enable SSH access. By enabling SSH access, I could sit at my comfortable workspace, using my normal computer, chair, and desk instead of at the workbench where I had the Raspberry Pi connected. Frankly, the computer was connected in an almost Frankenstein like mess of wires on an old desk with a chair that was normally reserved for people visiting, not typing. My own workspace looked very inviting compared to that mess. To enable access, I entered the command sudo raspi-config , selecting 5. Interfacing Options , then selecting P2 SSH , and finally answering Yes to the question Would you like the SSH server to be enabled? . After this, the computer took about 30 seconds before being responsive again, with the text The SSH server is enabled. appearing on the screen. Pressing the enter key, and then selecting Finish , I was then back at the command prompt. This was the moment I was working towards: being able to have a bare bones system to use that I could access from my own computer. Entering sudo reboot , I waited about 45 seconds for the system to reboot and to be greeted with the raspberrypi login: prompt. Looking just above that text, I saw the text: [ OK ] Started OpenBSD Secure Shell server. This gave me a bit of confidence to move forward. At the very least, the operating system was indicating that it should allow SSH access. At the command line, I entered: ssh pi@192.168.2.3 and with the exception of the input yes to answer the question, the output was as follows: The authenticity of host '192.168.2.3 (192.168.2.3)' can't be established. ECDSA key fingerprint is SHA256:`some text here`. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '192.168.2.3' (ECDSA) to the list of known hosts. ... pi@raspberrypi:~ $ Note After each repeated installation on the same Raspberry Pi, when I went back to open a new SSH connection, it would report the error WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! . To allow access, I needed to edit the ~/.ssh/known_hosts file and remove the line for the IP address of the machine, as indicated at the end of the provided error message. What Was Accomplished This article detailed the steps taken to install the Raspbian operating system on a MicroSD card. It started by my formatting of the MicroSD card and copying the NOOBs installer onto the card, followed by inserting it into the Raspberry Pi's MicroSD slot. The steps continued with the largely automated installation of the operating system, only requiring the answers to six questions on my part. Finally, it concluded with the setup for SSH to allow me to configure the Raspberry Pi remotely. What's Next? In the next article in this series, Glanceable Displays: Fine Tuning A Raspberry Pi Installation , I walk through the steps I took to move the installation from a bare bones system, to one that had Wi-Fi and time support set up properly.","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/07/21/glanceable-displays-installing-raspbian-on-a-raspberry-pi/","loc":"https://jackdewinter.github.io/2019/07/21/glanceable-displays-installing-raspbian-on-a-raspberry-pi/"},{"title":"Glanceable Displays: What Are They?","text":"Preface This is the first article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In this article, I will introduce the concept of glanceable displays and describe how they can be used in an everyday office or home. I will then discuss how the audience for the display is important, including a side bar about my opinion on Spousal Acceptance Factor. Understanding the limitations of your audience is also covered, leading up to developing a set of definitions on what should be displayed. Finally, I will talk about how my family followed those steps to arrive at the requirements for our glanceable display, which has buy in from every member of our household. What Are Glanceable Displays? According to Dictionary.com : 1 noting or relating to information on an electronic screen that can be understood quickly or at a glance: glanceable data; a glanceable scoreboard. 2 enabling information on a screen to be quickly understood: a glanceable design; glanceable interfaces. In essence, a glanceable display is a display that provides information that can be quickly understood by people reading the display. An important qualification of these displays is that the information they display needs to have a broad degree of applicability without any ability for a specific reader to customize the data to their needs. As the display must provide information with no personal customization, it is important to think about various things up front. Who Is the Display For? The audience of the display will drive many of the other choices for the glanceable display. Be honest about who the real audience is for your glanceable display, and what benefits that audience will gain from the display. Negotiate with the audience members and make sure that there is something on the display for each member. If possible, engage with those members to help with the display so their work becomes part of the display, fostering interest from the beginning. The key is for your project to make the jump from \"my toy project\" to \"our useful project\", solving real world issues with real world requirements. If you are not honest about your audience and do not communicate with them sincerely, you will invariably end up missing your goals for the display. Without these, your base requirements will not be correct, and your final product will not be correct. If you only want the display for your personal office, it is perfectly acceptable for you to say that you are the sole audience and geek out on the display. That is, it is acceptable if you are honest about that audience. In that case, just be realistic and don't expect people you didn't include in your original audience to appreciate your display. After all, you made it just for you! Spousal Acceptance Factor As an aside, I have often heard of concerns from people about something they made having a low SAF or Spousal Acceptance Factor. I honestly think that is silly. If there is a low SAF, it probably means that someone did something where their spouse was either not consulted on or not considered with in terms of requirements. It is human to want to share your excitement with family around you but remember that your excitement is not their excitement. Unless you sincerely include them in the audience, the chance of acceptance will remain low. How Are You Going To Display It? In terms of deciding how to display the glanceable information, you really have only one decision to make: are you going to ask the viewer to visit an online site with a device or are you going to provide access to a device at a given location? If you decide on the visit paradigm, you don't have any hardware concerns as the reader is supplying the hardware. With the viewer using their own device to view the display, another decision must be made regarding whether to standardize on a single device type or to support multiple device types. If you decide to support multiple device types, you will probably need to use an intermediate step where you start with support for only the most used device type. Once that device type has been completed, you can then slowly adapt your display to the next device type your viewers are using. You will also need to ensure that you have a clear definition on where your display can be accessed from. If you have a website for your office that you display on phones or tablets, can it be viewed from anywhere, or just within the office's WiFi area? If you decide on the specific location paradigm, you limit your device type to one, but you take on additional hardware concerns. You get to focus on a single device type, but in exchange, you need to provide for the setup and control of that device. Consider the case where the power goes off and then comes back. How will your hardware solution handle that or will someone need to reboot it? Another important consideration is the cost of the hardware and any needed peripherals. Will you reuse existing hardware that you already have, or will you require expenditures? The output from your evaluation of this section should be a choice of a single approach and a list of reasons why the selected approach was chosen. If possible, provide as many details as possible as it will help you in other sections going forward. Also, from your audience's point of view, it will help them understand the decisions that you have asked them to buy in to. What Do You Need To Consider Up Front? As the display is a glanceable display, this means that everyone should be able to view the display with few issues, if any. Common issues to think about are: Near and Far Sightedness A large segment of every population either wears glasses or contacts at some point in their life. Especially as people get older, their eyesight gets weaker and they rely on glasses more frequently. Depending on the differing quality of eyesight of your audience, you may want to consider using larger fonts to enable them to see the screen more clearly. In addition, you may want to consider a high contract mode that includes fewer, but bolder colors to improve visibility. Color Blindness Color blindness, as shown in this article on Wikipedia , is an inability to see differences in either all colors or certain types of colors. Keep in mind that if your audience is not a family environment, the person with color blindness may not disclose that they are color blind up front. If one of your audience is color blind, using colors on your display to indicate certain things is a bad idea. Use shapes or text instead of colors to denote differences in the data being displayed. Dyslexia and Other Reading Disorders Dyslexia, as shown in this article on Wikipedia , is actually a family of similar disorders involving the brain and how numbers and letters are processed when reading. Other reading disorders, such as those mentioned in this Wikipedia article , are often grouped by people as dyslexia, when they are only related. As with dyslexia, keep in mind that if you audience is not a family environment, the person with dyslexia may not disclose that they are color blind up front. Advances in research on reading issues have produced advances such as the Dyslexie font which is specially made for people with dyslexia. Engage with your audience to determine if any such issues apply, what their effects are, and talk with them and research with them on ways to adapt the display to make it easier for them to comprehend. Young Readers Young readers, due to their age, are either still learning how to read or are building their vocabulary as the grow. To assist any young readers that are going to use your display, consider replacing some of the objects that you want to display with pictures that indicate the object's meaning. For ‘older' young readers, keep in mind that their vocabulary is different than yours, and change you designs for the display accordingly. What Are You Going To Display? Once you have all the other sections taken care of, the decision of what to put on the display is almost anti-climactic. After going through the process of identifying your audience, the type of display to use, and any considerations for your audience, you have probably defined at least one or two things to display along the way. At the very least, armed with the information above, you can engage with your audience in a brainstorming session that will allow you to get buy-in from them. Two important things to remember at this stage: soliciting help and iterative development. Don't be afraid to ask your audience to help you in the design for the display. That can take any form you want, from design the information for the display with them there to asking them to create the displays and presenting them to the entire audience. Remember, for the glanceable display to be successful, you will need buy-in from your audience. Having them help with the work will do that. Iterative development means that you and your audience are going to try and make something that works, but you do not expect it to be perfect on the first try. You and your audience may be confident think something works when you look at it initially, but over time that confidence may change. Don't be afraid to iterate on the design, keeping the things that work and changing the things that don't work. Our Discussion About Our Glanceable Display Understanding that any decision would affect my entire family, I asked if we could talk about it after dinner one night. In discussing the possibility of a display, we all agreed that there were common things that it would be nice to see. These things came down to 3 categories: calendar related, weather related, and other. As we all have busy lives, the calendar was a slam dunk. Our whiteboard calendar didn't get updated when it should, which left us looking at our online calendars on our phones. But even with our online calendars, it was a pain to remember to invite other family members to any events we had, regardless of whether or not they were part of the event. Having a single place to display that information keyed to a given individual would clear things up and simplify things a lot. Information on the weather was another category, mostly due to the Seattle weather. While my son wears the same type of clothes every day, my wife and I vary our clothing by the type of weather and activities we have planned. Having that advance knowledge of weather would cut down on having to actively seek that information from a number of online sources. After those two big categories, there were also some other little things brought up. Not having a good place to put them, the \"others\" category was formed. The discussion then moved to decide which class of glanceable display to use, and our family made a simple decision to go with a monitor driven by a web page hosted on a Raspberry Pi. We all agreed that we wanted something that would replace a seldom updated whiteboard calendar in our kitchen. It needed to be big enough to show several weeks' worth of calendar at a time, to allow us to plan that far out. We also wanted to make sure we kept each other honest, so we explicitly wanted it not tied to any of our personal computers and tied to a location that we know we all frequent: the kitchen. The choice of the Raspberry Pi satisfied these concerns pretty easily. From a hardware point of view, I had a spare one at home from a project I had wanted to do, but never started. From an operating system point of view, I have enough knowledge of Linux systems that I was confident that I would be able to handle the configuration. Finally, I was prepared to take the challenge on of setting up the system and working with my family to define the elements of the display with their input at ever step. The Decisions for Our Glanceable Display So, from those discussions, I arrived at the following. Audience The audience was our family. Display The display itself would be a simple Raspberry Pi with a monitor that was left from a project that I had (almost) worked on. The display would be located in the kitchen in a location that would be visible to family members, but not visible outside of the house. Considerations In our family, we don't have any members that have vision issues other than needing glasses. As such, the primary concern is that we can all ready the text on the display from 2 meters or 6 feet away. What To Display? The primary two goals for the display were to display our calendars and the weather for the next 5 days. Any enhancements of those two goals were fine, as long as the primary goals were not ignored. Some of the other ideas that were floated may seem funny, but they nicely fit into our other category: chore lists from Trello quote of the day number of days until Christmas number of people on the International Space Station current exchange rates Wrapping It Up The rest of the articles in this series will detail my family and I worked on our glanceable display. Based on the information from above, we have had good success with our display. Most of the fixes to the display were tweaks to the information being displayed, rather than the Raspberry Pi itself. I emphatically stand by the previous sections about and making sure you understand and engage your audience. I credit my family, the audience for our glanceable display, with having an honest conversation on what would help, and getting the buy in from them from the beginning. What Was Accomplished? This article presented a set of things to consider when creating a glanceable display, followed by notes on how my family followed that pattern to arrive at our requirements for our glanceable display. Those considerations started with defining your audience, proceeded to understanding your audience, and finally arriving at a set of things that you and your audience want to display. I cannot promise that if you follow these considerations that your journey will be as successful as ours. However, I believe I can say with some certainty that it will help you along the way with your own journey. To succeed, you need information to help guide you, and each of the considerations above will help you inch closer to that success. What's Next? In the next article in this series, Glanceable Displays: Installing Raspbian on a Raspberry Pi , I walk through the steps I took to set up a Raspberry Pi as our glanceable display of choice. It documents the journey from installation on to a blank MicroSD card to a bare bones installation that enabled remote SSH access.","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/07/14/glanceable-displays-what-are-they/","loc":"https://jackdewinter.github.io/2019/07/14/glanceable-displays-what-are-they/"},{"title":"Starting With GitHub: Setting Up Credentials For My Personal Website","text":"Introduction Part of any project I do, private or open-source, is to set up a version control system and securing access to that version control system. In addition, it is always a high priority for me to make sure that the secure access follows best common practices on security, as it is protecting work that I am doing. Using Git as my version control system of choice, and GitHub as the open source repository host of choice, it made logical sense to make my website platform of choice GitHub Pages 3 . As such, in setting up this website I needed to make sure I had Git 1 and more specifically GitHub 2 setup, and setup securely. This article details the actions and choices I made in setting up my access to GitHub for my blog. It details how I followed the GitHub Pages instructions for creating a personal website and creating a personal repository on GitHub to achieve that. Then it describes the two modes of accessing GitHub, SSH and HTTPS, and why I chose SSH. Finally, it provides detailed notes on how I generated a SSH keypair specifically for GitHub, and configured both my local Git and the remote GitHub to use them. Getting Started With GitHub There are many good articles out there on how to install Git for different operating systems, so I feel it is safe to assume that anyone reading this can do the research needed to install Git for their operating system. Setting up access to GitHub is even easier, as the folks at GitHub have provided excellent instructions. Start the setup by simply going to the GitHub Home Page and following their instructions. The pivotal part is that to start using GitHub in a serious manner, you need to authenticate yourself to the GitHub servers. Thus, the workflow will either allow you to login, if you already have an account, or create a new account, if you don't have an account. Having already dealt with a couple of open source projects, I logged on to my account jackdewinter without any issues. After I logged in, the browser deposited me on my home page. From there I was able to see any projects that I had either contributed to or filed an issue against. Just starting in my Open Source journey, the contents were only a couple of projects that I had filed issues with. Prior to this point, I had no need to authenticate my Git client with GitHub as I was just downloading from public repositories. Having done some research on GitHub Pages, I knew that setting up my own website with GitHub would require me to create my own repositories. As such, my next task was to create that repository. Creating My First GitHub Repository The GitHub Pages home page has a really simple formula on their webpage for setting up personal webpages. The first step is pretty easy: make sure that that the name of the repository is my GitHub user id (in my case jackdewinter ) followed by .github.io . When the creation of my repository finished, GitHub deposited my browser at the base of my new repository: jackdewinter/jackdewinter.github.io . The remaining steps in the formula dealt with cloning the repository, defining a sample index.html file for the website, and pushing that code back to the repository. While I was familiar with those concepts, I wasn't shy about checking back with the Git Basics documentation on the Git website when I forgot something. From there I was able to find the correct helper article on what I need to accomplish within 2-3 clicks. In GitHub, unless you mark a repository as private, everyone can see that repository and read from that repository. As my website's repository is public, reading wasn't a problem. However, pushing the code back to my repository would be writing, and that was a problem. Each GitHub project has a list of who can write to it and the creator of the project is on that list by default. But to write to the project, I needed my local Git tool to login to GitHub when needed and authenticate itself. To do this securely, I was going to have to dive into credentials. GitHub Authentication: SSH vs HTTPS Any time you log in to a website or to a program to gain specific privileges, you establish your credentials by supplying your user id and password. You tell the website or program \"I can prove I am me, let me see my stuff!\". The GitHub website is no different that any of those sites. If you want to be able to see any of your private stuff or write to your stuff, it needs to verify who you are. Going to the Authenticating with GitHub from Git , there are two choices that allow us to connect to GitHub: HTTPS and SSH. Both of these are valid options, allowing for enhanced security when Git connects to GitHub. Each of these options has different things going for and against it. After doing some research, it seemed to me to break down to the following: SSH HTTPS set up keys set up credential manager setup is more involved easy setup more secure less likely blocked by firewall Looking at this information, I decided to go with SSH as I wanted to opt for more security. SSH Access to GitHub During my research at the GitHub site, I found this very good page on SSH over the HTTPS port . In it, they explain that there is a simple test to see if SSH will work from your system to GitHub. When you execute the following command: ssh -T git@github.com it will either return one of two responses. If it returns with: > Hi *username*! You've successfully authenticated, but GitHub does not provide shell access. then you can access GitHub via SSH without any issues. If you see the other response: > ssh: connect to host github.com port 22: Connection timed out then you have to setup SSH to connect to GitHub over the HTTPS port. This access can be verified with a small modification to the above command: ssh -T -p 443 git@ssh.github.com The command is now trying to establish a SSH session over port 443, and if you get the You've successfully... response, it's working fine. Running these tests myself, I found that I got a timeout on the first command and a success on the second command. Following the article, it recommends changes to ~/.ssh/config 4 to include the following: Host github.com Hostname ssh.github.com Port 443 The next time, when I executed the ssh -T git@github.com command, the response was the You've successfully response. Now I was ready to set up the SSH keys. Unique SSH Keys Going back to the Authenticating with GitHub from Git , the next step was to generate a new SSH key pair and add it to the local SSH keyring. The page that points to generating a new key is pretty detailed, so I won't try and improve over GitHub's work. On my reading of the page, it seems to assume that if you will only have 1 key pair 5 generated and that you will reuse that key pair for GitHub. I have issues with that practice, so I want to talk about it. Having a bit of a security background from my day job, I want to limit exposure if something gets broken. Just from a quick search, there are articles by Leo Notenboom , Malware Bytes Labs , and WikiHow that all describe how you should have different passwords for each account, and in many cases, use a password manager. And to be honest, that was just the first 3 that I clicked on. There were a lot more. I can sum up and paraphrase the justification raised in each of those articles by posing a single question: If someone breaks your password on one site, what is your exposure? If you have one password for all sites, then whoever breaks your password has access to that one site. If you have a different password for each site, the damage is limited to one site, instead of all sites using that password. In my mind, using a key pair for credentials is a similar concept to using a user-id and password for credentials. Therefore, it followed that if I follow good security practices for passwords, I should also follow the same practices for key pairs as credentials. Generating a New Key For GitHub To ensure I have secure access to GitHub, I followed the instructions for generating a new key . To generate a distinct key pair for GitHub, I made one small modification to the instructions: I saved the new key pair information with the filename github-key instead of the default id_ras . This resulted in the files ~/.ssh/github-key and ~/.ssh/github-key.pub being created as the key pair. With those files created, I followed the remaining instructions for setting up ssh-agent and uploading the key information to GitHub, replacing any occurrence of id_ras with github-key . With that accomplished, I had a specific key pair specifically for GitHub and it was registered locally. I also had setup GitHub with the public portion of the credentials using the contents of ~/.ssh/github-key.pub , as instructed. The only remaining step was ensure that any SSH connections to GitHub would use the GitHub credentials. Doing a bit more research on the SSH configuration files, I quickly found that there was built in support for this by adding the following to my ~/.ssh/config file: Host github.com User git PreferredAuthentications publickey IdentityFile /c/Users/jackd/.ssh/github-key Note The location of ~/ on my Windows machine is %HOMEDRIVE%%HOMEPATH%\\ or c:\\Users\\jackd\\ . The format for the IdentityFile property is a standard Unix path format. This requires a translation from the Windows path format C:\\Users\\jackd\\.ssh\\github-key to the Unix path format of /c/Users/jackd/.ssh/github-key . Combined with the change from earlier in this article, my ~/.ssh/config file now looked like: Host github.com Hostname ssh.github.com Port 443 User git PreferredAuthentications publickey IdentityFile /c/Users/jackd/.ssh/github-key Testing Against GitHub: GitHub Pages Having performed a number of thorough tests of the above steps, everything passed without any issues! Now it was time to try and push some commits for the blog to GitHub. To create a directory for the GitHub project, I largely followed these instructions detailed in the companion article on setting up your own static website. I then followed these instructions , adding the remote repository to my local configuration with the following line: git remote add origin jackdewinter/jackdewinter.github.io Having associated the directory with the remote repository, the final test was to make a change to the directory, commit it, and push it to the remote. For this test, I used a very simple index.html file: < html > < body > Hello world! </ body > </ html > Adding the file to the directory, I staged the file and committed it with: git add index.html git commit -m \"new files\" and then pushed it to the remote repository with: ssh-agent git push origin master --force Crossing my fingers, I waited until I got a response similar to: Counting objects: XXX, done. Delta compression using up to 8 threads. Compressing objects: 100% (XX/XX), done. Writing objects: 100% (XX/XX), XXX.XX KiB | 0 bytes/s, done. Total XX (delta XX), reused XX (delta XX) remote: Resolving deltas: 100% (XX/XX), completed with XX local objects. To blog:jackdewinter/jackdewinter.github.io.git XXXXXX..XXXXXX master -> master With no error messages, I double checked the repository at https://github.com/jackdewinter/jackdewinter.github.io and was able to see the index.html file present in the repository. Following through with the instructions for GitHub Pages, I then went to https://jackdewinter.github.io and was able to see the text \"Hello world!\" on in the browser. What Was Accomplished This article started with my creation of a GitHub repository to contain the files for my personal website using GitHub Pages. To securely access the repository, I chose the SSH protocol and discovered that I needed to employ SSH over HTTP. For enhanced security, I described a solid reason for wanting a unique SSH key for GitHub. Following that advice, I generated a new key and then changed the ~/.ssh/config file to use SSH over HTTPS and to point to that newly generated keypair. Finally, I committed a sample file to the project and was able to see it pushed successfully to the remote repository, and displayed as my personal home page. Git is an open-source source control tool. For more information, look here . ↩ GitHub is a common repository for open-source projects. For more information, look here . ↩ GitHub Pages are a feature of GitHub that allow people to host their personal websites on GitHub. For more information, look here . ↩ My primary system is a Windows 10 machine, so instead of modifying the ~/.ssh/config file, I modified the %HOMEDRIVE%%HOMEPATH%\\.ssh\\config file. On my system, that file is the c:\\Users\\jackd\\.ssh\\config file. ↩ When a SSH key is generated, it comes in two parts. The private part is kept on the user's system while the public part can be distributed to any interested parties. Together they are referred to as a key pair. ↩","tags":"Integrating Technology","url":"https://jackdewinter.github.io/2019/07/07/starting-with-github-setting-up-credentials-for-my-personal-website/","loc":"https://jackdewinter.github.io/2019/07/07/starting-with-github-setting-up-credentials-for-my-personal-website/"},{"title":"Extended Markdown Examples","text":"This is a continuation of the previous cheat sheet for my website. This article specifically addresses any extensions that are not part of the base Markdown specification. Each section here represents an extension that I have enabled on my website. The formatting from the previous page is continued, with one small exception. The title of each section specifies the name of the extension instead of the name of the feature being documented (see Admonitions ). If an extension contains more than one feature, such as the Extra extension, the title specifies the name of the extension, a dash, and the name of the feature (see Footnotes ). Introduction The authors of the Python Markdown Package anticipated the addition of extra features. To ensure people would have choice, the base package can be extended using configuration . The Markdown extensions have been activated on my website by inserting the following text into my peliconconf.py: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 MARKDOWN = { 'extension_configs' : { 'markdown.extensions.extra' : {}, 'markdown.extensions.admonition' : {}, 'markdown.extensions.codehilite' : { 'css_class' : 'highlight' }, 'markdown.extensions.meta' : {}, 'smarty' : { 'smart_angled_quotes' : 'true' }, 'markdown.extensions.toc' : { 'permalink' : 'true' }, } } Table Of Contents [TOC] Introduction Table Of Contents CodeHilite - Code Blocks With Line Numbers Extra - Footnotes Extra - Abbreviations Extra - Definition Lists Smartypants Admonitions CodeHilite - Code Blocks With Line Numbers ``` #!python # Code goes here ... ``` 1 # Code goes here ... Extra - Footnotes Here's a simple footnote,[&#94;1] and here's a longer one.[&#94;bignote] [&#94;1]: This is the first footnote. [&#94;bignote]: Here's one with multiple paragraphs and code. Here's a simple footnote, 1 and here's a longer one. 2 Extra - Abbreviations The HTML specification is maintained by the W3C. *[HTML]: Hyper Text Markup Language *[W3C]: World Wide Web Consortium The HTML specification is maintained by the W3C . Extra - Definition Lists Apple : Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange : The fruit of an evergreen tree of the genus Citrus. Apple Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange The fruit of an evergreen tree of the genus Citrus. Smartypants advantage is that code blocks are unaffected - apostrophe ' by itself - apostrophe as in 'quote me' - quotations mark \" by itself - quotations mark as in \"quote me\" - replacement of multi-character sequences with Unicode: << ... -- >> --- apostrophe ‘ by itself apostrophe as in ‘quote me' quotations mark \" by itself quotations mark as in \"quote me\" replacement of multi-character sequences with Unicode: « … – » — Admonitions broken down into section by the way that the Elegant theme colors the admonitions !!! note You should note that the title will be automatically capitalized. Note You should note that the title will be automatically capitalized. !!! important \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! hint You should note that the title will be automatically capitalized. Hint You should note that the title will be automatically capitalized. !!! tip \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! warning You should note that the title will be automatically capitalized. Warning You should note that the title will be automatically capitalized. !!! caution \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! attention \"\" You should note that this will have no title due to the empty title. You should note that this will have no title due to the empty title. !!! danger You should note that the title will be automatically capitalized. Danger You should note that the title will be automatically capitalized. !!! error \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. This is the first footnote. ↩ Here's one with multiple paragraphs and code. ↩","tags":"Website","url":"https://jackdewinter.github.io/2019/06/30/extended-markdown-examples/","loc":"https://jackdewinter.github.io/2019/06/30/extended-markdown-examples/"},{"title":"Standard Markdown Examples","text":"As I started writing my articles for my blog, I realized I needed something. To help me write articles using this flavor of Markdown 1 , I needed my own cheat sheet. My hope is that it provides clear guidance on which aspects of the various forms of Markdown worked for me, and which didn't. Introduction Horizontal Break Headings Text Emphasis Numbered lists Bulleted List Block quote Code Block Tables Links Local Links Remote Links Download Links ) Images Introduction I am writing articles and pages on Pelican 4.0.1 2 using the Elegant 3 theme, therefore I want to make sure I have a cheat sheet that is specific to this dialect of Markdown. The base Markdown used for Pelican uses the Python Markdown Package which (with 3 exceptions) follows John Gruber's Markdown definition very literally. Pelican configuration also supports providing Markdown with additional configuration that enables other features. Those features are documented separately in the next page . The format of this cheat sheet is simple. Each section is separated from the next with a horizontal break and the name of the section. Any notes regarding that section are placed at the top of the section in point form, to ensure they are brief. Then a Code Block section is used to show the literal code used to produce the effects that are presented right after the code block. Horizontal Break A horizontal break occurs after 3 or more hyphens. --- A horizontal break occurs after 3 or more hyphens. Headings # Heading Level 1 ## Heading Level 2 ### Heading Level 3 Heading Level 1 Heading Level 2 Heading Level 3 Text Emphasis two spaces at the end of a line will be equivalent to <br/> This text is **bold** and this text is also __bold__. This text is *italic* and this text is also _italic_. This text is **_italic and bold_**, but no two spaces at end. Single ```line``` block. Inline `code` has ```back-ticks like this ` around``` it. This text is bold and this text is also bold . This text is italic and this text is also italic . This text is italic and bold , but no two spaces at end. Single line block. Inline code has back-ticks like this ` around it. Numbered lists to maintain the indentation, place 4 spaces at the start of the line 1. One New para. Blah 2. Two - unordered - list 3. Three 1. ordered 2. list - unordered - list 3. items One New para. Blah Two unordered list Three ordered list unordered list items Bulleted List to maintain the indentation, place 4 spaces at the start of the line - This is a list item with two paragraphs. This is the second paragraph in the list item. You're only required to indent the first line. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. - Another item in the same list. - Bulleted item - Bulleted item This is a list item with two paragraphs. This is the second paragraph in the list item. You're only required to indent the first line. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Another item in the same list. Bulleted item Bulleted item Block quote > This is the first paragraph of a blockquote with two paragraphs. > Lorem ipsum dolor sit amet, > consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. > Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. > > This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. > > This is the first level of quoting. > > > This is nested blockquote. > > Back to the first level. This is the first paragraph of a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. This is the first level of quoting. This is nested blockquote. Back to the first level. Code Block line numbers can be added via extensions ```text Make things only as complex as they need to be. ``` ```Python # Blogroll LINKS = ( ('Pelican', 'Pelican', 'http://getpelican.com/'), ) ``` Make things only as complex as they need to be. # Blogroll LINKS = ( ( 'Pelican' , 'Pelican' , 'http://getpelican.com/' ), ) Tables colons can be used to align columns. | Column1 | Column 2 | Column 3 |---|---|---| | Value 1 | Value 2 | Value 3 | | Value 4 | Value 5 | Value 6 | | Value 7 | Value 8 | Value 9 | | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | Column1 Column 2 Column 3 Value 1 Value 2 Value 3 Value 4 Value 5 Value 6 Value 7 Value 8 Value 9 Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 Links Local Links {filename} tag indicates location in the content folder. [About Page]({filename}/pages/landing-page-about-hidden.md) About Page Remote Links proper URL indicates a remote website [Python Package Index](https://pypi.org) Python Package Index Download Links download links are not natively supported in Markdown must explicitly create HTML text inline to achieve that Creating a link to a file to download, not display, is not natively supported in markdown. [Pelican Brag Document (display)]({static}/images/markdown-1/pelican.txt) <a href=\"{static}/images/pelican.txt\" download>Pelican Brag Document (download)</a> Pelican Brag Document (display) Pelican Brag Document (download) Images {filename} tag indicates location in the content folder. ![python logo]({static}/images/markdown-1/python_icon.png) Markdown allows for HTML pages to be written using a simple text editor with no knowledge of HTML. ↩ Pelican is a Static Site Generator written in Python. ↩ The Elegant theme's repository is here . ↩","tags":"Website","url":"https://jackdewinter.github.io/2019/06/29/standard-markdown-examples/","loc":"https://jackdewinter.github.io/2019/06/29/standard-markdown-examples/"},{"title":"My Gallery Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada.","tags":"Website","url":"https://jackdewinter.github.io/2010/12/06/my-gallery-test/","loc":"https://jackdewinter.github.io/2010/12/06/my-gallery-test/"},{"title":"My Long Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada. Para 1 Donec quam neque, vulputate quis purus at, tempus tincidunt neque. Sed posuere eros eu massa lobortis varius. Ut condimentum elit eros. Sed vel nunc vitae nibh aliquet vestibulum vitae quis justo. Sed vel ligula turpis. Aliquam et mi mollis, suscipit sapien vel, molestie enim. Morbi sodales, dui nec congue tristique, risus mi luctus nulla, vel egestas sem nulla quis augue. Nulla vitae efficitur odio, quis egestas ex. Pellentesque a est viverra, fringilla dui ac, laoreet purus. Suspendisse porta aliquet nunc et pulvinar. Integer ante felis, tincidunt eu ipsum a, imperdiet convallis augue. Cras vulputate sapien sit amet metus placerat, sed congue turpis tempus. Nunc pretium ac dolor eget tincidunt. Para 2 Nunc id tortor lectus. Quisque fermentum sem ut elit ultricies sollicitudin. Curabitur blandit, elit at suscipit mattis, purus lectus eleifend felis, id rutrum neque sapien vitae arcu. Aenean elementum lacus tristique purus facilisis placerat. Nunc pharetra lorem ut finibus blandit. Aenean scelerisque elit nec malesuada accumsan. Proin eu orci eget odio scelerisque viverra a ac nulla. Vestibulum elementum lobortis quam. Morbi porta rutrum mi, quis laoreet nunc dictum at. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Ut in lobortis massa. Para 2a Phasellus et leo in nunc fermentum vulputate. Nullam sed interdum augue. Duis eu dignissim eros. Mauris pretium turpis non purus porta, non consequat enim rutrum. Fusce dui odio, consequat in rhoncus sed, interdum vulputate quam. Nullam nec dolor ex. Curabitur dapibus vestibulum odio at sodales. Para 2b Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum.","tags":"Website","url":"https://jackdewinter.github.io/2010/12/03/my-super-long-post/","loc":"https://jackdewinter.github.io/2010/12/03/my-super-long-post/"},{"title":"My Short Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada.","tags":"Website","url":"https://jackdewinter.github.io/2010/12/03/my-super-short-post/","loc":"https://jackdewinter.github.io/2010/12/03/my-super-short-post/"}]};