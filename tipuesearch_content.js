var tipuesearch = {"pages":[{"title":"What is Software Quality?","text":"When introducing myself to someone professionally, I usually start with the normal \"Hi, my name is …\" that is boiler-plated on nametags the world over. Getting past that initial point, if the person is so inclined, they ask that always fun lead off question \"So, what do you?\" For me, I always respond with \"I am an SDET\" 1 , to which anyone not in the software industry replies back with \"Um.... What is that?\" Spewing out \"It means I am a Software Development Engineer in Test.\", I wait for the response that most people use: \"Oh, so you are a tester.\" Often with gritted teeth, I try and explain that testing is only a small part of what I do. If I think they are still listening, I given them my quick elevator pitch that emphasizes that I focus on helping to produce good quality software by helping to increase the quality of the teams, the projects, and the processes that I am tasked to assist with. Approximately 60-70% the time I win people over with the elevator pitch, and a pleasant conversation continues. The next 20-30% of the time, usually with people not in the software field, I get blank stares and they fixate on the \"test\" in the title rather than the \"quality\" in my description. The remaining people are usually Software Development Engineers or SDEs 2 that for one reason or another, start to tune out. For the percentage of people that I win over, they seem to understand that I focus on quality, but the follow up question is almost always: \"What does quality software mean to you?\" Where do we start? For me, I almost always start at the beginning with requirements. Whether they are spoken or written down, each project has a set of requirements. It could be the requirements are to \"explore my ability to use X\" or \"fix the X in the Y project\" or \"create a project that can help me X\", but every project has requirements. In the software development industry, requirements are often presented to teams that are hard to deal with or are left to the developers to write themselves. This practice is so prolific that Scott Adam's Dilbert site has pages and pages of instance where requirements are talked about . One example is when a manager talks to their team and informs them that some process needs to be faster by 5%. Do they have enough information from that manager to understand the context of the requirement? Do they expect that increase by a specific time to meet their own goals? What does that requirement look like? How do they know when they have achieved it? Is it achievable? If it is achievable, how do they measure progress towards that goal? These are some of the core questions that I believe need answering. As those questions are at the front of my mind, when someone asks me how I define software quality, the first thing I immediately think back to is a course that I once took on setting S.M.A.R.T. requirements . In that class, the main focus was on taking unrefined requirements and curating them to a point where they could be more readily be acted upon. The instructor made a very good argument that each requirement must be Specific, Measurable, Assignable, Realistic, and Time-Related. When it comes to software quality, I believe those same questions needs to be asked with regards to any of the requirements teams put on their software. But to ask those questions properly, we need to have some context in which to ask those questions. To establish that context, it is helpful to have some guidelines to provide a framework for the requirements. Establishing Some Guidelines: The Four Pillars A good general article for anyone interested in software quality is the Wikipedia article on Software Quality . In fact, when asked by people where to get started in the software quality area, I often refer them to this article solely because of the excellent diagram in the Measurements section on the right side of the page. 3 The diagram in the Measurements section correlates very closely to what I believe are the four pillars of software quality: Reliability, Maintainability, Efficiency, and Security. The diagram then shows how their pillars relate to other attributes: Application Architecture Standards, Coding Practices, Complexity, Documentation, Portability, and Technical/Functional Volumes. From there, it provides more lists of how to break things down, with many references to other articles. In short, it is a great place to start from. Measuring Software Quality Before proceeding to talk about the pillars themselves, I feel strongly that we need to discuss the categories that I use for measuring the metrics talked about in the Wikipedia article. My intention is that by talking about the metrics before discussing each of the pillars, you can start building a mental model of how to apply them to your projects as you are reading about them. From my point of view, making that mental transition from something abstract that you read about to something concrete that applies to your work is essential to serious forward momentum on software quality. These metrics typically fall into two categories: seldom violated metrics and positive momentum metrics. The seldom violated metrics category contains rules that define rules that are pivotal to the quality of your project. Each rule are a combination of a given metric and a maximum or minimum weighed against that metric. As a guideline, teams should only ignore these rules on a case by case basis after providing a reason that is good, defensible, and documented. Examples of such metrics are Service Level Agreements (SLAs), Static Code Analysis (SCA) results, and Test Failure Rates. Examples of rules are \"the TP99 for the X API is Y millisecond\" or \"all PMD warnings (Java SCA tool) must be following with a minimal of suppressions\". Furthermore, to make these rules useful and to keep your team honest, your team needs to publish the selected metrics, with a description of what the metrics are, how your team measures those metrics, and why your team is measuring them. The positive momentum metrics category is usually reserved for metrics that are being introduced to an already existing project. When introducing software quality metrics into an already existing project, it is not realistic to expect those metrics to be adhered to in an instant. It is more realistic to expect positive momentum towards the goal until the point when your team achieves it, at which point is moves to the desired seldom violated metrics category. As such, a measure of the momentum of these metrics is used, and is hopefully in a positive direction. Similar to the previous category, your team should publish information about the selected metrics, with the added information on when your team feels they will translate it from the positive momentum category to the seldom violated category. Being consistent on these chosen metrics is very important. While dropping a metric looks better on any reporting in the short term, it usually negatively impacts the software quality, perhaps in a way that is not obvious until later. Adding a new metric will show lower the measured quality in the short term, but increases the measured quality in the long term. Your team can negate the short term impact by paying the immediate cost of making the new metric a seldom violated metric, but that has to be weighed against the other priorities for your project. As with everything, it is a balancing act that needs to be negotiated with your team. Exploring The Four Pillars Having established that S.M.A.R.T. requirements and the two categories for metrics from the previous sections are useful in measuring software quality, the focus of the article can return to the guidelines: the four pillars. Each one of these pillars will look at your software project from a different angle, with the goal of providing a set of data points to formulate a coherent measurement of software quality for that project. In the following sections, I strive to describe each of the four pillars, providing a jumping off point to another article that describes that pillar in a more comprehensive manner. I firmly believe that by providing metrics for each pillar that are specific to your project, with each of those metrics properly categorized into the two measurement categories documented above, that your team will take a decent step forward in clearly defining software quality for your project. Reliability The essence of this pillar can be broken down into two questions: Does the software do the task that it is supposed to do? Does the software execute that task in a consistent manner? Reliability is one of the areas in which \"pure\" testing shines. A lot of the tests that SDEs, SDETs, and testers are asked to write specifically verify if a given object does what it is supposed to do. Unit tests determine whether an individual software unit, such as a class, performs they way it is supposed to. Functional tests or integration tests take that a step higher, determining whether a group of related software units do what they are supposed to do. Another step higher are the scenario tests, which determine whether the software project, as a whole, responds properly to various use cases or scenarios that are considered critical to its operation. Finally, end-to-end tests or acceptance tests determine whether or not a group of projects respond properly from an end user's perspective. This pattern is so widely used, any search for test pyramid , will find many variations of the same theme. Different articles on the subject will stress different points about the pyramid, but they will all generally look like this: This pyramid, or other similar pyramids, are interpreted by authors to indicate a specific things about the tests, to highlight the position of their article. Some of these interpretations are: An article on test volume will typically stress that ~70-80% of the tests should be at the unit test level, ~10-15% at the functional test level, ~5-10% at the scenario level, and ~1-3% at the end-to-end level. An article on test frequency will typically stress that tests near the bottom of the pyramid should complete within 60 seconds and be executed every time the source code is checked in. Tests near the top of the pyramid may take minutes or hours and should be executed once a week. An article on test fragility will typically stress that tests near the bottom of the pyramid are closer to their components, the expectation is that they will not fail. Tests near the top of the pyramid require more orchestration between projects and teams, and therefore, are more likely to failure do to environmental or other reasons. While all of these interpretations have merit, the critical point for me is the issue of boiling down that information to a small number of bite sized observations that can be easily measured and communicated. In the upcoming article Software Quality: Reliability , I will delve more into breaking the Reliability pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. Maintainability The essence of this pillar can be broken down into one question: If you are asked to change the software to fix a bug or introduce a new feature, how easy is it to change the software, how many surprises do you expect to encounter, and how confident will you be about the change afterwards? The best, and most comedic, form of asking this question is captured by this cartoon from OSNews : Maintainability is a single pillar that encompasses the most diverse types of processes and measurements of any of the pillars. The reason for this is that maintainability is often a word that is used without a lot of provided context. For me, a good way to think about maintainability is that it is the cleanliness of your project. Different people will have different experiences, and after asking different people about how \"clean\" the project is, the collected answers will almost certainly by different. Try this in your office with your colleagues. Point to a given area of your office and ask 2-5 people how clean a given area, such as your desk is. Instead of accepting a single answer, dig in a bit as to why they answered the way they did. Most likely, you will get as many distinct answers as people that you talk to. This exercise illustrates how hard it is to give a good answer to how maintainable software a given piece of software is. The best way to provide metrics for maintainability is usually with various Static Code Analysis tools. Almost every mature language has at least one tool to do this, and each tool usually measures a fair number of metrics. These metrics will use established (and sometimes experimental) industry practices to look at the source code of your project and determine if there are issues that can be addressed. In addition to those metrics, those same tools often look for \"problematic\" and \"sloppy\" code. Problematic code is usually some manner of pattern that a fair number of experts have agreed is a bad thing, such as appending to a string within a loop. Sloppy code is usually things like having a variable or a parameter that is not being used, or a forgotten comment on a public method. In addition to Static Code Analysis, teams must continue to strive to have a good set of documentation on what the project is doing, and regularly maintain that documentation. While the \"correctness\" of the documentation is harder to measure than source code, it is pivotal for a project. How much of the information on the various projects that your team supports is in the head of one or two individuals? What is going to happen if they leave the team or leave the company. Your team should not need volumes of information on every decision that was made, but as a team, it is imperative to document the major decisions that affect the flow of the project. It is also a good idea to have solid documentation on building, deploying, and executing the project. Imagine yourself as a new team member looking at the software project and any documentation, and honestly ask yourself \"How much would I want to run away from that project?\" If the honest answer from each member of the team is something similar to \"I'm good\", you probably have a decent level of documentation. A Note On Static Code Analysis Before delving deeper into maintainability, I want to take a minute to talk about Static Code Analysis. Typically, Static Code Analysis is used as a gatekeeper for maintainability, and as such, any suggestions should be strictly followed. However, Static Code Analysis tends to be an outlier to the gatekeeper rule in that the metrics need to be \"bent\" every so often. This \"bending\" is accomplished using some form of suppression specified by the Analyzer itself. Static Code Analyzers tend to fall into two main categories: style and correctness. Any warnings that are generated by a style analyzer should be addressed without fail. In terms of stylistics, there are very few times where deviating from a common style are beneficial, and as such should be avoided. As stylistics can vary from person to person when writing code, it is useful to supplement the style analyzer with an IDE plugin that will reformat the source code to meet the team's stylistics, with the Static Code Analyzer acting as a backstop in case the IDE formatting fails. Warnings generated by correctness analyzers are more likely to require bending. Most correctness analyzers are based on rules that are normally correct, but do have exceptions. As such, your team should deal with these exception by having a follow up rule on when it is acceptable to suppress the exceptions, and specifically on a case-by-case basis. It is also acceptable to suppress the exception after generating a future requirement to address the exception, if your team is diligent on following up with these requests. In both cases, it is important to remember that SCAs are used to help your team keep the project's maintainability at a healthy level. Back to Maintainability In the upcoming article Software Quality: Maintainability , I will delve more into breaking the Maintainability pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. I will do this by presenting the 4-5 metrics that I consider to be useful as well as both patterns and anti-patterns to avoid. [ED: Need to rephrase that last sentence.] Efficiency The essence of this pillar can be broken down into one question: Does the software execute that task in a timely manner? Similar to my analogy of maintainability being the cleanliness of your software, efficiency is whether or not your software is executing \"fast enough\". Coming up with an answer to a question on whether or not something is \"fast enough\" is usually pretty easy. But when you ask for a definition of what \"fast enough\" means, that is when people start to have issues coming up with a solid answer. In my experience, a large part of the reason for that vagueness is usually not having a good set of requirements. As an example, let's figure out what \"fast enough\" means for two different video games that my family plays: Civilization and Rocket League. For the game Civilization (in multiplayer mode), the big delays in the game are the human interactions and decisions required before a player ends their turn. It is also very important that all of the information get conveyed between turns so that the multiplayer server can accurately record actions in a fair and just manner. For this game, \"fast enough\" for the software is largely dwarfed by the delays that the players introduce. However, if we have a game with 12 players, 2 of them human and the other 10 using the game's AI players, then we can start to formulate what \"fast enough\" is for the AI players. It really depends on the context. Rocket League is a different story. Rocket League is a sequel to the game \"Supersonic Acrobatic Rocket-Powered Battle-Cars\" released in 2008. In this game, you play a game of arena soccer using rocket powered cars, each match consisting of a series of games between teams of 1-3 players. Unless there is a LAN tournament between professional teams, it is very rare for more than one player to be in the immediate vicinity of their teammates, and often players are from different states/provinces and even countries. For the client software on the player's computers, \"fast enough\" is measured by latency and packet loss. With each player's action being relayed to the server and then back out to the other players, any packet loss or increase in latency will impact the server's reaction to various inputs from the player's controllers. For this type of game, \"fast enough\" depends on a good network connection and a server that is able to process many actions per second. As you can see from the video game example, efficiency greatly depends on what the requirements of the software are. In the upcoming article Software Quality: Efficiency , I will delve more into breaking the Efficiency pillar into S.M.A.R.T. requirements and I provide suggestions on how it can be measured. Security The essence of this pillar can be broken down into one question: How easy is it for a third party to perform malicious actions with your software? That is one dramatic question. \"Perform malicious actions.\" Wow! I have read all sorts of articles on various news sites about those, but surely they cannot affect my software? That is usually one of the first reactions of a lot of software developers. Just 10 minutes with a security researcher can open your eyes to what is possible. To understand this better, pretend that your software project is on a slide, being viewed through a microscope. If you look at the slide without the microscope, you just see your software on the slide, pretty much the same as any other slide. However, if you increase your magnification by one order of magnitude, you see that your project includes your source code and components developed by other people. You may be following proper security practices, but did they? Another order of magnitude down, and you are looking at the low level instructions for your project and any included components. Once the component was assembled, could a third party have added some malicious code to that component, executing normally until they activate it? Was that malicious code in their from the beginning? Or maybe it is a vulnerability at the source code, machine code, or machine levels? Someone can make a small change to a component to utilize that vulnerability with little effort if they know what they are doing. Reversing our direction, if we expand outwards instead of inwards, we have containerization. Containerization solutions, such as Docker , provides a complete computing environment to execute your software within. Popular with back end development, you encapsulate your software with it's intended operating system platform, reducing the number of platform's you need to design your software for to 1. But with containerization, we also have to ask the same questions of the platform as we did with the software. How secure is the operating system that the container uses as it's base? In today's world of software development, where componentization is key, the software you write is not the only place where security issues can be introduced. However, there are proactive steps you can take to reduce the vectors than users can follow to use your software maliciously. In the upcoming article Software Quality: Security , I will delve more into breaking the Security pillar into S.M.A.R.T. requirements and I provide suggestions on how they it be measured. Back To Requirements Having explored the 4 pillars, it is important to bring the discussion back to the definition of good requirements. Using the information from each of the individual pillar articles in concert with the information on S.M.A.R.T. terminology, your team can request requirements that are more focused. As any focused requirements will be Specific (the S. in S.M.A.R.T.), it is reasonable to expect that any impact on our 4 pillars will be noted. Asking for this change will almost guarantee some negotiations with the team's stakeholders. In my experience, when your team asks for more focused goals from your stakeholders, there will typically be some pushback from those stakeholders at the beginning. If your team has had some requirements mishaps in the past, highlight each mishap and how the ensuing loss of time and focus could have been avoided usually sways stakeholders. Don't point fingers, but simply point out something like: Hey, when we did the X requirement, we all had a different idea on what to fix, and as such,it took X hours of meeting and Y hours of coding and testing to figure out it was the wrong thing. We just want to help tune the requirements process a bit to help everyone try and avoid that waste.\" Most stakeholders are being asked to have their teams do the maximum amount of work possible in the shortest amount of time. By asking that question in such simple manner, you are asking if you can spend a small amount of time up front to hopefully eliminate any such missteps. Most stakeholders will grab on to that as a way for them to look good and for the team to look good, a win-win. What will these requirements look like? The requirements will typically come in two main categories. The first category, requirements focused on fixing bugs or adding features, will typically be the bulk of the requirements. Each requirement should outline any negative impact it will have on any of the metrics. If nothing is added on negative impacts, the assumption is that the impact will be neutral or positive. A good example of this is a requirement to add a new feature to the project. The requirement should be clearly stated using S.M.A.R.T. terminology, because it will remove any ambiguity in the requirements. As any source code added without tests would impact any reliability metrics, reliability tests should be added to meet any seldom violated metrics for your project. In similar ways for the other 3 pillars, it is assumed that any source code added will be a step forward or neutral in terms of quality, not backward. At some point in your project, you should expect that at least a few of the requirements will appear in the the second category: requirements specifically targeted at one or more of the pillars. These requirements allow your team to focus on some aspect of your project where your team feels that the quality can be improved. The big caveat with these requirements is to be mindful of the Achievable and Time-Related aspects of S.M.A.R.T. requirements. Make sure that whatever the goal of these requirements are, they are things that won't go on forever and are not pipe dreams. A good example of this is wanting to improve the efficiency of your project or processes. Without a good requirements that is Specific, Achievable and Time-Related, this can go on forever. A bad requirement would state something like \"Make the project build faster\". A good requirement might state something like \"Reduce the unit test time from 20 seconds to under 15 seconds\", timeboxed to 4 hours. The good requirement has good guard rails on it to keep it from exploding on someone who picks up that work. Publishing Software Quality Having gone through the previous sections and any related articles, you should have a better idea on: how to write better requirements to ask for software quality to be improved what metrics I recommend to use for each of the four pillars how to measure those metrics and integrate them into your projects Using this information as tools, your team can improve the quality of the project at it's own pace, be that either an immediate focus or a long term focus for your project. For any metrics that are in the seldom violated category, the best way to approach them is to make them gatekeeper metrics for your project. It should be possible to execute a great many of the gatekeeper metrics before a commit happens, which is optimal. For the remaining metrics in the seldom violated category and metrics in the the positive momentum category, your team should publish those metrics with every commit or push, giving the submitter that needed feedback. In addition, publishing the metrics to some kind of data store allows your team to determine how the project quality is trending over time, allowing any stakeholders or project members to observe any potential software quality issues and take steps to deal with them. Even for certain seldom violated metrics, it can be useful to track how they are trending, even if they are trending above the gatekeeper lines set for the project. If your team does not publish those metrics in some form, the only data point they have for the project is a binary one: it passes or it does not. From my experience, that binary metric is often a false positive that burns teams due to a lack of information. What Does Software Quality Mean To Me? Software quality means each software project has a plan. When requirements come in to the project, they are detailed using the S.M.A.R.T. terminology. If not specifically geared towards a given software quality pillar, each requirement may specify what kind of impact it has on one or more of the pillars. If not specified, it is assumed that it has a neutral or positive effect on all of the software quality pillars. The goals are also specific, not overly broad, and realistically achieved within a given time frame. Software quality means that metrics are well thought out for each project. Each metric is both defensible and reasonable for that project and that team. Any metrics that are not being used as gatekeepers are published so they can be tracked over time. For additional benefit, non-binary gatekeeper metrics are also published, to further improve the project and the quality of the project. Software quality means ensuring that software projects are reliable. Projects have well thought out tests that are performed at many levels to ensure that the project's components work together to meet the project requirements as well as verify the correctness of the components themselves. These tests are executed frequently, and a large number of them are used as gatekeepers, trying to ensure that only reliable changes are made to the project. When a project is released, the scenario coverage is 100% and the code coverage is either at 100% or whatever percentage the team has negotiated and documented for their project. Software quality means ensuring that software projects are maintainable. This entails sufficient documentation of project goals, architecture, design, and current state. The documentation is coupled with Static Code Analysis to measure a number of maintainability metrics and to gatekeep on most of them, ensuring that the project moves in a positive direction to a higher quality project. Software quality means ensuring that software projects and their processes are efficient. Team process to administrate and maintain the software and the software itself do not have to be blindingly fast, but they need to be as efficient as they need to be for that project and for that team. They do not need to be fast as lightning, only fast enough for the software project itself. Software quality means ensuring that software projects are secure. If third party components are used for the project, those components need to be monitored for vulnerabilities, and any issues that arise must be addressed quickly. Steps are taken, at a level that is appropriate for the type of software project, to reduce the possible ways that an user can use the software project do something malicious. To me, software quality is about the journey, continuously improving quality and showing that progress, while adding new features and fixing bugs at the same time. Wrapping It Up To put it succinctly, software quality for a project is about having a common nomenclature describing the various pillars of quality, having a common way of measuring against each of those pillars, and the publishing of those measures. Therefore, from my point of view, software quality is not a single metric but a collection of metrics and a philosophy. That philosophy is that your team can only really answer that question by having clearly defined goals for your project and it's quality metrics, and steering the project towards those goals. Does every project need to be super high quality? No, not even close. But I firmly believe that each project needs to have a solid understanding of what level of software quality they have in order to negotiate the definition of \"good enough\" for each project. In the United States, where I currently live, I am a Software Development Engineer in Test or SDET. I do not have an engineering degree. In any other country, including my native Canada, I am a Software Developer in Test or SDT. ↩ In the United States, where I currently live, a Software Development Engineer or SDE is the same as a Software Developer in any other country. ↩ Based on my experience, where the article breaks out Size as it's own pillar, I would place it in the Maintainability section. Similarly, while I can understand why they place Indentifying Critical Programming Errors in its own section, I would most likely fold half of the items into the Maintainability section and half of them into the Reliability section. To be clear, I agree with the content they present, it is just the organization that I disagree with on two small points. ↩","tags":"Quality","url":"https://jackdewinter.github.io/2019/09/15/what-is-software-quality/","loc":"https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"},{"title":"Static Websites: Getting Ready For Publishing - Themes and Minutiae","text":"This is the fourth article in a series about setting up my own web site using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous articles, I was able to verify my setup of Pelican, posting a test article that I was able to view in a browser. I was then able to improve the fidelity of that article by using Lorem Ipsum to make sure it looked more like a real article in terms of content and length. Almost as important, I was able to come up with a more efficient workflow for publishing changes as I work on them. To make the jump from authoring to publishing, there are a number of things that I needed to finish: Fixing The Build Warning File Types and File Paths Better Values For Defaults A Default About Page Selecting a Theme Once I have completed all of that, I will be ready to publish… so let's proceed! Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Task 1: Fixing The Build Warning When the site was regenerated using the pelican-build.bat file, I noticed a warning at the top of the output. WARNING: Docutils has no localization for 'english'. Using 'en' instead. This is an easy one to handle. I went to the pelicanconf.py and changed the value for DEFAULT_LANG from english to en . Running pelican-build.bat again, the warning went away. Task 2: File Types and File Paths Even though I am creating a static web site, there are various categories of content that I feel should be kept separate, for various reasons. Luckily, the contributors to Pelican though of this too, and the following change to pelicanconf.py will separate out content: ARTICLE_PATHS = [ 'articles' ] PAGE_PATHS = [ 'pages' ] STATIC_PATHS = [ 'images' ] This configuration informs Pelican that articles will be contained within the content\\articles directory, pages within the content\\pages directory, and static content, such as images, in the content\\images directory. While this isn't 100% necessary at the moment, I feel that the organization will pay off later in the website's history. To complete this change, I moved the content\\test.md file into the content\\articles\\test.md directory to follow this paradigm. To make the paradigm more complete, I wanted to make sure that the articles that I write have the date they were created as part of their published path. Searching around the Pelican site itself, this was easily accomplished with the following change: ARTICLE_URL = '{date:%Y}/{date:%m}/{date: %d }/{slug}/' ARTICLE_SAVE_AS = '{date:%Y}/{date:%m}/{date: %d }/{slug}/index.html' Task 3: Better Values For Defaults The pelicanconf.py file contains two variables, LINKS and SOCIAL, which are still at their default values. Giving each a better value will give me a better idea of what things will look like with various themes, so it makes sense to change them now to: LINKS = ( ( 'Pelican' , 'http://getpelican.com/' ), ) SOCIAL = ( ( 'github' , 'https://github.com/jackdewinter' ), ) Task 4: A Default About Page In trying to determine what I needed before looking at themes, I noticed one small gap: I had no static pages. While I expect most of my content to by blog posts, there will be the occasional times where I want some kind of page on my site that isn't an article. The first type of page that came to mind was an About page, so I quickly created a new file content\\pages\\about.md with the contents: # About Me This is me . While it is just a placeholder, the intent was that it would give me a good idea of what to expect going forward. Sidebar 1: What Are Themes? In Pelican, extensions are performed using a combination of configuration, pip installs, and git repositories. The configuration changes and pip installs felt natural to me, as they are common paradigms in Python. However, I found the repositories as source took a bit of getting used to. Not too bad, but a bit of change that was good! Themes themselves are each an individual git repository, containing all of the asserts that the theme needs. Luckily there is a site that shows samples of approximately 80% of the themes. While it is a bit to process in one viewing, Pelican Themes currently contains 126 themes, of which 100 of them have images of themed pages. The better themes have 3-4 pages shown in the index, whereas some of the themes only have 1 image. Regardless, it is a lot better than downloading 100 themes and trying each one out! Task 5: Selecting a Theme At this point, I was pretty sure I had all of the assets I needed before I looked at themes. Sample articles. Check. Sample page. Check. Sample Links. Check. Sample social links. Check. It was time to start! Seeing as I hadn't actually read anything in people's blog posts about how hard it was to select a theme, I wasn't sure what to expect. As such, I budgeted a decent amount of time to the effort. In fact, I actually budgeted a whole week for this next section. For me, the choice of theme was pivotal in being able to communicate effectively with readers. To be clear, I didn't need it to be the right choice. I did need it to be the right choice for me and my voice. It was a clear proposition in my mind: come out of the gate with a theme that wasn't me, or take the time and try and nail it. Even if I missed by a bit, I wouldn't regret taking that time. If you are following these articles as a guide, remember that. Give yourself the time to make a solid choice that you believe in. Which Themes to Try? As I was trying to find a theme for myself, I went through the complete list of themes 3 or 4 times, just taking a look and seeing which ones appealed to me and which ones had ideas that I liked. I started a list on the first pass, and with each pass through the list, I whittled that list down based on what I saw. On the final pass, I focused on getting the number of themes down to a manageable 3 themes. For that pass, I found it important, yet difficult, to pare down the choices to 3 themes. It was important, because I didn't want to be stuck analyzing the themes for a long time. It was difficult because there are a lot of good options for themes, and to come up with only 3 options wasn't easy. However, I found that by focusing on my primary goal of ease of use from my first article , both ease of writing and ease of reading, it helped me narrow things down. The use of an actual list, in my case a simple list written in Markdown, was pivotal. At first it contained the URLs of each of the repositories I wanted to look at more. With each pass through the list, it contained more information about individual themes and a smaller number of themes, This approach helped to quickly whittle the selection down to the real contenders. This approach, while pedantic, saved me a number of times, as it became more of a struggle to remember which theme had which look and which features. With my final pass through the list, I arrived at my top pick of the nice-blog theme, with alternates of the blueprint theme and the elegant theme. Nice-blog is simple and uncomplicated, with a decent looking sidebar. blueprint is the theme for the site of Duncan Lock which I found during my research. blueprint had a bit more of a finished techy feel, with a nice layout on the sidebar. Each article had \"X min read\" text with the clock icon really grabbed me, which really appealed to me. Finally, the elegant theme seemed to keep things simple but elegant, with lots of room for creativity. Trying Out The Themes Each of the themes, Nice-Blog , Blueprint , and Elegant , exists in it's own GitHub repository. As such, one approach to downloading the themes was to create a blog-themes directory at the same level as the base project directory , creating a directory for each theme. As Nice-Blog and Elegant are in the main Pelican themes repository, the other approach for those two themes was to clone the Pelican Themes Repository into the blog-themes directory using: git clone --recursive https://github.com/getpelican/pelican-themes For the first approach, I would have to individually add each theme, whereas with the second approach, I can get most of the themes all at once. The was also the concern that regardless of which way I chose for nice-blog and elegant , I would have to use the first approach for blueprint . Was it worth it to have two paradigms? After thinking about it a bit, I decided to go with the first approach, as I only had 3 themes I was interested in. So, on the command line, I entered: mkdir ..\\blog-themes git clone https://github.com/guilherme-toti/nice-blog ..\\blog-themes\\nice-blog git clone https://github.com/dflock/blueprint ..\\blog-themes\\blueprint git clone https://github.com/Pelican-Elegant/elegant ..\\blog-themes\\elegant The plan was to try each of the candidates, writing down notes about what aspects I found good and bad about each. Following the instructions on the Pelican home page, I modified pelicanconf.py to refer to the first theme as follows: THEME = '%%%MY_DIRECTORY%%%\\\\blog-themes\\\\nice-blog' Save the file, switch to the browser and refresh. Check to make sure it changed properly. Look around the site a bit and write down some notes. Easy. While I would end up coming back to this theme later for more information, the first pass was a solid 5 minutes with no issues. Expecting similar behavior, I did the similar change to make the THEME variable point to the blueprint directory. I switched to the browser and refreshed and it was the same as before. A quick examination of the Builder command window, and I got the following notification: CRITICAL: TemplateSyntaxError: Encountered unknown tag 'assets'. Critical error… encountering that was foreboding. I stopped the windows started by pelican-devserver.bat , and buckled down to do more research. This was the start of a long diversion. Off The Beaten Path: Getting Blueprint to Work Note: I have not contacted the developer of the Blueprint theme, and his blog and his theme have not had any recent changes. When I decided to try it out, it was with the knowledge that it would probably require more effort to get it to work. I had hoped to wait for a bit before exploring plugins, as there are many plugins listed on the Pelican Plugins web site. In addition, unless you know what you are looking for with a plugin, it's effects are either invisible or difficult to spot. For those reasons, I wanted to wait until the more major variables regarding the web site were set before tackling these more minor ones. Adding Required Plugins After doing my research, it appeared that blueprint was dependent upon the assets plugin. While installing Plugins faces the same issue as how to install Themes, I chose to do the \"all at once\" approach for the plugins. The main reason for this was to allow me in the future to try each plugin, figuring out that plugins is worth the impact. As such, having all of the common plugins together made a lot more sense. Similar to the way described above to install all the themes, I created a blog-plugins directory at the same level as the base project directory . Changing into that directory, I issued the following command to pull the contents down to my system. git clone --recursive https://github.com/getpelican/pelican-plugins ..\\blog-plugins Once I had the contents of the plugin repository on my machine, I added configuration to Pelican to point to the new plugin directory. Then I needed to add the assets plugin to satisfy the blueprint theme. This was done by adding the following to pelcianconf.py : PLUGIN_PATHS = [ '../../blog-plugins/' ] PLUGINS = [ 'assets' ] Running the pelican-build.bat script this time, I received the following error, buried deep within the output: WARNING: `assets` failed to load dependency `webassets`.`assets` plugin not loaded. Plugins With Python Package Requirements Luckily, as part of the previous research, this warning was mentioned, and it was because the assets plugin requires the webassets Python package. A quick pip install webassets later, it's time to build the website again. This time, after running the build script, the output ended with the following lines: ... CRITICAL: ValueError: not enough values to unpack (expected 3, got 2) ... File \"XXX\\blog-themes\\blueprint\\templates\\base.html\", line 37, in top-level template code {% for anchor_text, name, link in SOCIAL %} ValueError: not enough values to unpack (expected 3, got 2) Configuration Changes For Plugins Once again, research to the rescue, this being a relatively easy issue. Different plugins and themes have different configuration requirements, and this one is no different. By looking at the error message, mixed with my knowledge of Python, I saw that the plugin is expecting 3 values for the SOCIAL configuration variable: anchor_text, name, and link. A quick look at my current configuration, and I see the default settings of: SOCIAL = ( ( 'github' , 'https://github.com/jackdewinter' ), ) represent the 2 values that the theme is expecting. Needing a third, I simply cloned the first value into the second position: SOCIAL = ( ( 'github' , 'github' , 'https://github.com/jackdewinter' ), ) Wash. Rinse. Repeat. Running the script again, I received the following critical error: ... CRITICAL: TemplateAssertionError: no filter named 'sidebar_date_format' ... File \"XXX\\blog-content\\virtualenv\\lib\\site-packages\\jinja2\\compiler.py\", line 315, in fail raise TemplateAssertionError(msg, lineno, self.name, self.filename) jinja2.exceptions.TemplateAssertionError: no filter named 'sidebar_date_format' More Configuration Changes This time, it took a lot of looking. I did scans over the entire blog-themes directory as well as the blog-plugins and blog-content directories. Just in case. In fact, I had almost given up hope when I started to look at the pelicanconf.py file that Duncan himself used. Down near the bottom were a number of configuration entries, including one for the missing configuration item. from datetime import date ... def month_name ( month_number ): import calendar return calendar . month_name [ month_number ] def custom_strftime ( format , t ): return t . strftime ( format ) . replace ( '{S}' , str ( t . day ) + suffix ( t . day )) def archive_date_format ( date ): return custom_strftime ( '{S} %B, %Y' , date ) def sidebar_date_format ( date ): return custom_strftime ( '%a {S} %B, %Y' , date ) def suffix ( d , wrap = True ): tmp = 'th' if 11 <= d <= 13 else { 1 : 'st' , 2 : 'nd' , 3 : 'rd' } . get ( d % 10 , 'th' ) if wrap : return '<span class=\"day_suffix\">' + tmp + '</span>' else : return tmp # Which custom Jinja filters to enable JINJA_FILTERS = { \"month_name\" : month_name , \"archive_date_format\" : archive_date_format , \"sidebar_date_format\" : sidebar_date_format , } Copying this into my own pelicanconf.py file, it was time to run the build script again. To be honest, for this section of configuration, I started only copying the sidebar_date_format function. Then I realized it needed custom_strftime . Then I realized it needed… It was at that time that I figured it was easier to just copy all of this code over, and if I stayed with the theme, I would see about cleaning it up. With the complete section copied over, running the build script produced the following error: CRITICAL: UndefinedError: 'pelican.contents.Article object' has no attribute 'stats' More Build Iterations Realizing that more of the configuration may be in the pelicanconf.py file, with the above error in mind, I scanned the configuration and noticed a plugin called post_stats . Looking at the documentation for post_stats , it seemed like it would expose that attribute. So, adding ‘post_stats' to the plugins, I re-ran the build, with the attribute error disappearing, only to be replaced with: ModuleNotFoundError: No module named 'bs4' Having looked at the documentation for post_stats , I was able to solve this one right away. To get the proper word count for the stats, the plugin uses the Python Beautiful Soup package (version 4) to scrape the HTML output. Executing pip install beautifulsoup4 , and then rebuilding again, we get the following output: File \"%%%MY_DIRECTORY%%%\\blog-themes\\blueprint\\templates\\footer.html\", line 41, in top-level template code {% for anchor_text, name, link in LINKS %} ValueError: not enough values to unpack (expected 3, got 2) This is the same error as with the SOCIAL variable, and the same solution will work. Changing the LINKS variable to: LINKS = ( ( 'Pelican' , 'Pelican' , 'http://getpelican.com/' ), ) And recompile and… all errors gone. Switch back to the browser and refresh the page. Repeat the entire process as with the nice-blog theme, taking notes. Back to the Final Theme Bracing for the worst, I changed the theme to Elegant and it… just worked. After blueprint , I expected a lot more effort, but it just worked. Counting my blessings, I did the usual playing around and taking notes, then I sat back. From then on, I went back to the three themes frequently and took more detailed notes and observations until I got to a point where I wasn't adding any meaningful notes. It was then that I was sure that I had the information I needed to make my decision. Lessons Learned Before I go on to my decision process, I wanted to go over some things I learned or remembered along the way of finding a theme. Some are obvious, some are not. The more standard things are, the less you are going to have to do to make them work. Blueprint was not in the main collection of themes, and it took a lot of effort to make it work in it's out-of-the-box mode. Elegant and Nice-Blog both worked out-of-the-box with 0-2 modifications to configuration and no extra packages or plugins. The auto-build feature of the generator can hang. There were a number of times I had to Ctrl-C and restart the generator. This seemed to happen more when I was changing the themes or plugins. It also happened when I introduced something that required a new Python package. I didn't regret putting in the work. I wanted to put the work in to each theme on the short list to have a balanced comparison. I wanted to have a good understanding of what it would take to maintain each of the themes. The work I put in gave me the understanding that blueprint would take a lot more work than the others to maintain. As old school as it is, a simple pros and cons list help me figure out the best theme. The first two themes were easy to keep in my head at first. The more themes that I saw, the harder it was to remember any points about each of the themes. By the time I got down to the last 2 themes, I had so many parts of so many themes going through my head. Without the list, I would have been lost. The Final Selection Due to the amount of work required for blueprint , including a number of limitations I encountered clicking around, it was out of consideration right away. One of my core issues from the first article was ease of use, and I didn't get the feeling that blueprint would fall into that category. Looking at the notes a number of times, it was hard to distinguish the two remaining themes from each other. They both had a simple and elegant look-and-feel, and that was making the decision even more difficult. There were a couple of small features that were different, but those missing from one were balanced by ones missing from the other one. It was pretty much a stalemate in my head. After going back and forth a number of times to try and resolve the stalemate, I decided I needed more criteria to help. The first two that came to mind were the documentation and active maintenance of the theme. This was returning to the ease-of-use consideration that was a driving force in my first article, so I knew it would be relevant. It was then that the elegant theme became the winner by a fair amount. So, after about a weeks worth of shifting between themes in \"spare time\", I landed on the elegant theme for my web site. It took a couple of days for it to sink in while I was writing this article, but with each passing day, my confidence that I made the right choice increased. I took the time, examined the themes, wrote the notes, and in the end, it resulted in a single, clear choice. With finality I went to the configuration file, and near the top added: # Current Theme THEME = ' %% %MY_DIRECTORY %% % \\\\ blog-other-themes \\\\ elegant' What Was Accomplished At the beginning of this article, I had most of what I needed to start selecting a theme. It took some small updates to the configuration to make sure I had a good test site available. This was critical to allowing me to go through each theme I was interested in and see if it was for what I was looking for. While one of the themes proved to be a handful, the experience was good in advising me of possible issues I might have in customizing my own site. In the end, I made a strong, confident choice of the elegant theme, which as benefits, is actively being developed and has great documentation. What's Next? Now that I have completed all of the major tasks, the next step is to publish the website to an external host. For choices that will become evident, I will be publishing the website to GitHub pages. Completing this task will be covered in the next post Publishing To GitHub Pages . Epilogue: Jack's Notes on Themes In the previous sections, I mentioned taking notes on each themes. To do this, I used the main page, the about page, and the two sample articles as a litmus test. I clicked around the web site, looking for things that worked and didn't work, as well as the feel of the web site. I also went to the home repository of each theme and checked for how often it was updated, and what kind of support the repository offered for customization. In the end, I came up with three lists of items, with pros and cons, as follows: nice-blog pros starting point minimal but elegant look and feel categories support metadata for articles subtitle, cover image, gallery can change color easily cons doesn't seem to handle TOC properly no tags support simple tracking blueprint pros tags by name and frequency like \"4 min read\" nice blue date format Fri 3rd December, 2010 category and tag support origin of better figures plugin cons too limited otherwise no archives by default documentation for theme very limited elegant pros nice layout very elegant and simple feel category and tag support extensive documentation lots of metadata actively being maintained cons no \"4 min read\" support no links support","tags":"Technology","url":"https://jackdewinter.github.io/2019/09/08/static-websites-getting-ready-for-publishing-themes-and-minutiae/","loc":"https://jackdewinter.github.io/2019/09/08/static-websites-getting-ready-for-publishing-themes-and-minutiae/"},{"title":"Static Websites: Publishing To GitHub Pages","text":"This is the fifth article in a series about setting up my own web site using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous articles, I used Pelican as my Static Site Generator, generating a nicely themed site with some test articles in it. To make sure I can speak with my own voice, I took my time determining what theming I wanted for the website. By taking these deliberate steps, I was able to arrive at my choice for the site's theme with confidence and clarity. This entire journey has been geared towards generating an externally hosted web site that is primarily a blog. This article talks about the final step on that journey: publishing my website to the free GitHub Pages platform. Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Why GitHub Pages? In looking at a number of sites built on various Static Site Generators (SSGs), it became obvious that a majority of the pages were hosted on GitHub Pages . With that in mind, I looked into GitHub pages to figure out if it was best solution for me. The article What is GitHub Pages? goes into a decent amount of detail, but the summary boils down to: it's 100% free only static content can be hosted don't do anything illegal don't create an excessively large site (over 1 GB in size) don't create a site that is incredibly populate (over 100GB per month in bandwidth) For cases where the last two items occur, their website even mentions that they will send \"a polite email from GitHub Support or GitHub Premium Support suggesting strategies for reducing your site's impact on our servers\". To me, this seemed like a good place to start. I already use Git for source management, so familiarity with the website and tooling is already there. Their documentation is good, and it looks relatively easy to implement. Another plus. Most importantly, there are no fees for upload or serving the content, so I can experiment with various things and not worry about incurring extra charges. Branches on GitHub Pages After doing my research on GitHub, specifically about publishing on GitHub pages , I was confused about one point. From my experience with Git, most people and companies do either repository base development or branch based development. Even less frequent is something called monolith based development. The approach for GitHub pages is not one of those. Repository based development uses Git's distributed nature to create changes on your own repository, only merging the changes into the \"main\" repository when you are sure of your changes. Branched based development is similar, except the branching feature of Git is used on a single remote repository, only merging changes into the \"main\" branch when you are sure of your changes. Monolith development is more dangerous, with committing all changes to a single repository with a single branch. For all three type of development, there is one thread going through all of them: you are keeping versions of a single type of thing in your repository. In a number of sites that I researched, it appeared that they were using a tool called ghp-import . This tool allows for the content for the site to be stored in the content branch of the repository, while the produced website is pushed to the master branch of the same repository. While I can wrap my mind around it, to me it didn't seem like a good idea. As this is outside of my normal workflows, I was pretty sure that at some point I would forget and push the wrong thing to the wrong branch. To keep things simple, I wanted my website content in one repository, and my website content in another repository. That itself raised some issues with my current setup, having the output directory at the same level as the content directory. During my research, I came across the statement that Git repositories cannot contain other repositories. If you do need to have this kind of format, a concept called submodules was recommended. The plugins and themes repositories for Pelican make heavy use of submodules, so I knew it could be done. But after some experimentation with some sample repositories, I was unable to make this work reliably. Also, while I can learn to wrap my mind around it, it seemed like a lot of extra work to go through. In the end, I decided that it was best to keep things simple, keeping 2 repositories that were 100% separate from each other. If I do more research and figure out how to make submodules work reliably, I am confident that I can condense these distinct repositories into one physical repository. With that decision made, I needed to create a new output directory outside of the blog-content directory. I decided to call this new directory blog-output and have it at the same level as blog-content . To make sure it was initialized properly with a local repository, I entered the following commands: mkdir ..\\blog-ouptut cd ..\\blog-ouptut git init Once that was complete, I had to ensure that the pelican-* scripts were changed to point to the new location, taking a simple search and replace over all of the script files. That being completed, I executed each of my pelican-* scripts, to verify the changes were correct, with no problems. To further ensure things looked good, I performed a git status -s on both repositories to be sure I didn't miss anything. While this approach wasn't as elegant as the other solution, in my mind it was simpler, and therefore more maintainable. Adding Remotes Repositories Now that I had two local repositories, one for content and one for output, it was time to make remote repositories on GitHub for each of them. I already had a GitHub account for some other projects I was looking at, so no worry there. Even if I didn't have one set up, GitHub makes it simple to set up a new account on their home page . From there, it was a simple matter of clicking the plus icon at the top right of the main window, and selecting New Repository from the drop down list. The first repository I created was for the content, and I simply called it blog-content , which I entered in the edit box under the Repository Name label. As I wanted my content to be private, I changed the selection from Public to Private and clicked on the Create Repository button. For the other repository, I followed the same instructions with two exceptions. The first exception is that, as the output of Pelican needs to be public to be seen, I kept the selection on Public . The second exception was the name of the repository. According to the User Pages page, to publish any committed pages you need to use a site of the form user-name .github.io and push any changes to the master branch. As my user name on GitHub is jackdewinter , this made my repository name jackdewinter.github.io . If you are using this article as a guide, please note that you will need to change the repository name to match your own GitHub user name. Securing The GitHub Access The first time that I added my remote repositories to their local counterparts, I encountered a problem almost right away. When I went to interact with the remotes, I was asked to enter my user id and password for GitHub each time. This was more than annoying. Having faced this issue before on other systems, I knew there were solutions, so back to the research! Now, keep in mind that my main machine is a Windows machine, so of course this is a bit more complicated than when I am working on a Linux machine. If I was on a Linux machine, I would follow the instructions at Connecting to GitHub with SSH and things would probably work with no changes. To start with, I want to make sure that GitHub has it's own private/public key pair, so I would follow the instructions under Generating a New SSH Key and adding it to the ssh-agent . I would then follow the instructions under Adding a new SSH key to your GitHub account to make sure GitHub had the right half of the key. A couple of Git commands later, and it would be tested. In this case, I needed to get it running on windows, and the Win10 instance of SSH takes a bit more finessing. To make sure the service was installed and operational, I followed the instructions on Starting the SSH-Agent . Once that was performed, I was able to execute ssh-agent , and only then could I use ssh-add to add the newly created private key to ssh-agent . In a nutshell, I needed to execute these commands to setup the key on my local machine: ssh-agent ssh-keygen -f %USERPROFILE%\\.ssh\\blog-publish-key -C \"jack.de.winter@outlook.com\" ssh-add %USERPROFILE%\\.ssh\\blog-publish-key Attaching Remote Repositories to Local Repositories This was the real point where I would see if things flowed together properly. First, I needed to specify the remote for the blog-content repository. Looking at my GitHub account, I browsed over to my blog-content repository, and clicked on the clone or download button. Making sure the link began with ssh , I pressed the clipboard icon to copy the link into the clipboard. Back in my shell, and I change directory to blog-content and entered the following: git remote add origin %%%PASTE HERE%%% where %%%PASTE HERE%%% was the text I copied into the clipboard. As my user id is jackdewinter and the repository is blog-content , the actual text was: git remote add origin https://github.com/jackdewinter/blog-content.git This process was then copied for the blog-output directory and the jackdewinter.github.io repository. Publish the Content to Output Until this point, when I wanted to look at the website, I would make sure to have the windows from the pelican-devserver.bat script up and running. Behind the scenes, the pelican-autobuild.bat script and the pelican-server.bat scripts were being run in their own windows, the first script building the site on any changes and the second script serving the newly changed content. As long as I am developing the site or an article, that workflow is a good and efficient workflow. When generating the output for the actual website, I felt that I needed a different workflow. As that act of publishing is a very deliberate act, my feeling is that it should be more controlled than automatically building the entire site on each change. Ideally, I want to be able to proof a group of changes to the website before making those changes public. One of the major reasons for the deliberate workflow is that, from experience, the generation of anything production grade relies on some form of configuration that is specific to the thing you are producing. For my website, this needs extra testing specifically around that production configuration in order for my confidence in those changes to be high enough that I am confident in publishing it. The most immediate example of such configuration is the SITE_URL configuration variable. While it was not obvious in the examples that I researched, this variable must be set to the actual base URL of the hosting site. Using the Elegant theme, if you click on the Categories button in the header, and then the Home button, it will stay on the Categories page. Looking more closely at the source for the base.html page, the Home button contains an url is defaulted to '‘ . Digging into the template for the base.html page, the value being set for the anchor of that button is href=\"{{ SITEURL }}\" . Hence, for the Home button to work properly, SITE_URL needs a proper value. The default configuration in pythonconf.py for SITE_URL is '‘ , so that needed to be changed. For the developer website to work properly, SITE_URL must be set to ‘http://localhost:8000' in pythonconf.py . This however introduces a new issue: how do I make sure this variable is set properly when we publish the output? Luckily, the Pelican developers thought of situations like this. Back in the second article of this series, Step 4: Create a Generic Web Site , I mentioned a file called publishconf.py . This file was generated as part of the output of pelican-quickstart and has not been mentioned since. This file is intended to be used as part of a publish workflow, allowing the variables from publishconf.py to be overridden. Specifically, in that file, the following code imports the settings from publishconf.py before defining alternate values for them: sys . path . append ( os . path . abspath ( os . curdir )) from website.pelicanconf import * Below this part of the configuration, in the same manner as in pythonconf.py , the SITEURL variable in publishconf.py is set to '‘ . Therefore, when I publish the website with the publish configuration, it will use '‘ for the SITE_URL . To make sure the website publishes properly, I needed to change the SITE_URL variable in publishconf.py to reflect the website where we are publishing to, namely https://jackdewinter.github.io . Now that I took care of that, I just needed to come up with a batch script that makes use of publishconf.py . To accomplish that, I simply copied the pelican-build.bat script to pelican-publish.bat , and edited the file removing the –debug flag and referring to publishconf.py instead of pelicanconf.py : pelican --verbose --output ..\\blog-output --settings website\\publishconf.py website\\content To test this out, I stopped the pelican-autobuild.bat script and executed the pelican-publish.bat script. By leaving the pelican-server.bat script running, I was able to double check the published links, verifying that they were based on the jackdewinter.github.io site where I wanted to publish them. Pushing the Content To The Remote At this point, I had two local repositories, one with commits and one without, and two remote repositories with no information. While I wanted to see the results and work on the blog-output repositories first, it was more important to make sure my work was safe in the blog-content repositories. So that one would be first. Changing into the blog-content directory and doing a git status -s , I noticed a couple of changes that were not committed. A quick git add –all and a git commit later, all of the changes were committed to the local repository. At this point, the changes are present in the local repository, but not in the remote repository. The following command will push those changes up to the remote repository's master branch. git push --set-upstream origin master At this point, I did a quick check on the blog-content repository in GitHub and made sure that all of the repository was up there. Now, in the future, I knew I would be more selective than using git add –all most of the time, but for now it was a good start. So I carefully went through the files that GitHub listed and verified them manually against what was in the directory. I didn't expect any issues, but a quick check helped with my confidence that I had set up the repository correctly. Pushing the Output To The Remote Once that was verified, I carefully repeated the same actions with the blog-output directory but with one small change. In the blog-content directory, I want to save any changes. However, with the blog-output directory, I want to commit everything, ever if there are conflicts. This is something that is done with quite a few static sites, so the workflow is decently documented. As this is an action that I am going to repeat every time I publish, I placed in a script file called pelican-upload.bat : pushd ..\\blog-output git add --all . git commit -m \"new files\" ssh-agent git push origin master --force popd In order: switch to the blog-output directory, add all of the files, commit them with a simple reason, ensure the ssh-agent is up and running, push the committed files to remote repository, and go back to our original directory. If that last git push looks weird, it is. It is so weird and destructive that there are a number of posts like git push –force and how to deal with it and GIT: To force-push or not to force-push . However, even after I looked at the manual page for git push , I was still trying to figure it out. It wasn't until I came across The Dark Side of the Force Push , and specifically the Force Push Pitfalls section of that article, that things made sense. Under new script run pelican-upload.bat Viewing the Webpage To make sure things looked right, I wanted to do a side by side comparison of what I could see in my browser both locally and on the new website. To do that, I opened up one tab of my browser and pointed it to http://localhost:8000/ , and another tab beside it and pointed it to https://jackdewinter.github.io/ . To be honest, while I was hoping there would be no issues, I was expecting at least 1-2 items to be different. However, as I went through the comparison, there was 100% parity between the two versions of the web site. What Was Accomplished At the beginning of this article, I had most of what I needed to start selecting a theme. It took some small updates to the configuration to make sure I had a good test site available. This was critical to allowing me to go through each theme I was interested in and see if it was for what I was looking for. While one of the themes proved to be a handful, the experience was good in advising me of possible issues I might have in customizing my own site. In the end, I had a strong choice of the elegant theme, which as benefits, is actively being developed and has great documentation. What's Next?","tags":"Technology","url":"https://jackdewinter.github.io/2019/09/08/static-websites-publishing-to-github-pages/","loc":"https://jackdewinter.github.io/2019/09/08/static-websites-publishing-to-github-pages/"},{"title":"Static Websites: Posting My First Article","text":"This is the third article in a series about setting up my own web site using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction With Pelican installed, it was time to start creating stuff. The first I thing I wanted to do was to figure out if the base site was good by creating a default site and viewing it in a browser. Then I wanted to come up with a very simple test article, publishing it and viewing it to verify my ability to publish and exercise that workflow. Finally, I wanted to take some time to come up with a more representative test article and improve the publishing workflows. Similar to the last article, this is not about leaping forward, but to take a decent step forward in understanding the tools and processes. Before We Start Please read Operating System Paths from the second article in this series for my view on operating system pathing. (i.e. /mount/my/path vs. c:\\my\\path). To understand the term base project directory and the script text %%%MY_DIRECTORY%%% , please read the explanation under Step 2: Create a Project Directory For The Site , also from the second article in this series. Step 1: Verify Our Basic Web Site The first thing I wanted to do is to make sure I didn't mess anything up in the last article, Setting Up the Pelican Static Site Generator . Therefore, it made the most sense that I verify that my website was up and running before I added my first article. After doing some research, it was evident that there are two phases that I need to take care of: building the content and viewing the content. Step 1a: Building The Web Site Content The first article's section on Static Site Generators (SSGs) hopefully made it clear that the focus of SSGs is to render the site content when updated. In the second article, Setting Up the Pelican Static Site Generator , I created the base configuration for a generic web site, but did nothing to build that content. To build that content, I needed to find out how take the empty content and generate the output files from it. Looking at some articles, I was drawn to the output of the pelican-quickstart command that generated the base configuration. In particular, I noticed the prompt: > Do you want to generate a tasks.py/Makefile to automate generation and publishing? (Y/n) n I specifically answered No because of my research. If you answer Yes , the template will ask a number of additional questions related to publishing, and it will place the Makefile and tasks.py in the website directory. As my primary machine is a Windows machine, these two files are mostly useless. Their primary value would be to instruct me on what I needed to do to replicate their behavior on the command line or in Python. Thankfully, between the command pelican –help , using the Makefile for reference, and experimentation, I was able to arrive that this command for building the web site: pelican --debug --output website\\output --settings website\\pelicanconf.py website\\content Nothing too fancy. I started Pelican in debug mode to have more information about what was going on. Using the above command, the output was placed in the website\\output directory, the settings were retrieved from the website\\pelcianconf.py file, and the content for the website is located in the website\\content directory. I double checked to make sure these were all correct, and it ran once without issue. To make sure that I didn't lose this command, I created a simple script pelican-build.bat in my base project directory, with the above command being the only line in that file. Pelican generated a lot of output, but in the end, it looked like everything was okay. I was excited to check out the new web site, so I used my web browser to open the file website\\output\\index.html and got this result: I must admit… it was kind of underwhelming. And then I thought about it a bit. A lot of the stuff that makes web pages powerful are other parts that included by the web page, and may not be loaded if the page is loaded directly from the file system. To properly view the content, I was going to have to host it somewhere. Step 1b: Viewing The Web Site Content If you are like me, hearing or thinking the words \"I need to install my own web server\" are not exactly good words. Even for a pared down web server, there are usually 3-6 directories to set up, ports to clear with firewalls, and other little things. I must admit, when I started looking around, I was not exactly in a good mood. However, the first 3 web sites that talked about Pelican were from the Pelican project's various versions. Looking deeper into the documentation, I found the part of the documentation titled Preview Your Site . Without taking too much away from the documentation, it specified that Python 3 includes a pared down web server that is available for simple uses, like I needed. After a bit of experimentation and fiddling, and I came up with the following lines and placed them in pelican-server.bat : pushd website\\output python -m pelican.server popd Executing that script, I then saw the following output: XXX\\python\\python37-32\\Lib\\runpy.py:125: RuntimeWarning: 'pelican.server' found in sys.modules after import of package 'pelican', but prior to execution of 'pelican.server'; this may result in unpredictable behaviour warn(RuntimeWarning(msg)) WARNING: 'python -m pelican.server' is deprecated. | The Pelican development server should be run via 'pelican --listen' or 'pelican -l'. | This can be combined with regeneration as 'pelican -lr'. | Rerun 'pelican-quickstart' to get new Makefile and tasks.py files. -> Serving at port 8000, server . ... While this is what the official documentation suggests, it does look like it is out of date. Using the output from above, the installed Makefile for guidance, and more experimentation, I replaced the contents of the pelican-server.bat file with the following: pelican -l -p 8000 --debug --output website\\output --settings website\\pelicanconf.py website\\content This time, the output of the script was: DEBUG: Pelican version: 4.0.1 DEBUG: Python version: 3.7.3 DEBUG: Adding current directory to system path DEBUG: Temporarily adding PLUGIN_PATHS to system path DEBUG: Restoring system path WARNING: Docutils has no localization for 'english'. Using 'en' instead. -> Serving at port 8000, server . To me, it looked like the web pages were served up properly, or at least Pelican didn't report any errors to the screen. The only thing left was to actually see if it would allow me to load the generated web site to my browser. Instead of loading the file directly into the web browser as before, I entered \"localhost:8000\" in the address bar. To be honest, I took a guess that server . meant the localhost. This time, I got the following result: Unstyled, no unique content, and only locally visible. It's not a great result… but it's a start! Step 1c: Commit Changes Before I went forward, I wanted to save the current state of the web site, so back to Git and the git status -s command, whose output was now: ?? pelican-build.bat ?? pelican-server.bat ?? website/__pycache__/ ?? website/output/ In my head, I compared these results to the changes I made: I added 2 script files to help me build and serve the content both file are detected, and I want them as source in my project I built the web site, placing the output into website/output the directory was detected, but I want to generate this output, and not persist it from previous experience, the website/ pycache / contains pyc files that are built when the python scripts are interpreted the directory containing these files was detected, but these should never be persisted Using thought processes derived from what I documented in the section Step 5: Committing the Changes from the previous article, it was clear that I needed to git add pelican-build.bat and git add pelican-server.bat to persist those changes. However, the two items that I did not want to persist would require different tacks for each one. The first case, website/output/ , is a directory like the virtualenv directory in the section # Step 2: Create a Project Directory For The Site from the previous article. Therefore, I edited the .gitignore file to include the directory by name. That was the simple one. The second case was more complex, the website/ pycache / directory. This directory only exists to contain compiled Python byte code designed to speed up execution on subsequent passes. If I add the directory, as with the first case, it only takes care of that one directory. If I run any Python scripts from other locations, I will have to add those directories too. This was not an efficient option, hence I edited the .gitignore file to ignore the files themselves, by specifying &ast.pyc as the pattern to ignore. Therefore, following those changes, the .gitignore file looked like: virtualenv/ website/output/ *.pyc Using git status -s , I verified that only the 2 scripts were being added, as well as the .gitignore file itself being changed. Quickly adding git add .gitignore , I then used git commit -m \"my message\" to commit these changes to the repository. So let's take inventory. We have a basic web site up and running, we have tested it in our browser, and we have committed any changes to our local repository. It's definitely time to write our first article. Step 2: The First Article Looking through the Pelican Documentation, I found another good section on Writing Content . From here, I learned that certain Markdown processors, such as the Python implementation, support metadata on files. As a base example, they specify this as a sample post: Title : My super title Date : 2010 - 12 - 03 10 : 20 Modified : 2010 - 12 - 05 19 : 30 Category : Python Tags : pelican , publishing Slug : my - super - post Authors : Alexis Metaireau , Conan Doyle Summary : Short version for index and feeds This is the content of my super blog post . Reading further, a lot of the metadata fields have defaults, but the Title metadata is required for the page to be picked up and processed by Pelican. That got some wheels turning in my head, but I put it aside for a later article. First I wanted to have a solid way of writing the articles before adding to that. Taking the markdown content from above, I created a file in the website/content directory called test.md and placed the content in there and saved it. Following the same pattern I used for the base content, I ran pelican-build.bat and then pelican-server.bat to view the content, providing the following: Granted, its a stock article, but I now have a way to publish my articles! However, along the way, I noticed a couple of things I want to improve on. Step 3a: A More Streamlined Build/Preview Workflow In the previous section, the workflow I used took 4 actions to go from something I wrote to something I could see and validate: save the file, run pelican-build.bat , run pelican-server.bat , and refresh the web page. Wondering if there was a more efficient way to do that, I checked back on the Makefile that I used as the inspiration for the two scripts, and noticed a devserver entry. As the devserver entry only differed from the server entry by the introduction of the -r flag, I quickly created a pelican-devserver.bat with that simple change, and executed it. Within seconds, I got the feedback: CRITICAL: TypeError: can't pickle generator objects Weird error, but understandable. In Python, the default serialization of objects to be passed outside of executables is referred to as pickling . If Pelican was already hosting the server and wanted to spawn a process to rebuild the web site, it would make sense that it pass the current configuration to that new process. Doing a bit more searching, I came across this issue logged against the Pelican project in GitHub. There are a fair number of entries for this issue, offering various pieces of advice. The low down for me is that due to the way pickling works, I cannot use it on my Windows machine with that given program. Reading further in the comments for the issue, a number of people did mention that the -r flag works fine when applied to the command I am using for building the pages, just not for building the pages from the server. Trying it out, I renamed pelican-devserver.bat to pelican-autobuild.bat , and switched the file contents to: pelican -r --output website\\output --settings website\\pelicanconf.py website\\content Based on what I read, this should have my desired effect, and a quick modification of the test.md file confirmed it. When I saved the file, the generator detected it and rebuilt the contents. However, thinking through things a bit more, I wondered if the Windows start command would help build a better solution. When I write a Windows batch file and I need to execute another batch file, I use the call primitive. This allows me to run the new batch file within the current batch file's shell. When that new batch file completes, I can check the return code from that script, and determine what action, if any, to do next. By structuring batch files and scripts this way, I find that I assemble things together more quickly, keeping the batch files and scripts more readable and more maintainable. The start primitive is almost identical to the call primitive, with one small exception: the new program is started in a newly created shell spawned from the current shell. As the current shell and the new shell are separate processes, by default, both shells operate independently of each other, unless the /wait flag is specified. While this is not ideal for composing batch files, this behavior seemed to fit what I was trying to achieve. Creating a new version of the pelican-devserver.bat file, I placed within it: @echo off start pelican-autobuild.bat start pelican-server.bat And executed the batch file. Two full size shell windows popped up, each one running one of the scripts, with the original shell window left alone. A new modification to the test.md file resulted in the rebuild happening in the window running pelican-autobuild.bat . A quick refresh of the browser page, and the newly changed article appeared in the browser. I was mostly where I needed the script to be. Further tweaking the contents of pelican-devserver.bat to: @echo off start \"Builder-Pelican\" /min pelican-autobuild.bat start \"Server-Pelican\" /min pelican-server.bat got me to where I wanted to be. To keep things clean, I wanted to specify a title for each shell window and I wanted to start them minimized. Thus, by default both windows are out of the way, but if I need them, I can find them quickly. Side Note: If you are using this as a guide, note that this only seems to be an issue on Windows machines. In quick tests I did using an Ubuntu image running under Docker, I did not see this issue, and the served pages updated properly. Step 3b: A More Representative Test Article The first thing I noticed about my test article is that it was short. Really short. When I posted it, it was hard to notice where the test article was being summarized, and where it was just the article. I needed to come up with a test article that would be a better representation of an actual article. For that, I dug back into my testing experience. When coming up with test information for test projects, I have used the site Lorem Ipsum and sites like it for years. The site goes into more details, but by using this text it allows someone viewing the text to focus on what is around the text instead of the text itself. This is also better than using random words, as someone viewing that random text may try and find patterns in the paragraphs, once again distracting from what is around the text. After a couple of quick clicks on the site, and it generated 5 decent sized paragraphs of random text that started with: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc eget velit porta, efficitur justo at, sagittis nulla. Donec neque arcu, condimentum sed massa a, elementum rhoncus justo. ... I didn't include all 5 paragraphs here, as it was randomly generated. If you are using this as a guide, you can easily generate your own test paragraphs and insert them into your test message. However, when the page was regenerated, this is what it looked like: Also, to give a bit of contrast, I created a copy of the page, called it test-2.md , changed the title a bit, and removed all except the first two paragraphs. My reasoning behind this was to make sure I had a sample that was long and a sample that was short. Step 3c: Commit The Changes As always, before moving on, I needed to make sure I committed the changes. This one was easy, simply performing the git add action on the files test.md , pelican-autobuild.bat , and pelican-devserver.bat , followed by a git commit . What Was Accomplished At the beginning of this article, I only had the foundation for a basic web site. The first thing I did was to make sure I had a good workflow for generating the web site and viewing it in a browser as a web site. After that, I added a sample article, and also improved the original build/preview workflow. Finally, I created more realistic data for the test article, so I could see what a normal article would look like. As for the goal of creating a test article that I could use to start fine-tuning the site, I believe I am in a good place. So next, fine-tuning and selecting a theme! What's Next? Next, I need to clean up a couple of small things before selecting a theme for the website. Getting a solid choice for a theme is the last major task I have to complete before publishing the web site itself. Completing this last major task will be covered in the next post Getting Ready For Publishing - Themes and Minutiae .","tags":"Technology","url":"https://jackdewinter.github.io/2019/09/01/static-websites-posting-my-first-article/","loc":"https://jackdewinter.github.io/2019/09/01/static-websites-posting-my-first-article/"},{"title":"Static Websites: Setting Up the Pelican Static Site Generator","text":"This is the second article in a series about setting up my own web site using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction In the previous article, I decided that the Pelican Static Site Generator (SSG) was the right choice for me. This article goes through the steps that I took in setting up Pelican on my system, providing a play-by-play for anyone thinking of doing the same thing. Feel free to use this as a guide for setting up your own web site to experiment with. If you are not using this as a guide, I hope it provides you with any details that you require for regarding Pelican and it's setup. From my experience, it is more important for me to make a small step forward and lay a good foundation for what comes next than it is to take leaps and bounds and miss things. Hence, I feel that focusing on the setup of Pelican is a good and properly scoped step. Why Another Pelican How-To-Guide? In looking around, there was a real mish-mash of articles out there: Making a Static Blog with Pelican Using pelican to generate static sites on windows Creating a Blog on GitHub.io with Python Creating your blog with Pelican How to Create Your First Static Site with Pelican and Jinja2 The first thing that was obvious to me was that all of the Pelican posts I found were written in 2017 or earlier. This means that these articles refer to versions of Pelican before the current 4.0.1 release that I am running, so they are either out of date or inaccurate. The second thing that was obvious was there were very few posts written about using Pelican on a Windows machine. According to the site NetMarketShare.com , Windows machines account for over 87% of the desktop machines surveyed. While it is true (from experience) that developers prefer Macs over Windows, projects like WSL are starting to chip away at those reasons. And it still remains that for a non-developer, Windows is by far the most common machine type. As it is my primary environment, I want to make sure it is represented. Component Versions If you are using this as a guide, you may try versions of any listed component other than those specified, but note that Your Mileage May Vary . This article was written from my detailed notes of how I set up my web site, using the component versions listed. As such, if you experience any issues, I would fall back to those versions as an established baseline. If you would like to try other versions of components, I strongly encourage you to go to the baseline established in this article and commit it to your local repository (see Step 5: Committing the Changes below). Once you have that point set and committed, you can then try and use other versions of the components, having a known state to return to. Regardless, keep detailed notes about what you try, and if you find yourself in a bad state, fall back to one of your previous known states and try again. Operating System Paths From long experience as a developer, there is virtually no sematic difference between pathing that is meaningful. Windows uses a format of C:\\this\\is\\my\\directory and Linux systems use /my-mount/this/is/my/directory . I personally work on both types of systems equally and do not have any issues switching back and forth between them. One reason that I enjoy using Python over PowerShell, Bat/Cmd, and *Sh shells, is that I can properly obfuscate any such issues in my code. Python scripts can easily be written that are platform agnostic, eliminating duplicated scripts to handle platform issues. Add in to that additional support from editors such as VSCode and PyCharm, and it becomes a powerful scripting language with some real muscle behind it. While I realize others may feel differently, I expect the reader to be able to perform the same translation task while reading, with practice if required. Step 1: Install Pre-requisites There is one pre-requisite for Pelican itself, and that is having Python 3.7 installed with a current version of Pip. The information in this post was generated with Python 3.7.3 and Pip 19.1.1 . As I have a great habit of fat fingering commands, I prefer to keep most of my files in a version control system, specifically Git . While I use Source Tree as a graphical interface to Git, I realize most people use the command line. Therefore, for the purpose of any examples, I will use the Git command line interface, assuming that anyone reading this who uses a Git GUI can find the appropriate command in their GUI if needed. The commands I will be using in this article are as follows: git init - Create an empty git repository. git status - Working tree status of the current repository. git add - Add a new file to the working tree. git commit - Commit any changes to the working tree to the repository. If you are new to Git or need a refresher, please go to the links about and do some searching online for examples. These are some of the base concepts of Git, and should be understood before proceeding. With respect to how to install Python and Git/SourceTree, there is plenty of information on how to install those programs for each platform. Please google Python install , Git install , and SourceTree install for the best installation instructions for a given platform. For the Windows system I am using, I simply downloaded the installations from the web sites linked to in the previous 2 paragraphs. After installing Python on my system, the installation of required packages was very simple. At the command prompt, I entered the following line: pip install pip==19.1.1 virtualenv==16.6.0 The two packages installed were the Pip Package Manager itself and the virtualenv package. The first installed package, pip, makes sure that Python's own package manager is at the specified version. While pip does not often change meaningfully and I am not planning on using any new features, it usually pays to keep things current. The second one is a virtual environment system for Python. Virtualenv is a Python tool that allows you to isolate a given Python project. While it is not portable between different systems, it does provide for a manner in which to isolate different versions of Python and different versions of Python packages from each other. Using virtualenv will allow me to install a specific versions of Python and each package with no fear of any global changes affecting this project. To verify that I have the correct version of pip and virtualenv installed, I executed each tool with the –version parameter, expecting to see output similar to the following: XXX> pip --version pip 19.1.1 from XXX\\lib\\site-packages\\pip (python 3.7) XXX> virtualenv --version 16.6.0 After these pre-requisites were installed, I was ready to create a directory to host the content for the web site. Step 2: Create a Project Directory For The Site Before generating content, I needed to create a place to install Pelican that was isolated and self-contained. As mentioned in the previous section, that is exactly the use case that the Virtualenv program was created for. That is the first of the two tools that I needed to set up for the new directory. The other tool is version control, to ensure I can replicate and version the web site. For this purpose, I use Git. The first thing this accomplishes is to ensure that if the computer hosting the web site content gets destroyed, I still have the latest information about the web site. The other thing that this accomplishes is to ensure that if I make a change (or two, or three) to the web site that I don't like, I can always return back to previous versions of any of the documents. That out of the way, I selected a directory as a location of the web site. In my case, I keep all of my local Git repositories in a directory c:\\enlistments , so I created the directory I wanted to keep the web site in was c:\\enlistments\\blog-content . The location is totally arbitrary, so for the sake of clarity, if I refer to this directory indirectly, I will use the term base project directory . If I refer to this directory directly in a script, I will use the pattern %%%MY_DIRECTORY%%%. To create the base project directory, I executed the following commands in order: mkdir %%%MY_DIRECTORY%%% cd %%%MY_DIRECTORY%%% git init virtualenv virtualenv virtualenv\\scripts\\activate.bat In order of execution, the commands first create a directory and then changed the current directory to that directory. Once the base project directory was created, git init was used to create an empty Git repository with nothing in it, ready for the project to populate. Next, virtualenv virtualenv was used to create a virtual environment for the web site, housing that environment in the virtualenv directory of the project. Finally, the activate script of virtualenv was executed to enable the virtual environment. The script activate.bat on my Windows platform ( activate.sh on Linux platforms) performs two simple tasks: change the shell's path to use the virtual environment AND change the path to make sure that change is evident. To be sure, I checked the PATH environment variable to make sure it starts with the Python path of the project's virtual environment and that the prompt started with (virtualenv) . Note that while I used git init to create a local repository, I was still getting started with the project. As such, I didn't need to worry about ensuring that the local repository is reflected in a remote repository. At that point, the purpose of having the local repository was to ensure that I could see what changed and revert back to previous versions if needed. If you are using this as a guide, please note that from this point forward, any commands that I entered were entered in the virtual environment shell. If for some reason you close your shell and need to restore the shell to where you were, you will need to open a new shell and submit the following commands: cd %%%MY_DIRECTORY%%% virtualenv\\scripts\\activate.bat I had a good directory ready to go, but I had one small issue to fix. When I submitted the git status -s command, I encounter the output: ?? virtualenv/ As mentioned above, the virtual environment is specific to a given operating system and version of Python. Because of this, committing the virtualenv directory to Git didn't make sense, as it contains system specific information. Luckily, this issue was easily addressed by creating a file in the base directory called .gitignore with the contents: virtualenv/ The format of .gitignore is pretty simple to understand. In my case, I only wanted to ignore the virtualenv directory off the base project directory, so I just needed to add that directory to the .gitignore file. Submitting the git status -s command again, I then saw the output of: ?? .gitignore This showed me that Git is ignoring the entire virtualenv directory, instead showing the .gitignore file that I just created. Since I have only done limited setup in the base project directory, having only the .gitignore file showing up as untracked is what I expected. To be safe, I used the git add and git commit commands to save these changes as follows: git add .gitignore git commit -m \"initial commit\" The directory was ready, time to focus on Pelican. Step 3: Install Pelican Pelican itself is installed as a package using Python's Pip program. Based on information from Pelican's Installing page , both the markdown and typogrify package are useful, so I installed them as well. The markdown package allows for content authoring with Pelican using using Markdown. Using a simple text file, special annotations can be placed in the text that alter how it will look when rendered with a Markdown processor. The full range of Markdown annotations and their effects are shown here . As this is one of the requirements I established in the previous page , this support was critical. The Typogrify package \"cleans\" up the text to make it look more professional. It accomplishes this by wrapping certain blocks of text in HTML span tags to allow for CSS styling. In addition, it replaces certain sequences of characters with other sequences that add polish to the finished document. While not required to get the site up and running, I figured it would be of use later. To install these three packages, I submitted the command: pip install pelican==4.0.1 markdown==3.1.1 typogrify==2.0.7 This resulted in the installation of any dependent packages, is essence, going from the stock packages (displayed using pip list ) of: Package Version ---------- ------- pip 19.1.1 setuptools 41.0.1 wheel 0.33.4 to: Package Version --------------- ------- blinker 1.4 docutils 0.14 feedgenerator 1.9 Jinja2 2.10.1 Markdown 3.1.1 MarkupSafe 1.1.1 pelican 4.0.1 pip 19.1.1 Pygments 2.4.2 python-dateutil 2.8.0 pytz 2019.1 setuptools 41.0.1 six 1.12.0 smartypants 2.0.1 typogrify 2.0.7 Unidecode 1.0.23 wheel 0.33.4 To make sure that I would be able to keep these libraries at their current versions in the future, I needed to take a snapshot and save it with the repository. Thankfully, this is a scenario that the Pip contributors though of. On the command line, I typed in: pip freeze > requirements.txt This command produces a terse list of each package and it's version, which I redirected into the file requirements.txt . The benefit to doing this is that, at any time, I can execute the following command to restore the packages and versions: pip install -r requirements.txt Step 4: Create a Generic Web Site Now that Pelican is installed and ready to go, I needed to enact the templating system of Pelican to form the basis of my web site. The authors of Pelican have kept this simple. All I needed to do is run the command: pelican-quickstart During the installation, I was asked a number of questions: > Where do you want to create your new web site? [.] website > What will be the title of this web site? Jack's Web Site > Who will be the author of this web site? Jack De Winter > What will be the default language of this web site? [English] > Do you want to specify a URL prefix? e.g., https://example.com (Y/n) n > Do you want to enable article pagination? (Y/n) > How many articles per page do you want? [10] > What is your time zone? [Europe/Paris] America/Los Angeles > Do you want to generate a tasks.py/Makefile to automate generation and publishing? (Y/n) n Done. Your new project is available at %%%MY_DIRECTORY%%%\\website For a decent number of the questions, the defaults were sufficient. The questions that I answered specifically were: website directory keep the web site isolated in a subdirectory, for future use title something simple for now author my name here url prefix No, we can change this later time zone I didn't know this, so I entered in something silly, and it gave me an URL to a web page where I looked it up generate no, I will provide simple scripts for that When this was completed, my base directory had a new directory website/ which contained 2 files and 2 directories. The first file pelicanconf.py had the settings that I entered using pelican-quickstart . The second file publishconf.py has any remaining settings that Pelican will use when publishing. If things change with the web site, I just need to change the settings in these files, and the next time I publish, they will be in effect. The 2 directories are the key here. The content directory was created to contain any files that are the source parts of the web site. During the publishing action (covered in a subsequent post), Pelican will translate that content into output and place it in the other directory that was created, output directory. Step 5: Committing the Changes At this point, it was useful to use git add and git commit to commit what I did to the local Git repository, as there was useful progress. Entering the git status -s command, it reported that the only meaningful changes were that the website directory and the requirements.txt file were added. As both of these objects are considered source and configuration, I added them as follows: git add requirements.txt git add website git commit -m \"Base website\" If there was something that was added that was not part of source, configuration, or documentation for the web site, I would have edited the .gitignore file to include a pattern that cleanly removed those changes from Git's preview. When this comes up in future articles in this series, I will point out what I added and why. What Was Accomplished Having decided that SSGs were the correct paradigm for my web site, and Pelican the correct SSG for me to use, it was time to set it up. I documented how I installed various components, as well as how I set up the base project directory for the web site itself. Finally, I created a default web site as a solid foundation for my purposes, and made sure that I committed the base project directory to Git for version control. In my professional career, most of the time it is advantageous to build a foundation for your application, committing it to version control often. Having provided that foundation in this article, I can now proceed with the actual building of the web site. What's Next? Next, I will start with publishing a simple file to the web site and make sure I check it online for any errors. By providing realistic samples and getting experience with the publishing workflows, I will get invaluable information on using Pelican. This task will be covered in the next post Posting My First Article .","tags":"Technology","url":"https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/","loc":"https://jackdewinter.github.io/2019/08/25/static-websites-setting-up-the-pelican-static-site-generator/"},{"title":"Embracing Something Hard","text":"If you look at my LinkedIn profile , you'll see that I have been around for a while. I was a Software Developer for many years beginning in 1991 at a small 2 person company in Waterloo, until a stint in Denmark gave me a bit of a wakeup call. When I came back to the United States in January 2011, I changed from a Software Developer to a Software Developer in Test. 1 While I started my career \"making things that work\", after 20 years I was more focused on \"making things work better.\" That career change was a wonderful and happy change for myself that I have never looked back negatively on. Ed: Jenn, not sure how to phrase that last sentence. Help? To be specific, I am not a tester. I do know a lot about testing, but that is not my primary focus. My focus is looking at interconnected systems and figuring out how to improve them. It is not about breaking things, like a lot of people assume, but making the team around me better. It is about standing up and mentioning that having 5 developers struggle through the same setup process for 4 days each is just plain inefficient. It is about standing up and mentioning that having 5 developers struggle through the same setup process for 4 days each is just plain inefficient. Solution: Add a responsibility for documenting the setup process for the first person, with each subsequent person responsible for the process of updating the document to current standards. Benefits: The team owns the process and its updating, with a clear path of responsibility if it doesn't work the next time it is used. It is about looking holistically at a group of systems and, while developers enjoy being creative, focusing that creativity on the parts of the systems that will benefit them and the team the most. Solution: Use templates and common libraries where possible to reduce the amount of \"creativity\" needed for anything remotely common to as close to zero as possible. Benefits: The team spends time on stuff that needs to be solved and not stuff that has already been solved. It is about asking each member of the team what they expect of documentation from the projects that their projects rely on. Solution: Setup a set of \"how-to\" guides that documents common practices for the documentation of projects for the team, hopefully with a \"dummy\" project that gives a live interpretation of those guides. Benefits: The team writes a set of documentation for their systems that looks like they were written by the same team, instead of a mish-mash of people thrown together. My job is as much about asking the simple but tough questions, as it is about having an idea on how to respond to both the questions and answers that follow those tough questions. The actual job is almost always about automation or process, it almost always involves changing the way people look at things, and unfortunately it almost always includes having someone's ego bruised along the way. Partially due to me having Autism, I can see certain patterns very easily, almost as if I was reading a book. Changing the way developers look at things almost always brings around the bruised egos. A lot of developers associate code they write with themselves. As such, saying that the code or process can be improved becomes confused with saying that the developers themselves can be improved. And yes, when that happens, it is often the people asking the questions and making suggestions on how to make things better that take the brunt of the anger that results. I still remember my daughter asking me one time why I liked being a software developer in test, as I am often frustrated with people over a perceived lack of momentum. Thinking about it quickly, the immediate answer was easy: I am good at a decent portion of it. If you are in a box and looking around you, all you see is the box. I am able to elevate my perspective to a higher level and not only see the one box, but the boxes around it. I can see how they are stacked, and if they are looking like they will tip over. That came second nature to me. But it wasn't the complete answer. Even as I responded with that answer to my daughter, there was something missing in that answer, and it bothered me when I thought about that conversation over the next couple of years. It was years later during one of those teaching moments we as parents have with our children that it occurred to me. I was reminding one of my children that we have a saying in our family: being knocked flat on your ass is not the last step, it's just the step before we pick ourselves up, brush ourselves off, and try again. Yeah, having issues and making mistakes sucks, but they helped make us who we are, and it's how we stand up again that defines us. It was then that I realized: I became a software developer in test because it was hard. I wanted the challenge to make things better, and to help others get better at what they were doing. I knew I was going to encounter stubborn people along the way, but I was determined that I would try and figure out a way to get through to them. Sure, I get knocked flat on my rear end a fair number of times, but I always get back up and try again. And it wasn't just the other people, it was myself. I had to learn when to strive for perfection and when to strive for \"just good enough\". I had to learn to find the balance between what I felt was the right decision and what the right decision was for the business right now. I had to learn that while my own passion and vision were wonderful, unless I was able to share those things in a way that others were receptive to, they meant nothing. I had to learn to get in a state of continuous learning. After all that time, I finally had my answer: I liked being a Software Developer in Test because I was good at it and because it was a hard challenge that forced me to learn and grow. That takes me to the last couple of months. For a long time I have wanted to start my own blog and help out an open source project. I was under no illusion that either objective was going to be easy, I just didn't have a clue about how different it would evolve into from what I thought it originally was. As I was going through and picking out a platform for my blog, I kept notes and started to turn them into articles. That was relatively easy. Or at least the first pass was. I found out that when it comes to articles, I want to make sure I am saying the right thing in the right way, and can literally spend 45 minutes working on one sentence. Shortly after that, I also learned that I can spend 5 minutes getting said sentence in a way that makes sense, add text a marker like **TBD** before it, and then come back to it at the end of the article. And yes, following that, I realized that about half the time, going downstairs and doing something totally unrelated caused me to think of THE EXACT phrase that I needed within seconds of coming back after the break. Yup, learning is fun, and hindsight is perfect! This blog isn't hard in terms of writing for me, but the production of it sometimes gets me. If you want to stay on track, you have to give yourself some kind of schedule. For me it was to publish once a week on something technical. It is a challenge to always make sure you have a couple of articles on the go at any time, and that you can polish one up and publish it on a weekly basis. I also have to balance the writing with exploring stuff so that I can write about it in my blog. And I realized I have to extend that out 4-6 weeks to give me time to go through a good ideation process. In picking a theme for my website, my attention was drawn to the Elegant theme for its simplicity and crispness. Looking into the documentation a bit, I noticed that some things were close, but not spot on. I wanted to get one of those features working for my website, so asking for some help getting it working, I changed the document to better document what needed to be done. The change was welcomed, and I volunteered to help out with any other articles. That is how I started contributing to the Elegant theme. What does it entail? Take the work I am doing for my blog articles, subtract the subject matter research, in certain cases add some localized research, and supplement that with making sure I write the articles in a more professional and international. 2 On top of that, apply a bit of my developer in test training and try and make sure I have a solid theme, and that I am making the process easier for me and other users of Elegant in the process. For sure, doing these things at the same time can be nuts, but I am thoroughly enjoying the challenge. I am growing both personally and professionally as I work though things on each project, some inside of my expertise and some outside of it. Sometimes I wish there were more hours in the day, but I wouldn't trade the learning I am doing for the world. Yeah, it's quite often hard, but it wouldn't be worth it if it wasn't hard, would it? In the United States, where I currently live, I am a Software Development Engineer in Test or SDET. I do not have an engineering degree. In any other country, including my native Canada, I am a Software Developer in Test or SDT. ↩ I will probably write an upcoming article about this to explain it fully. In essence, if you are writing in English for an international audience, you have to remember that a fair percentage of your readers are not native English speakers. Until they reach a point in their English proficiency, they will typically think in their native language and translate what they are reading from English to that native language. As such, you want to keep your language as free from idioms and imprecision as possible. ↩","tags":"Technology","url":"https://jackdewinter.github.io/2019/08/18/embracing-something-hard/","loc":"https://jackdewinter.github.io/2019/08/18/embracing-something-hard/"},{"title":"Static Websites: Choosing a Static (Web) Site Generator","text":"This is the first article in a series about setting up my own web site using a Static Site Generator. For other articles in the series, click on the title of the article under the heading \"Static Site Generators\" on the right side of the webpage. Introduction Why do I want a static web site? It has been on my mind for a couple of years that I would like to talk about some of the things that I find cool and other things that I feel strongly about. As a big proponent of servant leadership , I want to help and inspire people to do more, be more, and help more. Basically, I want a low-friction, easy to use platform that allows me to communicate with and help others in my own way. I also want tooling and workflows to be efficient so that I can spend more time communicating and less time fidgeting on the web site. To this end, I want to focus on something that is simple, but extensible. I want to be able to have a good mix of my own pages and a blog, allowing for flexibility on what I am doing. I want to be able to focus on the message and my voice, rather than the medium. From my research, Static Site Generators fulfils those requirements in spades. But as with a number of things I do, I want to take the time to determine if they are the right choice for my web site, and if so, select the right one for me. I would rather take a small amount of time now to ensure it is a good fit, than to take time reworking things because it wasn't. What are Static Site Generators? Static Site Generators (SSGs) are programs that take a certain type of web site and shift the heavy loading of the web site from the content-request point to the content-update point. Put another way, the decision of what content to show the viewer of the web page is changed from when the web page is requested by the browser to when the web page content is updated by the author. Therefore, to be eligible to use a SSG for a given web site, that web site must have all its content available when the content is updated, with no real time content present. There are small ways to get around that restriction, but those present their own challenges. As such, I am going to assume that 95% of the web site is going to be static, and that all the important information is within that 95%. SSGs themselves usually combine different technologies to achieve their results: writing, templating/styling, metadata, and publishing. The pages for the site need to be written in either a supplied WYSIWYG editor or in some format like Markdown. Once the pages are written, generic styling and templating are applied to the collection of pages to make sure that the branding on the pages is consistent. To help organize the pages, most SSGs include some form of metadata that is attached to the pages, used by the SSG to group pages and provide other guidance to the SSG itself. Finally, when the pages are checked over, there needs to be a simple way to publish them to the intended web site. Reducing the barrier to entry and keeping it low is essential to a SSGs success. Therefore providing multiple technology choices for parts of the SSG is common. All the SSGs that I looked at had a variety of options for most of these technologies, with templating and styling being the exception. In the case of templating, most of the SSGs support a single default templating engine that is used on all the pages. With respect to styling, common HTML technologies such as Cascading Style Sheets (CSSs) are most commonly used. Besides keeping the cost of the templating and styling low, using a common technology allows for lots of examples to be provided by the Internet at a low cost. Is a Static Site Generator The Right Choice For Me? In determining whether SSGs are the right technology for my site, I started making a list of the things I was primarily worried about with a web site: Security As I deal with security concerns at work, this one was top of mind. How often have I heard about various platforms and websites being hacked in the last month? To avoid this, I want to keep my web site as simple as possible. Static pages created by a SSG leave a single attack vector: the repository where my website's HTML pages are being stored. Mitigating attacks by only having a single attack vector is very attractive solution for me. Ease Of Use I don't want to be messing with the web site more than I have to. I either want to be updating something on the site in response to something I am working on, or working on that thing. I am already used to writing documentation at work in Markdown, so writing web pages in Markdown is already very efficient for me. In a similar manner, at work I keep documentation in a version control system, so keeping the pages in their various stages of completion in a readable form in version control is also efficient. Using my already existing workflows, with minor modifications, is an acceptable solution that keeps ease of use high. Real-Time Contents and User Input A lot of my focus at work is making sure that we can anticipate and plan for events that happen to change our system's state. If a user or another system sends a request to our system, can we validate the request, verify the system is in a good state for the request, and enforce the correct behavior for the response? Any good SSG takes care of this issue by eliminating any change of state. Resolving the concerns of state management by removing all state information seems like an ideal solution for the limited scope of my personal web site. Once those were out of the way, only the big question remained: Do I have any plans for a web site that would require it to support dynamic content? My primary purpose is to allow me to communicate to interested readers. As such, the sites's content is largely going to change only when I write something new and publish it. Based on the 95% barrier I set for myself above, such content appears to be will within that barrier. To handle the remaining 5%, I am fine with any dynamic content for my site being generated using JavaScript. A good example of that is using something like Disqus for dynamic comments. By handling such things with a Javascript approach, I can keep the simple things simple, only making things more complex when they need to be. To me, that seems to be a solid way to handle the few exceptions that may arise. For those reasons, I believe an SSG is an ideal choice for a personal web site with a blog. Which One to Choose? In case I need or want to do any extension work, I want the SSG to be in a language I am comfortable with, maintaining the ease of use concern from the previous section. At the moment, that limits the SSG language to C#, Java, and Python. While I can work effectively in other languages, the comfort and enjoyment levels are not there. As I am doing this for myself, I want the writing of any extensions to be easy and fun to increase the chances that I will stick with the writing and the SSG choice. Looking at a number of sites, these are the SSGs that appear the most. Generator Language Website Last Updated Hugo Go https://gohugo.io/ May 30, 2019 Pelican Python http://blog.getpelican.com/ May 13, 2019 Hexo Node.js https://hexo.io/ Jun 6, 2019 Jekyll Ruby http://jekyllrb.com Jun 9, 2019 Gatsby NodeJs https://github.com/gatsbyjs Jun 9, 2019 (Note that the last updated column is current as of 2019-Jun-09.) Given these goals in mind, I looked through the choices for static site generators and decided to go with Pelican. It's written in Python, has been maintained recently, any extensions are written in Python, and seems to be easy to use. This choice supports my desire to write pages and articles in Markdown, and any needed extensions can be tweaked if needed. What Was Accomplished At the beginning of this article, I was intrigued by Static Site Generators and whether or not SSGs would be an effective tool for me to communicate through. During the article, I not only determined that it was the right fit, but selected the Pelican as the SSG to use, based on its Python coding and recent updating. I feel that this determination to use SSGs, and Pelican specifically, puts me in a good position to start building my web site with confidence. If You Are Trying To Decide… If you are trying to decide if SSGs, or a specific SSG, will work for you and your site, I would encourage you to walk through a similar process to what I did. Figure out your own concerns for your own web site, and determine whether or not an SSG will address those concerns. If you think you may write extension for it, take a look at the language of the SSG and make sure you are familiar with the extension language. Most importantly, figure out whether your choice of SSGs in general or a specific SSG will serve as a tool for you to publish your information, or whether it will be an impediment to you publishing. From the point where I was at when I made the decision, Pelican appeared to be a good choice for me. I did some research to make sure this was the right choice for me. Make sure you ask yourself those questions, get those answers through research, and make sure your choice is going to work for you! What's Next? Next, I need to start exploring Pelican and how to set it up with default content. This will be covered in the article Setting Up the Pelican Static Site Generator .","tags":"Technology","url":"https://jackdewinter.github.io/2019/08/18/static-websites-choosing-a-static-web-site-generator/","loc":"https://jackdewinter.github.io/2019/08/18/static-websites-choosing-a-static-web-site-generator/"},{"title":"Glanceable Displays: Setting Up Our Display","text":"Preface This is the fourth article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In the first article, Glanceable Displays: What Are They? , I introduced the concept of a Glanceable Display, describing a number of things to consider if you decided to embark on a project like this. Assuming you decide to proceed, the article Glanceable Displays: Installing Raspbian on a Raspberry Pi details how I went from an unformatted Raspberry Pi to one with the Raspbian operating system installed. As I prefer working from my own workstation, I also detailed the setup of the SSH service on the Raspberry Pi to allow for remote connections. Finally, the article Glanceable Displays: Fine Tuning A Raspberry Pi Installation shows how I filled in a number of gaps that I encountered with the normal installation, namely setting up a wireless connection with my network and ensuring the Raspberry Pi has a solid understanding of the current time. Having taken all of those steps to be confident that setting up the actual display will work, it is time to jump right in and set the display up. But what is it actually that I was setting up? What Are Our Display's Implementation Requirements? When it comes down to it, the articles in the series have been building up to this point. I now have a glanceable display is a Raspberry Pi. Upon boot, it will start a web browser pointing at a specific webpage. It's that simple. However, the devil is in the requirements. The high level requirements for the display were covered in the first article in this series, Glanceable Displays: What Are They? . What is left are the specific requirements that will realize those high level requirements into an actual display, the implementation requirements. The first implementation requirements, already fulfilled, is that any administration of the machine can be performed from my desktop. With the exception of the Raspberry Pi seizing up, which I have noticed from time to time, I should not have to touch the Raspberry Pi itself. For the most part, I will be running sudo reboot to restart the machine, but all of that should be done without the need to plug a keyboard into the Raspberry Pi. The second implementation requirements is that after a reboot of the machine, I shouldn't need to plug in a keyboard and type some commands to get it started. After the machine starts, it should open a browser and display the page or pages that are required. To be clear, on reboot with no keyboard and no mouse, the display should start by itself. The physical cost of plugging a keyboard into the machine kind of defeats the \"appliance\" feel that I want it to have. The final implementation requirements is that, to the best extent possible, any processing of what to show on the display should be performed on a machine other than the Raspberry Pi. While some of the more recent machines have more power on them, I want to be able to use lower cost Raspberry Pi machines for the display. If I must also run scripts to pull information to generate information for the display, it means I need a heftier machine. While this may change later, I believe that starting with the lower machine requirements is the right thing to do. Installing the Right Tools To make sure that I had the right browser and other utilities I needed, I ran the following command: sudo apt-get install chromium-browser x11-xserver-utils unclutter A lot of Raspbian installations will come with chromium-browser package already installed, but I included it for anyone following along. The x11-xserver-utils package has one or two small utilities that will make the display look cleaner. The unclutter package allows me to hide the mouse cursor after inactivity, perfect for a display where I know there will be no mouse attached. Note When I was testing out the installation instructions, one of the things that made me include instructions on setting up a time server is the apt-get command. In certain cases, if your Raspberry Pi's clock is too far in the past, you will not be able to access the right packages with apt-get . Please make sure your Raspberry Pi's clock is current before using the apt-get command. Creating a Local Startup file By default, Raspbian comes with a heavily modified version of the LXDE or \"Lightweight X-11 Desktop Environment\". According to the documentation , the startup configuration file for the pi user needs to be located at the path /home/pi/.config/lxsession/LXDE-pi/autostart . If it is not there, it will default to the generic file located at the path /etc/xdg/lxsession/LXDE-pi/autostart . Following the advice of various articles, I elected to create a copy of the autostart file in my local directory. That way, if something bad happened, I could always start from the beginning again by copying the default file over again. To accomplish this, I executed the following commands: mkdir -p /home/pi/.config/lxsession/LXDE-pi/ cp /etc/xdg/lxsession/LXDE-pi/autostart /home/pi/.config/lxsession/LXDE-pi/autostart To place the file in the proper directory, I performed a mkdir command as the LXDE-pi directory did not exist with the clean Raspbian installation I was using. Once I had the directory created, I used the cp command to copy the default version of the autostart file into that directory. At that time, the file looked like this: @lxpanel --profile LXDE-pi @pcmanfm --desktop --profile LXDE-pi @xscreensaver -no-splash @point-rpi Starting Chromium In Kiosk Mode Now that the local startup file was present, I needed to edit it with the command nano /home/pi/.config/lxsession/LXDE-pi/autostart to add my own startup commands. To be honest, I tried a number of different things recommended by different articles, and each one had good points and pad points. In the end, I ended up with a simple addition to the the autostart file which was as follows: @unclutter @xset s off @xset s noblank @xset -dpms @chromium-browser --incognito --start-maximized --disable-notifications --disable-extensions --disable-hang-monitor --disable-infobars --kiosk https://bingwallpaper.anerg.com/ As mentioned above, the unclutter tool makes the mouse disappear when not used, which is perfect for a display that is never going to have a mouse. The xset tools allows for the setting of various XWindows related settings. Specifically, the s setting is for the screen saver and the -dpms setting is for the monitor's Energy Start features. Finally, the Chromium browser is the browser I chose to start with for displaying the webpages as it has the most documentation on command line switches . Note When I say the most documentation on command line switches, look at the link. The list is way too large to confidently comprehend. As such, I had to take guesses as to which of the –disable switches I needed. In order, the changes I made to the configuration file: hide the mouse turn the screen saver off don't blank the screen turn off any Energy Star power save monitor features start the browser (window maximized, incognito, in kiosk mode) pointing at the Bing wallpaper page After I finished editing the file, I made sure to save the file (ctrl-X, yes , enter), and then double checked all of my changes. When I confident I had all of them entered correctly, I proceeded to the next step. Verifying things are Working Properly I issued a sudo reboot to reboot the Raspberry Pi. And waited. And waited. And waited. If the repetition doesn't make it clear, it felt like forever. I was sure I had followed my own instructions properly. Even going off of my own notes, there was the anticipation of seeing whether or not it would work. The Raspberry Pi I was using seemed slower than usual, but after a while, everything started up and it was displaying the website https://bingwallpaper.anerg.com/ in the browser. As I checked the display, I saw the mouse pointer disappear after a few minutes. Check. After a couple of hours, the screen saver had not kicked in. Check. After a couple of hours, the monitor was still displaying the website. Check. For the most part, due to some good notes that I kept, everything was up and running the way it was supposed to. But What if it Is Not? The first log file that I used to debug things was the /home/pi/.xsession-errors file. After executing the cat /home/pi/.xsession-errors command, I noticed that while it didn't have a lot of useful information, it had two important pieces of information: ** Message: 17:00:13.893: main.vala:101: Session is LXDE-pi ** Message: 17:00:13.894: main.vala:102: DE is LXDE ** Message: 17:00:16.375: main.vala:133: log directory: /home/pi/.cache/lxsession/LXDE-pi ** Message: 17:00:16.376: main.vala:134: log path: /home/pi/.cache/lxsession/LXDE-pi/run.log This information let me know I was putting the changes in the right place, and where to look for the log for the current user's session: /home/pi/.cache/lxsession/LXDE-pi/run.log . Also, the fact that it was putting the logs in the /home/pi/.cache/lxsession/LXDE-pi directory meant that it noticed the autostart file that I added, as was using it. That was a useful piece of verification. When I started looking at that file, I was at first overwhelmed before I found a couple of tricks to help me out. The first trick was to look for a file that looks like this: ** Message: 17:00:16.917: autostart.vala:42: Autostart path : /home/pi/.config/lxsession/LXDE-pi/autostart Everything before that point is just the other parts of LXDE getting set up, and it really didn't have any effect on what I was trying to do. After that line were a series of lines that began with Launching , corresponding with each line of the autostart file. The next section of lines complemented those lines, providing for the exit codes of each of the lines in that file. Finally, there is a section starting with Connecting ... that signifies the section of the log where the wired and wireless link status is logged. While the link status is important, the fact that it gets to this point successfully generally means that the display is ready to go! What Was Accomplished This article detailed the work that was needed to take my Raspberry Pi from a sufficiently set up machine to fulfilling 2 out of the 3 requirements of my a glanceable display. After noting some simple requirements for my glanceable display, I proceeded to add some tools to the machine that were needed for the configuration changes that followed. In case someone is following what I am doing for their own glanceable display, I detailed some of the debugging steps that I used to check to make sure that my configuration changes were working. Administration was already enabled by setting up SSH access , satisfying the first implementation requirements. The big step forward in this article was satisfying the second implementation requirements by automatically launching a web browser pointing to a specific webpage after rebooting the glanceable display. This leaves the final implementation requirement to be completed: generating the display's content without What's Next? In the final article in this series, Glanceable Displays: What Do I Want To Display? I walk through the steps I took to determine where to get finalized webpages to display in my glanceable display, while adhering to the final implementation requirement of my glanceable display.","tags":"Technology","url":"https://jackdewinter.github.io/2019/08/04/glanceable-displays-setting-up-our-display/","loc":"https://jackdewinter.github.io/2019/08/04/glanceable-displays-setting-up-our-display/"},{"title":"Glanceable Displays: Fine Tuning A Raspberry Pi Installation","text":"Preface This is the third article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In the previous article , I detailed the steps required to get the Raspbian (Linux) operating system installed on a Raspberry Pi with remote access through SSH enabled. This article attempts to move the needle forward by tackling a number of the issues that I had in getting necessary foundation services up and running on my Raspberry Pi. The two services that I had issues with were: ensuring that Wi-Fi was working properly and that the Raspberry Pi clock was being set properly. Until I was confident that these two issues were resolved, I was not confident that I would be able to use my glanceable display in it's intended location, as that location does not have any wired network access. Note As the steps in the previous article concluded with providing SSH access to the Raspberry Pi, I performed all of the following configuration using an SSH client. While you are welcome to enter them through a keyboard directly connected to the Raspberry Pi, I did not test any of the steps in that manner. Warning From the experience of reviewing and retrying these steps, sudo reboot is a good friend. If it looks like something didn't work and you think it should, consider using sudo reboot to reboot the machine and carry on from there. This is very handy when changing configuration. Step 1: Wi-Fi Access Unless you have the fortune of having a wired network outlet available near your display AND enjoy the aesthetic of having a network cable going into your display, you most likely want to use Wi-Fi to access your display. Here are the steps I went though to get wireless access enabled. Step 1a: Searching for Existing Wireless Access The first command that I used to check for wireless network access was the ifconfig command. This is a general command used to determine what network connections are available. I had a network cable running into the machine, but nothing else, so the following response 1 was expected: eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 ... While it looks like gibberish to some, it was what I expected. Linux systems often use the tag eth0 for the first wired connection and the tag lo for the loopback connection. The important thing I noticed was that I didn't see a wlan0 tag, typically used for the first wireless connection. The clear observation I had was that the system did not currently have a capability to have a wireless connection. This observation on a normal computer running Linux might be farfetched, but from information gained researching the Raspberry Pi on various forums, it is relatively normal for a Raspberry Pi to not have any wireless capability built-in. As wireless components have recently become cheaper, it seems like it is only the latest versions of the Raspberry Pi 3 have wireless access built in. Note I have not repeated these steps on a newer Raspberry Pi, but I would expect that doing so would allow me to skip the next section on installing the adapter. I will update the article once I have tested that scenario. Step 1b: Installing a Wireless Adapter In my instance, the machine I was configuring did not have any wireless capabilities built in. That was resolved in short order by noting down the connection requirements for the household router (WPA2-PSK (AES)) and purchasing an inexpensive USB Wi-Fi adapter from a nearby store. Note If possible, use that store's online presence to inspect the possible adapters, verifying 1-3 choices for adapters that meet the router's specifications. While not necessary, it can avoid round trips to the store to try and find the right adapter. Doing this, I found that my local store had a TP-Link TL WN823N USB Adapter that was right for the job for $15. Returning home from the store with an inexpensive TP-Link TL WN823N adapter, I powered off the Raspberry Pi, installed the Wi-Fi adapter in one of the open USB slots, and powered up the Raspberry Pi. Once Raspbian had booted up, I reconnected to the machine using SSH and entered the lsusb command. This command is similar to the ls command to list files, but is used to list various USB components that the machine has recognized. The response I received from that command was: Bus 001 Device 006: ID 2357:0109 TP-Link TL WN823N RTL8192EU Bus 001 Device 005: ID 0461:4e26 Primax Electronics, Ltd Bus 001 Device 004: ID 0461:4d64 Primax Electronics, Ltd Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. SMC9514 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Seeing as the adapter I bought was a TP-Link TL WN823N, I was relieved to see it listed as device 06. At this point it was clear to me that the Raspberry Pi recognized the adapter, and it was time to install the required drivers. Step 1c: Installing the Wi-Fi Drivers From a research point of view, this was the most frustrating part of getting wireless access working. In most of the articles I researched, the authors seemed to assume that the drivers for a given adapter would already be installed in the Raspbian distribution. The few articles I found that handled missing drivers were very specific, and not very helpful. They often used the Raspbian apt-get family of commands to look for an adapter, and each adapter I found seemed to have a slightly different way of making sure it worked properly. As I was writing down notes to help other people, that experience was far from helpful. Everything changed when my research led me to a reference to the Fars Robotics driver install script. This breaks down the process into the following three commands: sudo wget http://www.fars-robotics.net/install-wifi -O /usr/bin/install-wifi sudo chmod +x /usr/bin/install-wifi sudo /usr/bin/install-wifi Instead of playing around with apt-get commands and looking for a specific driver, this script automated the process, finding the correct driver and installing it. The commands first download the script into the /usr/bin directory, set it to be executable, and then execute the script. Within 60 seconds, it had completed, and it's output suggested that the driver for my TP-Link TL WN823N had installed successfully. To double check that it worked properly, I resubmitted the ifconfig command and got the following: eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.2.3 netmask 255.255.255.0 broadcast 192.168.2.255 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 ... wlan0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 ... Great! Progress! The new Wi-Fi adapter was installed, the driver was connected, and Raspbian was reporting that it could talk with it! From experience, I now had to configure the machine to connect to the wireless network. From the output, this was somewhat obvious. For the eth0 and lo adapters, there was a line beginning with inet that described the current IPv4 setup for that adapter. As the wlan0 adapter was missing the inet line (and also the inet6 line), Raspbian could talk to the adapter, but the adapter was not connected to the local Wi-Fi network. Next, configuring Wi-Fi access. Step 1d: Configuring Wi-Fi Access Similar to the previous section on Installing the Wi-Fi Drivers, I found many articles on how to do this by hand, each with its own little twist on how to set things up better. In my case, I wanted to go for repeatable and easy, and the sudo raspi-config command used to set up SSH access proved to be the best solution. From the main menu, I selected 2 network options and then N2 WiFi . As it was my first time setting up the network on this machine, I was prompted for my country, which I entered. Next I was prompted with Please enter SSID , and I responded with my router's SSID. This was followed with the inevitable Please enter passphrase. Leave it empty if none. , to which I responded with my router's password. Hoping everything would work out, I pressed the Next button. After waiting for approximately 30 seconds, focus was returned to the main configuration screen. Exiting out of that screen and back to the main screen, executing the ifconfig now had the following response: eth0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500 inet 192.168.2.3 netmask 255.255.255.0 broadcast 192.168.2.255 ... lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 ... wlan0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.2.67 netmask 255.255.255.0 broadcast 192.168.2.255 ... This was what I was waiting for! At the moment, the eth0 wired connection had an address of 192.168.2.3 and the wlan0 wireless connection had an address of 192.168.2.67. The address 192.168.2.3 is the one I had been SSHing into while fixing up the Wi-Fi, so that lined up. Based on my router's configuration, 192.168.2.67 was a likely address for a new machine, so that lined up. Warning If you have your wireless network secured, make sure you follow the normal steps for adding a new machine to your network. In my case, forgetting my normal steps cost me over an hour trying to figure out why the Raspberry Pi would not connect to the network! To verify things were working, I repeated the relevant portions of Step 5: Setting Up SSH Access from the previous article, using the new address 192.168.2.67, instead of the old one 192.168.2.3. Once this succeeded, the final test was to disconnect the physical connection and see if the wireless connection worked. It did take a lengthy bit of time, and one trip to a local store for hardware, but it was very gratifying being able to talk to the glanceable display over a wireless connection! Step 2: System Clock Synchronization While the Raspberry Pi is a useful little machine, it's small form causes it to not have a component that many machine take for granted: a system clock. Not having this set up for your glanceable display can cause any display of time to be extremely off. In addition, if the Raspberry Pi's time is not decently close to the actual time, the downloading of extra components through mechanisms such as the apt-get commands may fail. To avoid these issues, setting up proper system clock synchronization is a must. Pre-Requisite The list of servers and download locations for the apt family of commands has most likely changed since the Raspbian image was constructed. If this is the case, any apt commands that require any kind of package updates will most likely fail. To solve this before it becomes an issue, I issued the following command to update those tables. sudo apt-get update Step 2a: Installing the NTP Service The quickest and most efficient solution to solve the synchronization issue was to install the NTP time service. In the distribution of Raspbian that I was using, the NTP service was not installed. When I entered the command sudo systemctl status ntp , I saw the following output: Unit ntp.service could not be found. Based on various articles, the clear solution was to install the NTP service using apt-get with the following command: sudo apt-get install ntp . Once that finished, when I repeated the sudo systemctl status ntp command, I then received the following output: ● ntp.service - Network Time Service Loaded: loaded (/lib/systemd/system/ntp.service; enabled; vendor preset: enabled) Active: active (running) since ... At this point, I was satisfied that the NTP service was up and running, but not that it was working correctly. Step 2b: Diagnosing the NTP Service Configuration Once the service was loaded, I needed to confirm that the NTP service was working properly. The easy way to do this was to reboot the machine using sudo reboot and then use the date command to check the date. When I followed that pattern, the date was over 2 weeks off from the actual time. Time to go into debug mode. The documentation on the NTP service mentions a useful command: ntpq -p . This command lists information about what the current NTP service is doing. Presented as a series of columns, the important column was the number of seconds since the service successfully contacted a specific NTP server. When I checked the output of the command, I was greeted with the following table: remote refid st t when poll reach delay offset jitter ============================================================================== 0.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 1.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 2.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 3.debian.pool.n .POOL. 16 p - 64 0 0.000 0.000 0.004 Looking at the when column, the issue seemed to be that it was not able to connect to those servers and get information about them. Doing a bit of research, it turned out that other people have had problems with the default servers for the NTP server, and there were ways to address that. Step 2c: Changing the NTP Service Configuration Now that I had the NTP service installed and had issues with the default configuration, it was time to look at it. The configuration for the service is located in the file /etc/ntp.conf . As I had found issues with the default configuration, I needed to learn more about this configuration file to fix the problem. Looking at the file using a command like more /etc/ntp.conf was daunting. There were a number of 1-2 line sections with multiple lines of comments before them. When I saw that, I was concerned. In my experience, a large comment to useful line ratio means they are commenting the heck out of it, because people have done stupid things in the past. Learning more about the configuration, it turned out that I only needed to look at one particular section. The most important part of the configuration is a section that started with the follow text: # pool.ntp.org maps to about 1000 low-stratum NTP servers. Your server will # pick a different set every time it starts up. Please consider joining the # pool: <http://www.pool.ntp.org/join.html> Right after this text, there was a group of servers specified where the first part of the server name was a number between 0 and 3, and the rest of the server name was the same. Many of the articles I found didn't touch the rest of the file, but focused on that small set of lines. A very useful page I found during my research was the NTP Pool Time Servers page. Near the bottom of that page is a table that lists the various geographic areas, from which I selected the North America selection. At the top of the next page was a clearly marked section of text, with clear instruction to add that text to the ntp.conf file. Given that information, I went back to the Raspbian machine, entered sudo nano /etc/ntp.conf to edit the /etc/ntp.conf file, and replaced the default debian servers in that section with the information from the clearly marked section of the NTP Pool Time Servers page. Followed up with Ctrl-X to save, y to save the file, enter to use the provided file name, and it was changed. Just to make sure, I did another sudo reboot after verifying my changes, and the date command now returned the right date. Step 2d: Verify that the NTP Service is Working Warning As a number of Denial-Of-Service attacks have used the NTP port to return bad information, a number of routers come pre-configured with their firewalls set to block port 119 and 123, the NTP ports. If you follow these instructions and are still having issues, check the firewall settings for your computer and your router. Previously, when I had checked the status of the NTP service using the ntpq -p command, I did not seeing anything other than - in the when column of the output. If things were working properly, I would expect that column to change. Submitting the ntpq -p command once the system rebooted, I got the following output: remote refid st t when poll reach delay offset jitter ============================================================================== -grom.polpo.org 127.67.113.92 2 u 21 64 1 59.302 9.614 6.053 +199.180.133.100 140.142.234.133 3 u 17 64 1 26.382 5.225 3.611 +time1-inapsales 216.218.254.202 2 u 17 64 1 41.043 -2.463 4.417 *vps5.ctyme.com 216.218.254.202 2 u 18 64 1 36.759 -5.673 2.870 Submitting the command multiple times, I observed the when column values increasing to a certain point before starting over at a low single digit number. From the documentation, this was a good indication that each of those servers was being queried successfully, then registering itself for the next time to check against that server. Seeing the successful connections with the NTP servers, the time synchronization issues were cleared up! Just to be sure, I did a couple of cycles of sudo reboot and checking the time, with no errors. What Was Accomplished This article detailed the steps taken to fix two of the major issues I had after installing Raspbian on my Raspberry Pi: the Wi-Fi access and the system clock being out of sync. I worked through the various steps taken to ensure that the wireless access was up and running, including the various checks I did throughout the process. I then walked through the steps I took to ensure that the time on the Raspberry Pi was reflecting actual time, despite not having an onboard system clock. With this accomplished, I had confidence in having firm foundations on which to start building the display part of my glanceable display. What's Next? In the next article in this series, Glanceable Displays: Setting Up Our Display I walk through the steps I took to setup the Raspberry Pi to start the display upon boot for the glanceable display. In any of these examples, a set of ellipses (\"…\") are used to denote that content was removed that was either sensitive or not relevant to the example being presented. ↩","tags":"Technology","url":"https://jackdewinter.github.io/2019/07/28/glanceable-displays-fine-tuning-a-raspberry-pi-installation/","loc":"https://jackdewinter.github.io/2019/07/28/glanceable-displays-fine-tuning-a-raspberry-pi-installation/"},{"title":"Glanceable Displays: Installing Raspbian on a Raspberry Pi","text":"Preface This is the second article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction The methods detailed in this article provide for simple installation of the Raspbian operating system using the New Out Of Box Software (NOOBS) installation method, as suggested for beginners by the Raspberry Pi site . While there are more direct methods for experienced users, the NOOBs installation was selected for it's simplicity and ease of installation. Using the NOOBs installation, this article details the first steps I took in setting up one of my Raspberry Pi systems to be a glanceable display for my family. Those steps start with the formatting of a MicroSD card and installation of the NOOBs installer on to that MicroSD card. After installing that card into the Raspberry Pi, the steps continue with the installation of a stock Raspbian distribution, detailing the a couple of questions that need to be answered to complete the installation. Finally, to enable remote access, the last step is to ensure that I can access the Raspberry Pi using SSH for later configuration and control. Requirements Raspberry Pi, version 3.0 or later Power Supply for Raspberry Pi Keyboard and Mouse HDMI cable connected to monitor Cat5 Ethernet cable Note Please keep your own notes as you go, and refer back to them. While I have tested the steps on my own Raspberry Pi machines, they were by no means exhaustive tests. Step 1: Interfacing With a MicroSD Card The configuration and main drive for a Raspberry Pi is a MicroSD card. To get the card ready for use, your computer must be able to interface with the card. Most computers do not come with MicroSD slots, but there are a fair number with SD slots. To make things easier for MicroSD buyers, some of the more high end MicroSD makers include a MicroSD-to-SD adapter in their packaging, such as this 32GB MicroSD card from Amazon . I started out using this, but found that the adapter was only good for 3-4 uses, not for continual use. An alternative is a more multi-purpose adaptor, such as this multi-adapter from Amazon . As it is made from a more durable material, it will survive more uses. The one that I bought from Amazon at 7.00 USD is still working after about 70+ uses, so at 0.10 USD per use, it has already paid for itself. Also, as it has a USB adapter, I can plug it into a USB extension cable that I already have on my desk. Whichever way you decide to go, make sure to add the MicroSD card to the adapter before plugging the adaptor into you computer. Once it is securely in the adapter, make sure to apply it to the relevant slot on your computer firmly, and make sure the connection is there. On my Windows 10 machine, I can tell this happens as it will acknowledge the connection by opening up an Explorer window, with either a \"please format\" instruction or a list of any files in the directory. Step 2: Getting the MicroSD Card Ready Note Note that the steps that follow are for my Windows 10 machine. The NOOBs site , has sections for installing on Mac and Linux, but I did not test them. If they do not work,please Google/Bing linux microsd card format and linux microsd card mount . Feel free to replace the generic linux in the searches with the name of the Linux distribution that you are using. Step 2a: Reformatting a Used MicroSD Card Reusing old hardware is important, for many reasons such as the environment and cost. To make sure that is possible, it took me a number of tries to create a solid recipe for reformatting the MicroSD cards. As I mention in the Requirements section, keep good notes of what you do, or if following a recipe like this one, what changes you made to the recipe. While there is a command line only tool that will also do the job, I found it clunky and hard to use. Instead, the Disk Management applet for the control panel was the tool I settled on. This can be invoked by typing partition in the search window on the tool bar and selecting the create and format hard disk partitions item. Selecting that item brought up the Disk Management window, showing a break down of every drive connected to my computer. When the MicroSD card was properly connected to the computer, it showed up as Removable media after all of my permanent drives. Using the right mouse button, I clicked on each of the blocks on my MicroSD card and selected the Delete Volume menu item until all of the volumes were gone. When that was accomplished, I was left with two blocks, and right clicking on the rightmost block presented me with a Delete Partition menu item, which consolidated all of the partitions into a single unallocated block. From there, I was able to right click on the Unallocated partition to select the Create Volume menu item. This started a simple wizard that quickly walked me through the options for creating a new volume. I used all defaults that were presented with the exception of the file system and quick format settings. I changed the file system setting to FAT32 and unchecked the Use Quick Format checkbox, before clicking on finish and waiting for about 30 minutes before the format was complete. Step 2b: Formatting a New MicroSD Card From a Windows 10 point of view, this was easy. When the MicroSD card was properly connected to my computer, it prompted me to format the card, presenting me with the format dialog itself. When formatting the MicroSD card, it was important to select FAT32 as the type of format and to unselect Quick Format on the dialog. Once I clicked the format button, it took a good while before it was completed. As a rough estimate, I guessed that it was roughly 1 minute per gigabyte on the MicroSD card, regardless of computer speed. Step 3: Install Raspbian Lite Using NOOBS Note When the format is finished from the previous step, it is important to go to your taskbar and eject the media from your computer. I accomplished this by right clicking on the USB stick icon and selecting \"Eject Mass Storage Device\" from the menu. At that point, I cleanly removed the adaptor and the MicroSD card from the computer to ensure the ejection was complete. When I tested various scenarios, any time that I forgot to eject the media at this point, it did not take later on. The people behind the Raspberry PI made sure there is a simple to use installation system that simplifies the task of installing operating systems on to the Raspberry Pi. The New Out Of Box Software (NOOBS) site aims to allow a fairly basic installation of Raspberry Pi operating systems with little effort. Unless you are familiar with Linux systems, their installation can be very daunting, so it is best to keep the installation as simple as possible. To start that process, I downloaded the NOOBS zip file from their web site to my computer. After reinserting the MicroSD card and adapter to my computer, I then unzipped the contents of the NOOBS_V3_2_0.zip file to the root of the drive for the MicroSD card. I had to take care to ensure that the contents were in the root of the drive, not in a subdirectory of the drive. This happened enough times that I actually unzipped the files to a local directory and just used XCOPY to copy the files over, solving the placement problem for myself. As with the note at the start of this section, once this action was done, I once again ejected the USB device before disconnecting it from the computer, for the same reasons. Taking the MicroSD card, I found the MicroSD port on the Raspberry Pi. The port is flat with the motherboard of the Raspberry Pi, and the cases I have all have a hole in the case to make it easy to find. Inserting the card into the port, I then attached the other cables for monitor (HDMI), ethernet (Cat5), keyboard (USB), and mouse (USB), with the 5V adapter cable being last. Two minutes later, I was presented with a screen which prompted me to select the operating system to install. I tried a number of times to get the Raspbian Lite install to work, but encountered a number of issues, so I defaulted to the stock Raspbian [RECOMMENDED] install. Once I made this choice, I selected Raspbian [RECOMMENDED] from the top of the list in the NOOBs installation dialog, followed by pressing the Install button at the top. From there, it took about 30 minutes or so before I was prompted with a dialog box that said: OS(es) installed successfully When I pressed the OK button on that dialog, the system rebooted with first a rainbow screen, then a screen with a big raspberry on it, then lots of text scrolling by quickly. After a relatively small amount of action and a lot of waiting, it was now time to set up the operating system for simple uses! Step 4: Initial System Setup There was a lot of text that scrolled by so quickly, I was unable to read it. From what I could see, there were a lot of green OK texts on the left side, so I guessed that the installation had succeeded. After a nice round of blinking lights from the Raspberry Pi, the desktop showed up and proceeded to walk me through the setup configuration. The first dialog was pretty simple, with the title Welcome to Raspberry Pi . The important thing to note off of this dialog is at the bottom right of the dialog is the IP address that the system currently has assigned to it. As this was important, I wrote it down, and proceeded with the rest of the configuration. The configuration is a series of simple dialogs, each giving a clear indication of what is required. Whenever I pressed the Next button, it wrote the information to the system configuration files. As such, I expect delays between when I pressed the Next button and when the next dialog showed up. Turns out that was a rather healthy expectation. Some of the things that were setup were: country language time zone setting a new password take care of black border update software Having tested this workflow, I knew that the next workflow for my glanceable display would include updating existing packages and installing new packages. As such, I skipped the update software, knowing I would do it later. Both paths produce the same results, so feel free to skip it like I did, or update at a later point. Warning If you forget the password that you change it to, there is no easy way to recover what you changed the password to. Consider creating a fake entry in a password manager, like LastPass, and storing the password there for later use. Step 5: Setting Up SSH Access That being accomplished, the last thing to complete before stopping the installation of the bare bones system was to enable SSH access. By enabling SSH access, I could sit at my comfortable workspace, using my normal computer, chair, and desk instead of at the workbench where I had the Raspberry Pi connected. Frankly, the computer was connected in an almost Frankenstein like mess of wires on an old desk with a chair that was normally reserved for people visiting, not typing. My own workspace looked very inviting. To enable access, I entered the command sudo raspi-config , selecting 5. Interfacing Options , then selecting P2 SSH , and finally answering Yes to the question Would you like the SSH server to be enabled? . After this, the computer took about 30 seconds before being responsive again, with the text The SSH server is enabled. appearing on the screen. Pressing the enter key, and then selecting Finish , I was then back at the command prompt. This was the moment I was working towards: being able to have a bare bones system to use that I could access from my own computer. Entering sudo reboot , I waited about 45 seconds for the system to reboot and to be greeted with the raspberrypi login: prompt. Looking just above that text, I saw the text: [ OK ] Started OpenBSD Secure Shell server. This gave me a bit of confidence to move forward. At the very least, the operating system was indicating that it should allow SSH access. At the command line, I entered: ssh pi@192.168.2.3 and with the exception of the input yes to answer the question, the output was as follows: The authenticity of host '192.168.2.3 (192.168.2.3)' can't be established. ECDSA key fingerprint is SHA256:`some text here`. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '192.168.2.3' (ECDSA) to the list of known hosts. ... pi@raspberrypi:~ $ Note After each repeated installation on the same Raspberry Pi, when I went back to open a new SSH connection, it would report the error WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! . To allow access, I needed to edit the ~/.ssh/known_hosts file and remove the line for the IP address of the machine, as indicated at the end of the provided error message. What Was Accomplished This article detailed the steps taken to install the Raspbian operating system on a MicroSD card. It started by my formatting of the MicroSD card and copying the NOOBs installer onto the card, followed by inserting it into the Raspberry Pi's MicroSD slot. The steps continued with the largely automated installation of the operating system, only requiring the answers to six questions on my part. Finally, it concluded with the setup for SSH to allow me to configure the Raspberry Pi remotely. What's Next? In the next article in this series, Glanceable Displays: Fine Tuning A Raspberry Pi Installation , I walk through the steps I took to move the installation from a bare bones system, to one that had Wi-Fi and time support set up properly.","tags":"Technology","url":"https://jackdewinter.github.io/2019/07/21/glanceable-displays-installing-raspbian-on-a-raspberry-pi/","loc":"https://jackdewinter.github.io/2019/07/21/glanceable-displays-installing-raspbian-on-a-raspberry-pi/"},{"title":"Glanceable Displays: What Are They?","text":"Preface This is the first article in a series about Glanceable Displays and my efforts to create one for my household. For other articles in the series, click on the title of the article under the heading \"Glanceable Displays\" on the right side of the webpage. Introduction In this article, I will introduce the concept of glanceable displays and describe how they can be used in an everyday office or home. I will then discuss how the audience for the display is important, including a side bar about my opinion on Spousal Acceptance Factor. Understanding the limitations of your audience is also covered, leading up to developing a set of definitions on what should be displayed. Finally, I will talk about how my family followed those steps to arrive at the requirements for our glanceable display, which has buy in from every member of our household. What Are Glanceable Displays? According to Dictionary.com : 1 noting or relating to information on an electronic screen that can be understood quickly or at a glance: glanceable data; a glanceable scoreboard. 2 enabling information on a screen to be quickly understood: a glanceable design; glanceable interfaces. In essence, a glanceable display is a display that provides information that can be quickly understood by people reading the display. An important qualification of these displays is that the information they display needs to have a broad degree of applicability without any ability for a specific reader to customize the data to their needs. As the display must provide information with no personal customization, it is important to think about various things up front. Who Is the Display For? The audience of the display will drive many of the other choices for the glanceable display. Be honest about who the real audience is for your glanceable display, and what benefits that audience will gain from the display. Negotiate with the audience members and make sure that there is something on the display for each member. If possible, engage with those members to help with the display so their work becomes part of the display, fostering interest from the beginning. The key is for your project to make the jump from \"my toy project\" to \"our useful project\", solving real world issues with real world requirements. If you are not honest about your audience and communicated with them sincerely, you will invariably end up missing your goals for the display. Without these, your base requirements will not be correct, and your final product will not be correct. If you only want the display for your personal office, it is perfectly acceptable for you to say that you are the sole audience and geek out on the display. That is, it is acceptable if you are honest about that audience. In that case, just be realistic and don't expect people you didn't include in your original audience to appreciate your display. After all, you made it just for you! Spousal Acceptance Factor As an aside, I have often heard of concerns from people about something they made having a low SAF or Spousal Acceptance Factor. I honestly think that is silly. If there is a low SAF, it probably means that someone did something where their spouse was either not consulted on not considered in terms of requirements. It is human to want to share your excitement with family around you but remember that your excitement is not their excitement. Unless you sincerely include them in the audience, the chance of acceptance will remain low. How Are You Going To Display It? In terms of deciding how to display the glanceable information, we really have only one decision to make: are we going to ask the viewer to visit an online site with a device or are we going to provide access to a device at a given location? If you decide on the visit paradigm, you don't have any hardware concerns as the reader is supplying the hardware. With the viewer using their own device to view the display, another decision must be made regarding whether to standardize on a single device type or to support multiple device types. If you decide to support multiple device types, you will probably need to use an intermediate step where you start with support for only the most used device type. Once that device type has been completed, you can then slowly adapt your display to the next device type your viewers are using. You will also need to ensure that you have a clear definition on where your display can be accessed from. If you have a web site for your office that you display on phones or tablets, can it be viewed from anywhere, or just within the office's WiFi area? If you decide on the specific location paradigm, you limit your device type to one, but you take on additional hardware concerns. You get to focus on a single device type, but in exchange, you need to provide for the setup and control of that device. Consider the case where the power goes off and then comes back. How will your hardware solution handle that or will someone need to reboot it? Another important consideration is the cost of the hardware and any needed peripherals. Will you reuse existing hardware that you already have, or will you require expenditures? The output from your evaluation of this section should be a choice of a single approach and a list of reasons why the selected approach was chosen. If possible, provide as many details as possible as it will help you in other sections going forward. Also, from your audience's point of view, it will help them understand the decisions that you have asked them to buy in to. What Do You Need To Consider Up Front? As the display is a glanceable display, this means that everyone should be able to view the display with few issues, if any. Common issues to think about are: Near and Far Sightedness A large segment of every population either wears glasses or contacts at some point in their life. Especially as people get older, their eyesight gets weaker and they rely on glasses more frequently. Depending on the differing eyesight of your audience, you may want to consider using larger fonts to enable them to see the screen more clearly. In addition, you may want to consider a high contract mode that includes fewer, but bolder colors to improve visibility. Color Blindness Color blindness, as shown in this article on Wikipedia , is an inability to see differences in either all colors or certain types of colors. Keep in mind that if you audience is not a family environment, the person with color blindness may not disclose that they are color blind up front. If one of your audience is color blind, using colors on your display to indicate certain things is a bad idea. Use shapes or text instead of colors to denote differences in the data being displayed. Dyslexia and Other Reading Disorders Dyslexia, as shown in this article on Wikipedia , is actually a family of similar disorders involving the brain and how numbers and letters are processed when reading. Other reading disorders, such as those mentioned in this Wikipedia article , are often grouped by people as dyslexia, when they are only related. As with dyslexia, keep in mind that if you audience is not a family environment, the person with dyslexia may not disclose that they are color blind up front. Advances in research on reading issues have produced advances such as the Dyslexie font which is specially made for people with dyslexia. Engage with your audience to determine if any such issues apply, what their effects are, and talk with them and research with them on ways to adapt the display to make it easier for them to comprehend. Young Readers Young readers, due to their age, are either still learning how to read or are building their vocabulary as the grow. To assist any young readers that are going to use your display, consider replacing some of the objects that you want to display with pictures that indicate the object's meaning. For ‘older' young readers, keep in mind that their vocabulary is different than yours, and change you designs for the display accordingly. What Are You Going To Display? Once you have all the other sections taken care of, the decision of what to put on the display is almost anti-climactic. After going through the process of identifying your audience, the type of display to use, and any considerations for your audience, you have probably defined at least one or two things to display along the way. At the very least, armed with the information above, you can engage with your audience in a brainstorming session that will allow you to get buy-in from them. Two important things to remember at this stage: soliciting help and iterative development. Don't be afraid to ask your audience to help you in the design for the display. That can take any form you want, from design the information for the display with them there to asking them to create the displays and presenting them to the entire audience. Remember, for the glanceable display to be successful, you will need buy-in from your audience. Having them help with the work will do that. Iterative development means that you and your audience are going to try and make something that works, but you do not expect it to be perfect on the first try. You and your audience may be confident think something works when you look at it initially, but over time that confidence may change. Don't be afraid to iterate on the design, keeping the things that work and changing the things that don't work. Our Discussion About Our Glanceable Display Understanding that any decision would affect my entire family, I asked if we could talk about it after dinner one night. In discussing the possibility of a display, we all agreed that there were common things that it would be nice to see. These things came down to 3 categories: calendar related, weather related, and other. As we all have busy lives, the calendar was a slam dunk. Our whiteboard calendar didn't get updated when it should, which left us looking at our online calendars on our phones. But even with our online calendars, it was a pain to remember to invite other family members to any events we had, regardless of whether or not they were part of the event. Having a single place to display that information keyed to a given individual would clear things up and simplify things a lot. Information on the weather was another category, mostly due to the Seattle weather. While my son wears the same type of clothes every day, my wife and I vary our clothing by the type of weather and activities we have planned. Having that advance knowledge of weather would cut down on having to actively seek that information from a number of online sources. After those two big categories, there were also some other little things brought up. Not having a good place to put them, the \"others\" category was formed. The discussion then moved to decide which class of glanceable display to use, and our family made a simple decision to go with a monitor driven by a web page hosted on a Raspberry Pi. We all agreed that we wanted something that would replace a seldom updated whiteboard calendar in our kitchen. It needed to be big enough to show several weeks' worth of calendar at a time, to allow us to plan that far out. We also wanted to make sure we kept each other honest, so we explicitly wanted it not tied to any of our personal computers and tied to a location that we know we all frequent: the kitchen. The choice of the Raspberry Pi satisfied these concerns pretty easily. From a hardware point of view, I had a spare one at home from a project I had wanted to do, but never started. From an operating system point of view, I have enough knowledge of Linux systems that I was confident that I would be able to handle the configuration. Finally, I was prepared to take the challenge on of setting up the system and working with my family to define the elements of the display with their input at ever step. The Decisions for Our Glanceable Display So, from those discussions, I arrived at the following. Audience The audience was our family. Display The display itself would be a simple Raspberry Pi with a monitor that was left from a project that I had (almost) worked on. The display would be located in the kitchen in a location that would be visible to family members, but not visible outside of the house. Considerations In our family, we don't have any members that have vision issues other than needing glasses. As such, the primary concern is that we can all ready the text on the display from 2 meters or 6 feet away. What To Display? The primary two goals for the display were to display our calendars and the weather for the next 5 days. Any enhancements of those two goals were fine, as long as the primary goals were not ignored. Some of the other ideas that were floated may seem funny, but they nicely fit into our other category: chore lists from Trello quote of the day number of days until Christmas number of people on the International Space Station current exchange rates Wrapping It Up The rest of the articles in this series will detail my family and I worked on our glanceable display. Based on the information from above, we have had good success with our display. Most of the fixes to the display were tweaks to the information being displayed, rather than the Raspberry Pi itself. I emphatically stand by the previous sections about and making sure you understand and engage your audience. I credit my family, the audience for our glanceable display, with having an honest conversation on what would help, and getting the buy in from them from the beginning. What Was Accomplished? This article presented a set of things to consider when creating a glanceable display, followed by notes on how my family followed that pattern to arrive at our requirements for our glanceable display. Those considerations started with defining your audience, proceeded to understanding your audience, and finally arriving at a set of things that you and your audience want to display. I cannot promise that if you follow these considerations that your journey will be as successful as ours. However, I believe I can say with some certainty that it will help you along the way with your own journey. To succeed, you need information to help guide you, and each of the considerations above will help you inch closer to that success. What's Next? In the next article in this series, Glanceable Displays: Installing Raspbian on a Raspberry Pi , I walk through the steps I took to set up a Raspberry Pi as our glanceable display of choice. It documents the journey from installation on to a blank MicroSD card to a bare bones installation that enabled remote SSH access.","tags":"Technology","url":"https://jackdewinter.github.io/2019/07/14/glanceable-displays-what-are-they/","loc":"https://jackdewinter.github.io/2019/07/14/glanceable-displays-what-are-they/"},{"title":"Starting With GitHub: Setting Up Credentials For My Personal Website","text":"Introduction Part of any project I do, private or open-source, is setting up a version control system and securing access to that version control system. In addition, it is always a high priority for me to make sure that any Git access is secure and follows best common practices on security. To allow me to write articles with little or no changes to my normal workflow, it made logical sense to make my website platform of choice GitHub Pages 3 . As such, in setting up this website I needed to make sure I had Git 1 and more specifically GitHub 2 setup, and setup securely. This article details the actions and choices I made in setting up my access to GitHub for my blog. It details how I followed the GitHub Pages instructions for creating a personal website and creating a personal repository on GitHub to achieve that. Then it describes the two modes of accessing GitHub, SSH and HTTPS, and why I chose SSH. Finally, it provides detailed notes on how I generated a SSH keypair specifically for GitHub, and configured both my local Git and the remote GitHub to use them. Getting Started With GitHub There are many good articles out there on how to install Git for different operating systems, so I feel it is safe to assume that anyone reading this can do the research needed to install Git for their operating system. Setting up access to GitHub is even easier, as the folks at GitHub have provided excellent instructions. Start the setup by simply going to the GitHub Home Page and follow the instructions. The workflow will either allow you to login, if you already have an account, or create a new account, if you don't have an account. Having already dealt with a couple of open source projects, I logged on to my account jackdewinter without any issues. After I logged in, the browser deposited me on my home page. From there I was able to see any projects that I had either contributed to or filed an issue against. Just starting in my Open Source journey, the contents were only a couple of projects that I had filed issues with. Prior to this point, I had no need to authenticate my Git client with GitHub as I was just downloading from public repositories. Having done some research on GitHub Pages, I knew that setting up my own website with GitHub would require me to create my own repositories. As such, my next task was to create that repository. Creating My First GitHub Repository The GitHub Pages home page has a really simple formula on their webpage for setting up personal webpages. The first step is pretty easy: make sure that that the name of the repository is my GitHub user id (in my case jackdewinter ) followed by .github.io . When the creation of my repository finished, GitHub deposited my browser at the base of my new repository: jackdewinter/jackdewinter.github.io . The remaining steps in the formula dealt with cloning the repository, defining a sample index.html file for the website, and pushing that code back to the repository. While I was familiar with those concepts, I wasn't shy about checking back with the Git Basics documentation on the Git website when I forgot something. From there I was able to find the correct helper article on what I need to accomplish within 2-3 clicks. In GitHub, unless you mark a repository as private, everyone can see that repository and read from that repository. As my website's repository is public, reading wasn't a problem. However, pushing the code back to my repository would be writing, and that was a problem. Each GitHub project has a list of who can write to it and the creator of the project is on that list by default. But to write to the project, I needed my local Git tool to login to GitHub when needed and authenticate itself. To do this securely, I was going to have to dive into credentials. GitHub Authentication: SSH vs HTTPS Any time you log in to a website or to a program to gain specific privileges, you establish your credentials by supplying your user id and password. You tell the website or program \"I can prove I am me, let me see my stuff!\". The GitHub website is no different that any of those sites. If you want to be able to see any of your private stuff or write to your stuff, it needs to verify who you are. Going to the Authenticating with GitHub from Git , there are two choices that allow us to connect to GitHub: HTTPS and SSH. Both of these are valid options, allowing for enhanced security when Git connects to GitHub. Each of these options has different things going for and against it. After doing some research, it seemed to me to break down to the following: SSH HTTPS set up keys set up credential manager setup is more involved easy setup more secure less likely blocked by firewall Looking at this information, I decided to go with SSH as I wanted to opt for more security. SSH Access to GitHub During my research at the GitHub site, I found this very good page on SSH over the HTTPS port . In it, they explain that there is a simple test to see if SSH will work from your system to GitHub. When you execute the following command: ssh -T git@github.com it will either return one of two responses. If it returns with: > Hi *username*! You've successfully authenticated, but GitHub does not provide shell access. then you can access GitHub via SSH without any issues. If you see the other response: > ssh: connect to host github.com port 22: Connection timed out then you have to setup SSH to connect to GitHub over the HTTPS port. This access can be verified with a small modification to the above command: ssh -T -p 443 git@ssh.github.com The command is now trying to establish a SSH session over port 443, and if you get the You've successfully... response, it's working fine. Running these tests myself, I found that I got a timeout on the first command and a success on the second command. Following the article, it recommends changes to ~/.ssh/config 4 to include the following: Host github.com Hostname ssh.github.com Port 443 The next time, when I executed the ssh -T git@github.com command, the response was the You've successfully response. Now I was ready to set up the SSH keys. Unique SSH Keys Going back to the Authenticating with GitHub from Git , the next step was to generate a new SSH key pair and add it to the local SSH keyring. The page that points to generating a new key is pretty detailed, so I won't try and improve over GitHub's work. On my reading of the page, it seems to assume that if you will only have 1 key pair 5 generated and that you will reuse that key pair for GitHub. I have issues with that practice, so I want to talk about it. Having a bit of a security background from my day job, I want to limit exposure if something gets broken. Just from a quick search, there are articles by Leo Notenboom , Malware Bytes Labs , and WikiHow that all describe how you should have different passwords for each account, and in many cases, use a password manager. And to be honest, that was just the first 3 that I clicked on. There were a lot more. I can sum up and paraphrase the justification raised in each of those articles by posing a single question: If someone breaks your password on one site, what is your exposure? If you have one password for all sites, then whoever breaks your password has access to that one site. If you have a different password for each site, the damage is limited to one site, instead of all sites using that password. In my mind, using a key pair for credentials is a similar concept to using a user-id and password for credentials. Therefore, it followed that if I follow good security practices for passwords, I should also follow the same practices for key pairs as credentials. Generating a New Key For GitHub To ensure I have secure access to GitHub, I followed the instructions for generating a new key . To generate a distinct key pair for GitHub, I made one small modification to the instructions: I saved the new key pair information with the filename github-key instead of the default id_ras . This resulted in the files ~/.ssh/github-key and ~/.ssh/github-key.pub being created as the key pair. With those files created, I followed the remaining instructions for setting up ssh-agent and uploading the key information to GitHub, replacing any occurrence of id_ras with github-key . With that accomplished, I had a specific key pair specifically for GitHub and it was registered locally. I also had setup GitHub with the public portion of the credentials using the contents of ~/.ssh/github-key.pub , as instructed. The only remaining step was ensure that any SSH connections to GitHub would use the GitHub credentials. Doing a bit more research on the SSH configuration files, I quickly found that there was built in support for this by adding the following to my ~/.ssh/config file: Host github.com User git PreferredAuthentications publickey IdentityFile /c/Users/jackd/.ssh/github-key Note The location of ~/ on my Windows machine is %HOMEDRIVE%%HOMEPATH%\\ or c:\\Users\\jackd\\ . The format for the IdentityFile property is a standard Unix path format. This requires a translation from the Windows path format C:\\Users\\jackd\\.ssh\\github-key to the Unix path format of /c/Users/jackd/.ssh/github-key . Combined with the change from earlier in this article, my ~/.ssh/config file now looked like: Host github.com Hostname ssh.github.com Port 443 User git PreferredAuthentications publickey IdentityFile /c/Users/jackd/.ssh/github-key Testing Against GitHub: GitHub Pages Having performed a number of thorough tests of the above steps, everything passed without any issues! Now it was time to try and push some commits for the blog to GitHub. To create a directory for the GitHub project, I largely followed these instructions detailed in the companion article on setting up your own static website. I then followed these instructions , adding the remote repository to my local configuration with the following line: git remote add origin jackdewinter/jackdewinter.github.io Having associated the directory with the remote repository, the final test was to make a change to the directory, commit it, and push it to the remote. For this test, I used a very simple index.html file: < html > < body > Hello world! </ body > </ html > Adding the file to the directory, I staged the file and committed it with: git add index.html git commit -m \"new files\" and then pushed it to the remote repository with: ssh-agent git push origin master --force Crossing my fingers, I waited until I got a response similar to: Counting objects: XXX, done. Delta compression using up to 8 threads. Compressing objects: 100% (XX/XX), done. Writing objects: 100% (XX/XX), XXX.XX KiB | 0 bytes/s, done. Total XX (delta XX), reused XX (delta XX) remote: Resolving deltas: 100% (XX/XX), completed with XX local objects. To blog:jackdewinter/jackdewinter.github.io.git XXXXXX..XXXXXX master -> master With no error messages, I double checked the repository at https://github.com/jackdewinter/jackdewinter.github.io and was able to see the index.html file present in the repository. Following through with the instructions for GitHub Pages, I then went to https://jackdewinter.github.io and was able to see the text \"Hello world!\" on in the browser. What Was Accomplished This article started with my creation of a GitHub repository to contain the files for my personal website using GitHub Pages. To securely access the repository, I chose the SSH protocol and discovered that I needed to employ SSH over HTTP. For enhanced security, I described a solid reason for wanting a unique SSH key for GitHub. Following that advice, I generated a new key and then changed the ~/.ssh/config file to use SSH over HTTPS and to point to that newly generated keypair. Finally, I committed a sample file to the project and was able to see it pushed successfully to the remote repository, and displayed as my personal home page. Git is an open-source source control tool. For more information, look here . ↩ GitHub is a common repository for open-source projects. For more information, look here . ↩ GitHub Pages are a feature of GitHub that allow people to host their personal websites on GitHub. For more information, look here . ↩ My primary system is a Windows 10 machine, so instead of modifying the ~/.ssh/config file, I modified the %HOMEDRIVE%%HOMEPATH%\\.ssh\\config file. On my system, that file is the c:\\Users\\jackd\\.ssh\\config file. ↩ When a SSH key is generated, it comes in two parts. The private part is kept on the user's system while the public part can be distributed to any interested parties. Together they are referred to as a key pair. ↩","tags":"Technology","url":"https://jackdewinter.github.io/2019/07/07/starting-with-github-setting-up-credentials-for-my-personal-website/","loc":"https://jackdewinter.github.io/2019/07/07/starting-with-github-setting-up-credentials-for-my-personal-website/"},{"title":"Extended Markdown Examples","text":"This is a continuation of the previous cheat sheet for my website. This article specifically addresses any extensions that are not part of the base Markdown specification. Each section here represents an extension that I have enabled on my website. The formatting from the previous page is continued, with one small exception. The title of each section specifies the name of the extension instead of the name of the feature being documented (see Admonitions ). If an extension contains more than one feature, such as the Extra extension, the title specifies the name of the extension, a dash, and the name of the feature (see Footnotes ). Introduction The authors of the Python Markdown Package anticipated the addition of extra features. To ensure people would have choice, the base package can be extended using configuration . The Markdown extensions have been activated on my website by inserting the following text into my peliconconf.py: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 MARKDOWN = { 'extension_configs' : { 'markdown.extensions.extra' : {}, 'markdown.extensions.admonition' : {}, 'markdown.extensions.codehilite' : { 'css_class' : 'highlight' }, 'markdown.extensions.meta' : {}, 'smarty' : { 'smart_angled_quotes' : 'true' }, 'markdown.extensions.toc' : { 'permalink' : 'true' }, } } Table Of Contents [TOC] Introduction Table Of Contents CodeHilite - Code Blocks With Line Numbers Extra - Footnotes Extra - Abbreviations Extra - Definition Lists Smartypants Admonitions CodeHilite - Code Blocks With Line Numbers ``` #!python # Code goes here ... ``` 1 # Code goes here ... Extra - Footnotes Here's a simple footnote,[&#94;1] and here's a longer one.[&#94;bignote] [&#94;1]: This is the first footnote. [&#94;bignote]: Here's one with multiple paragraphs and code. Here's a simple footnote, 1 and here's a longer one. 2 Extra - Abbreviations The HTML specification is maintained by the W3C. *[HTML]: Hyper Text Markup Language *[W3C]: World Wide Web Consortium The HTML specification is maintained by the W3C . Extra - Definition Lists Apple : Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange : The fruit of an evergreen tree of the genus Citrus. Apple Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange The fruit of an evergreen tree of the genus Citrus. Smartypants advantage is that code blocks are unaffected - apostrophe ' by itself - apostrophe as in 'quote me' - quotations mark \" by itself - quotations mark as in \"quote me\" - replacement of multi-character sequences with Unicode: << ... -- >> --- apostrophe ‘ by itself apostrophe as in ‘quote me' quotations mark \" by itself quotations mark as in \"quote me\" replacement of multi-character sequences with Unicode: « … – » — Admonitions broken down into section by the way that the Elegant theme colors the admonitions !!! note You should note that the title will be automatically capitalized. Note You should note that the title will be automatically capitalized. !!! important \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! hint You should note that the title will be automatically capitalized. Hint You should note that the title will be automatically capitalized. !!! tip \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! warning You should note that the title will be automatically capitalized. Warning You should note that the title will be automatically capitalized. !!! caution \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. !!! attention \"\" You should note that this will have no title due to the empty title. You should note that this will have no title due to the empty title. !!! danger You should note that the title will be automatically capitalized. Danger You should note that the title will be automatically capitalized. !!! error \"Replacement Title\" You should note that the default title will be replaced. Replacement Title You should note that the default title will be replaced. This is the first footnote. ↩ Here's one with multiple paragraphs and code. ↩","tags":"Markdown","url":"https://jackdewinter.github.io/2019/06/30/extended-markdown-examples/","loc":"https://jackdewinter.github.io/2019/06/30/extended-markdown-examples/"},{"title":"Standard Markdown Examples","text":"As I started writing my articles for my blog, I realized I needed something. To help me write articles using this flavor of Markdown 1 , I needed my own cheat sheet. My hope is that it provides clear guidance on which aspects of the various forms of Markdown worked for me, and which didn't. Introduction Horizontal Break Headings Text Emphasis Numbered lists Bulleted List Block quote Code Block Tables Links Local Links Remote Links Download Links ) Images Introduction I am writing articles and pages on Pelican 4.0.1 2 using the Elegant 3 theme, therefore I want to make sure I have a cheat sheet that is specific to this dialect of Markdown. The base Markdown used for Pelican uses the Python Markdown Package which (with 3 exceptions) follows John Gruber's Markdown definition very literally. Pelican configuration also supports providing Markdown with additional configuration that enables other features. Those features are documented separately in the next page . The format of this cheat sheet is simple. Each section is separated from the next with a horizontal break and the name of the section. Any notes regarding that section are placed at the top of the section in point form, to ensure they are brief. Then a Code Block section is used to show the literal code used to produce the effects that are presented right after the code block. Horizontal Break A horizontal break occurs after 3 or more hyphens. --- A horizontal break occurs after 3 or more hyphens. Headings # Heading Level 1 ## Heading Level 2 ### Heading Level 3 Heading Level 1 Heading Level 2 Heading Level 3 Text Emphasis two spaces at the end of a line will be equivalent to <br/> This text is **bold** and this text is also __bold__. This text is *italic* and this text is also _italic_. This text is **_italic and bold_**, but no two spaces at end. Single ```line``` block. Inline `code` has ```back-ticks like this ` around``` it. This text is bold and this text is also bold . This text is italic and this text is also italic . This text is italic and bold , but no two spaces at end. Single line block. Inline code has back-ticks like this ` around it. Numbered lists to maintain the indentation, place 4 spaces at the start of the line 1. One New para. Blah 2. Two - unordered - list 3. Three 1. ordered 2. list - unordered - list 3. items One New para. Blah Two unordered list Three ordered list unordered list items Bulleted List to maintain the indentation, place 4 spaces at the start of the line - This is a list item with two paragraphs. This is the second paragraph in the list item. You're only required to indent the first line. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. - Another item in the same list. - Bulleted item - Bulleted item This is a list item with two paragraphs. This is the second paragraph in the list item. You're only required to indent the first line. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Another item in the same list. Bulleted item Bulleted item Block quote > This is the first paragraph of a blockquote with two paragraphs. > Lorem ipsum dolor sit amet, > consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. > Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. > > This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. > > This is the first level of quoting. > > > This is nested blockquote. > > Back to the first level. This is the first paragraph of a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. This is the first level of quoting. This is nested blockquote. Back to the first level. Code Block line numbers can be added via extensions ```text Make things only as complex as they need to be. ``` ```Python # Blogroll LINKS = ( ('Pelican', 'Pelican', 'http://getpelican.com/'), ) ``` Make things only as complex as they need to be. # Blogroll LINKS = ( ( 'Pelican' , 'Pelican' , 'http://getpelican.com/' ), ) Tables colons can be used to align columns. | Column1 | Column 2 | Column 3 |---|---|---| | Value 1 | Value 2 | Value 3 | | Value 4 | Value 5 | Value 6 | | Value 7 | Value 8 | Value 9 | | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | Column1 Column 2 Column 3 Value 1 Value 2 Value 3 Value 4 Value 5 Value 6 Value 7 Value 8 Value 9 Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 Links Local Links {filename} tag indicates location in the content folder. [About Page]({filename}/pages/about.md) About Page Remote Links proper URL indicates a remote website [Python Package Index](https://pypi.org) Python Package Index Download Links download links are not natively supported in Markdown must explicitly create HTML text inline to achieve that Creating a link to a file to download, not display, is not natively supported in markdown. [Pelican Brag Document (display)]({filename}/images/markdown-1/pelican.txt) <a href=\"{filename}/images/pelican.txt\" download>Pelican Brag Document (download)</a> Pelican Brag Document (display) Pelican Brag Document (download) Images {filename} tag indicates location in the content folder. ![python logo]({filename}/images/markdown-1/python_icon.png) Markdown allows for HTML pages to be written using a simple text editor with no knowledge of HTML. ↩ Pelican is a Static Site Generator written in Python. ↩ The Elegant theme's repository is here . ↩","tags":"Markdown","url":"https://jackdewinter.github.io/2019/06/29/standard-markdown-examples/","loc":"https://jackdewinter.github.io/2019/06/29/standard-markdown-examples/"},{"title":"My Gallery Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada.","tags":"Python, Fred","url":"https://jackdewinter.github.io/2010/12/06/my-gallery-test/","loc":"https://jackdewinter.github.io/2010/12/06/my-gallery-test/"},{"title":"My Long Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada. Para 1 Donec quam neque, vulputate quis purus at, tempus tincidunt neque. Sed posuere eros eu massa lobortis varius. Ut condimentum elit eros. Sed vel nunc vitae nibh aliquet vestibulum vitae quis justo. Sed vel ligula turpis. Aliquam et mi mollis, suscipit sapien vel, molestie enim. Morbi sodales, dui nec congue tristique, risus mi luctus nulla, vel egestas sem nulla quis augue. Nulla vitae efficitur odio, quis egestas ex. Pellentesque a est viverra, fringilla dui ac, laoreet purus. Suspendisse porta aliquet nunc et pulvinar. Integer ante felis, tincidunt eu ipsum a, imperdiet convallis augue. Cras vulputate sapien sit amet metus placerat, sed congue turpis tempus. Nunc pretium ac dolor eget tincidunt. Para 2 Nunc id tortor lectus. Quisque fermentum sem ut elit ultricies sollicitudin. Curabitur blandit, elit at suscipit mattis, purus lectus eleifend felis, id rutrum neque sapien vitae arcu. Aenean elementum lacus tristique purus facilisis placerat. Nunc pharetra lorem ut finibus blandit. Aenean scelerisque elit nec malesuada accumsan. Proin eu orci eget odio scelerisque viverra a ac nulla. Vestibulum elementum lobortis quam. Morbi porta rutrum mi, quis laoreet nunc dictum at. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Ut in lobortis massa. Para 2a Phasellus et leo in nunc fermentum vulputate. Nullam sed interdum augue. Duis eu dignissim eros. Mauris pretium turpis non purus porta, non consequat enim rutrum. Fusce dui odio, consequat in rhoncus sed, interdum vulputate quam. Nullam nec dolor ex. Curabitur dapibus vestibulum odio at sodales. Para 2b Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum. Etiam dignissim lorem at turpis suscipit pharetra. Ut velit velit, pellentesque sit amet ex sed, efficitur laoreet enim. Etiam pharetra neque quam, in elementum massa molestie vel. Duis nec venenatis nisi. Etiam condimentum, leo vel ultrices placerat, dolor nibh pretium tortor, vel egestas odio nulla ut lectus. Etiam ultrices nulla quis felis tincidunt, quis sollicitudin elit lacinia. Suspendisse porttitor nulla rhoncus est blandit, eu faucibus lacus vestibulum.","tags":"Python, Fred","url":"https://jackdewinter.github.io/2010/12/03/my-super-long-post/","loc":"https://jackdewinter.github.io/2010/12/03/my-super-long-post/"},{"title":"My Short Article","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer malesuada sed tortor et pulvinar. Donec a vehicula ligula. Quisque porta erat vitae lectus lacinia varius. Integer sed lacus in libero volutpat lobortis ac vitae velit. Praesent rutrum turpis sem, id mattis sem pulvinar id. Morbi leo felis, facilisis in ex a, viverra placerat justo. Donec ac risus non sapien feugiat malesuada.","tags":"Python, Fred","url":"https://jackdewinter.github.io/2010/12/03/my-super-short-post/","loc":"https://jackdewinter.github.io/2010/12/03/my-super-short-post/"}]};