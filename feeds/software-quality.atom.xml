<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jack's Digital Workbench - Software Quality</title><link href="https://jackdewinter.github.io/" rel="alternate"></link><link href="https://jackdewinter.github.io/feeds/software-quality.atom.xml" rel="self"></link><id>https://jackdewinter.github.io/</id><updated>2019-11-10T00:00:00-08:00</updated><entry><title>Software Quality: Reliability</title><link href="https://jackdewinter.github.io/2019/11/10/software-quality-reliability/" rel="alternate"></link><published>2019-11-10T00:00:00-08:00</published><updated>2019-11-10T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2019-11-10:/2019/11/10/software-quality-reliability/</id><summary type="html">
&lt;p&gt;In the main article titled &lt;a href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"&gt;What is Software Quality?&lt;/a&gt;, I
took a high level look at what I believe are the 4 pillars of software quality.  This article
will focus specifically on the Reliability pillar, with suggestions on how to measure
Reliability and how to write good requirements for this …&lt;/p&gt;</summary><content type="html">
&lt;p&gt;In the main article titled &lt;a href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"&gt;What is Software Quality?&lt;/a&gt;, I
took a high level look at what I believe are the 4 pillars of software quality.  This article
will focus specifically on the Reliability pillar, with suggestions on how to measure
Reliability and how to write good requirements for this pillar.&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;From the main article on
&lt;a href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"&gt;What is Software Quality?&lt;/a&gt;,
the essence of this pillar can be broken down into two questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the software do the task that it is supposed to do?&lt;/li&gt;
&lt;li&gt;Does the software execute that task in a consistent manner?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This article will take an in-depth look at common types of tests, discussing how those
tests can help us gather the information necessary to answer those questions.  At
the end of this article, the section
&lt;a href="https://jackdewinter.github.io/2019/11/10/software-quality-reliability/#how-to-measure-reliability"&gt;How To Measure Reliability&lt;/a&gt; will use that information to provide a cohesive answer
to those questions.&lt;/p&gt;
&lt;h2 id="how-does-testing-help-measure-reliability"&gt;How Does Testing Help Measure Reliability?&lt;a class="headerlink" href="#how-does-testing-help-measure-reliability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As discussed in the main article’s section on
&lt;a href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/#Reliability"&gt;Reliability&lt;/a&gt;,
many articles on testing and reliability refer to a test pyramid that defines the 4
basic types of reliability tests: unit tests, functional/integration tests, scenario
tests, and end-to-end tests.  While those articles often have slightly different takes
on what the pyramid represents, a general reading of most of those articles leaves me
with the opinion that each test in each section of the pyramid must pass every time.
With tests and reliability being closely related, it is easy for me to draw the
conclusion that if tests must pass every time, then reliability is a binary choice:
they all pass and the project is reliable, or one or more fail and the project is not
reliable.&lt;/p&gt;
&lt;p&gt;As such, my main question is: Does it have to be a
binary choice?  Are the only two choices that either all tests did pass or all tests did
not pass? If the answer to that question is a binary answer, then the answer is simple:
it is either 100% reliable or 0% reliable.  More likely, there are other answers that
will give use a better understanding of how to measure reliability and how to interpret
those measurements.&lt;/p&gt;
&lt;h2 id="can-we-identify-groups-of-tests"&gt;Can We Identify Groups of Tests?&lt;a class="headerlink" href="#can-we-identify-groups-of-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Before determining whether or not reliability is a binary choice, I feel that it is important
to make some foundational decisions on how to measure reliability based on the types of tests
that are already identified.  To aid in making those decisions, it helps to examine the four
categories of tests, looking for groupings between them.&lt;/p&gt;
&lt;p&gt;&lt;img alt="test pyramid" src="https://jackdewinter.github.io/images/quality-1/test-pyramid.png"/&gt;&lt;/p&gt;
&lt;p&gt;Using the definitions established in the main article, unit tests are used to test
the reliability of individual software components and functional tests are used to test the
reliability of more than one of those components working together.  Both of these categories
are used to determine the reliability of the components themselves, and not their objectives.
As such, they make for a good grouping as they have a common responsibility: technical
reliability.&lt;/p&gt;
&lt;p&gt;Observing the scenario tests and end-to-end tests through a similar lens, those tests are used to
determine whether or not the software project meets its business requirements.  The end-to-end tests are often a set of
tests that are very narrow and deep of purpose.   At a slightly lower level, the scenario
tests provide extra support to those end-to-end tests by breaking those “bulky” end-to-end
tests into more discrete actions matched to the overall business use cases for the project.
A good grouping for these tests is by what they: business reliability.&lt;/p&gt;
&lt;p&gt;Another way to think about it is to view the groups of tests in terms of whether or not they
are inside or outside of the
&lt;a href="https://www.techopedia.com/definition/3552/black-box-testing"&gt;black box&lt;/a&gt;
that is the software project.  The first group of tests verify the inside of that black box,
ensuring that all of the technical requirements or “what needs to be done to meet
expectations” are met.  The second group of tests verify the outside of that black box,
ensuring that all of the business requirements or “what is expected of the project” are met.&lt;/p&gt;
&lt;p&gt;[Add picture of pyramid showing inside and outside?]&lt;/p&gt;
&lt;h2 id="give-me-an-example"&gt;Give Me an Example&lt;a class="headerlink" href="#give-me-an-example" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For the follow sections, I use the example of a simple project that uses a data store to
keep track of contact information. By providing a simple example that most developers have
encountered before, my hope is that it will make it easier for the reader to picture the
different types of tests and how they will interact with their team’s project.  As I
examine each type of tests, I try and explain my thinking on what I write and how
I write it for that group of tests, hoping to guide others on making better decisions
for their testing strategy.&lt;/p&gt;
&lt;p&gt;Note that I do not believe that the definition of “data store” is relevant to the example,
therefore the definition of “data store” is left up to the reader’s imagination and
experience.&lt;/p&gt;
&lt;h3 id="end-to-end-tests"&gt;End-To-End Tests&lt;a class="headerlink" href="#end-to-end-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Starting at the top of test pyramid, each end-to-end test needs to be a solid,
representative test of the main focus of the project itself.  These tests are usually a
small set of tests meant as a solid litmus test on whether the software project is
reliably meeting the requirements of the project.  In forming the initial end-to-end
tests, my intent is to start with a focus on positive cases which occur more than
60% of the time.&lt;/p&gt;
&lt;p&gt;For the example project, I started with a test to successfully add a new contact. As a
nice bonus, starting with that test allowed me to add the remove, list, and update
end-to-end tests, as they all need to add a new contact as a foundation of each of those
3 individual tests. Given my experience measuring quality, I believe that all of those
tests together provide that check with confidence for the example project.  If I had
found out
that the number of end-to-end tests I needed was more than a handful of tests, I would
have then examined the requirements and try to determine if the project had too many
responsibilities.  Doing this exercise with a new project often helps me figure out if
the project is properly scoped and designed, or if it requires further refinement.&lt;/p&gt;
&lt;p&gt;Having identified the end-to-end tests for a project and assuming that no further
refinement is necessary, I rarely write source code for these tests right away.  Most
of the time I just add some simple documentation to the project outlined in
&lt;a href="https://en.wikipedia.org/wiki/Pseudocode"&gt;pseudocode&lt;/a&gt; to capture that information.  I
find that the main benefit of doing this in the early stages is to provide a
well-defined high level goal that myself and my team can work towards. Even having rough
notes on what the test will eventually look like can help the team work towards that
goal of a properly reliable project.&lt;/p&gt;
&lt;h3 id="scenario-tests"&gt;Scenario Tests&lt;a class="headerlink" href="#scenario-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Still on the outside of the box, I then add a number of scenario tests to expand on the
scope of each of end-to-end tests.  For these tests, I focus on
&lt;a href="https://en.wikipedia.org/wiki/Use_case"&gt;use cases&lt;/a&gt;
that the user of the project will experience in typical scenarios. The intent here is to
identify the scenario tests that collectively satisfy 90% or more of the projected
business use cases for a given slice of the project.&lt;/p&gt;
&lt;p&gt;For the example project, adding a test to verify that I can successfully add a contact
was the first scenario test that I added.  I then added a scenario for the negative use
case of adding a contact and being told there are invalid fields in my request and a
third for a contact name that already existed.  Together, these scenarios met my bar for
the “add a contact” slice of the scenarios for the project.&lt;/p&gt;
&lt;p&gt;It is important to remember that these are tests that are facing the user and systems
they interact with. Unless there is a very strong reason to, I try and avoid scenario
tests that depend on any specific state of the project unless the test explicitly sets
that state up.  From my experience, such a dependency on external setup of state is very
fragile and hard to maintain.  It also raises the question on whether or not it is a
realistic or valuable test if that setup is not something that the project itself sets
up.&lt;/p&gt;
&lt;h4 id="why-only-those-3-scenario-tests"&gt;Why only those 3 scenario tests?&lt;a class="headerlink" href="#why-only-those-3-scenario-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Here is a simple table on what types of scenario tests to add that I quickly put
together for that project.  The estimates are just that, examples,  but helped me
determine if I hit the 90% mark I was aiming for.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Percentage&lt;/th&gt;
&lt;th&gt;Scenario&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Success&lt;/td&gt;
&lt;td&gt;60%&lt;/td&gt;
&lt;td&gt;Add a contact successfully to the project.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bad/Invalid Data&lt;/td&gt;
&lt;td&gt;25%&lt;/td&gt;
&lt;td&gt;Add an invalid contact name and validate that a ValidateError response is returned.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Processing Error&lt;/td&gt;
&lt;td&gt;10%&lt;/td&gt;
&lt;td&gt;Add an contact name for an already existing contact and validate that a ProcessingError response is returned.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I sincerely believe that between those 3 scenario tests, I can easily defend that they
represent 90%+ of the expected usage of the project for the specific task of adding a
contact. While the percentages in the table are
&lt;a href="https://en.wikipedia.org/wiki/Scientific_wild-ass_guess"&gt;swags&lt;/a&gt;
that seem to be “plucked out of thing air”, I believe they can be reasonably
defended&lt;sup id="fnref:defense"&gt;&lt;a class="footnote-ref" href="#fn:defense"&gt;1&lt;/a&gt;&lt;/sup&gt;.  This defense only needs to be reasonable enough to get the project
going. Once the project is going, real data can be obtained by monitoring and more
data-driven percentages can be used, if desired.&lt;/p&gt;
&lt;h4 id="how-did-i-get-there"&gt;How did I get there?&lt;a class="headerlink" href="#how-did-i-get-there" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;From experience, there are typically 4 groups of action results, and therefore,
scenarios: the action succeeded, the action failed due to bad data, the action failed
due to a processing error, or the action failed due to a system error.&lt;/p&gt;
&lt;p&gt;The first scenario test represents the first category.  Unless there was a good reason to
show another successful “add” use case, I will typically stick with a single “add” test.
As the goal is to achieve 90% of the typical use cases for the project, unless a
variant of that success case is justified by it’s potential contribution towards the 90%
total, it can be better performed by other tests.  In addition, tests on variations of
the input data are better performed by unit tests and functional tests, where executing
those tests have lower setup costs and lower execution costs.&lt;/p&gt;
&lt;p&gt;The second scenario test is used to satisfy the second group of tests where the data is
found to be bad or invalid. In general, I use these to test that there is consistent
error handling &lt;sup id="fnref:errorHandling"&gt;&lt;a class="footnote-ref" href="#fn:errorHandling"&gt;2&lt;/a&gt;&lt;/sup&gt; on the boundary between the user and the project.  At
this level, I ideally need only one or two tests to verify that any reporting of bad or
invalid data is being done consistently. By leaving the bulk of the invalid testing to
unit testing and/or functional testing, I can simulate many error conditions and check
them for consistent output at a low execution cost.  To be clear, if possible I try and
verify the general ability that consistent error handling is in place and not that a
specific instance of error is being reported properly.&lt;/p&gt;
&lt;p&gt;The third scenario test is used to verify the third group of tests where data is valid
but fails during processing.  Similar to the second group of tests, there is an
assumption that the reporting of processing errors should be done consistently.  However,
as most processing errors result due to a sequence of actions originating from the user,
representative types of processing errors should be tested individually.  The key to this
type of scenario tests is to represent processing errors that will help the group of
scenario tests hit that 90% mark.  Relating this to the example project, getting a
“already add a record with that name” response from the project is something that would
occur with enough frequency to qualify in my books.&lt;/p&gt;
&lt;p&gt;From experience, the fourth group of tests, testing for system errors, rarely makes it
to the level of a scenario test.  In this example, unless a system error is so
consistent that it was estimated to occur more than 10% of the time, a higher priority
is placed on the other types of responses.&lt;/p&gt;
&lt;p&gt;One of the exceptions to these generic rules are when a business requirement exists to
provide extra focus on a given portion of the interface.  These requirements are often
added to a project based on a past event, either in the project or in a related project.
As the business owners have taken the time to add the business requirement due to its
perceived priority, it should have a scenario test to verify that requirement is met.&lt;/p&gt;
&lt;p&gt;In the contact manager example, I made a big assumption that unless there were
requirements that stated otherwise, the data store is local and easy to reach.  If
instead we are talking about a project where the data is being collected on a mobile
device and relayed to a server, then a test in this last group of system errors would
increase in value.  The difference that this context introduces is that it is expected
that project will fail to reach the data store on a frequent basis, and hence, providing
a scenario for that happening helps us reach that 90% goal.&lt;/p&gt;
&lt;h3 id="commonalities-between-end-to-end-tests-and-scenario-tests"&gt;Commonalities between End-to-end tests and scenario tests&lt;a class="headerlink" href="#commonalities-between-end-to-end-tests-and-scenario-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;While I took the long way around describing end-to-end tests and scenario tests, I
believe the journey was worth it.  These two types of tests test against the external
surface of the project, together painting a solid picture of what that external surface
will look like once the project is done.  For both of those tests, the project needs
clear business requirements on what benefit it provides to the user, which will be
highlighted by translating the requirements into the various tests.  By including either
actual data (for existing projects) or projected data (for new projects) on the usage
patterns for that project, the requirements can be prioritized to ensure the most
frequently used requirements are more fully tested.&lt;/p&gt;
&lt;p&gt;For each of those requirements and goals, the team can then set goals for the project
based on those documented requirements.  By codifying those goals and requirements with
end-to-end and scenario tests, you firm up those goals into something concrete.  Those
actions allow the team to present a set of tests or test outlines to the authors of the
requirements, validating that things are going in the right direction before writing too
much source code or setting up of interfaces with the user.  That communication and
changing the course before writing code can save a team hours, days, or weeks,
depending on any course changes discovered.&lt;/p&gt;
&lt;p&gt;What happens if the requirements change?  The project has a set of tests that
explicitly test against the outside of the box, and informs the team on what changes
will be needed if that requirement change is applied to the project.  At the very least,
it starts a conversation with the author of the requirement about what the external
surface of the project will look like before and after the change.  With that
conversation started, the team can have a good understanding of how things will change,
with some level of confidence that the change is the change specified by the
requirements author.&lt;/p&gt;
&lt;h3 id="unit-tests-and-functional-tests"&gt;Unit Tests and Functional Tests&lt;a class="headerlink" href="#unit-tests-and-functional-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Transitioning to inside of the black box, unit tests and functional tests are more
understood by developers and more frequently used than end-to-end tests or scenario
tests. The unit tests isolate a single component (usually a class) and attempt to test
that each interface of that component and is functioning properly.  The functional tests
do the same thing, but with a single group of components that work together as a single
component rather than a single component itself.&lt;/p&gt;
&lt;p&gt;From an implementation point of view, the main difference is in how these tests are
created. Unit tests, as they are testing a single component, should only contain a
project reference to the one component being tested.  If the components are created
properly and have a good separation from the rest of the project, this should be
achievable for a good number of
components for the project, especially the support components.  Therefore, the degree to
which these tests are successful is determined by the amount of clean division of
responsibilities the project has between it’s components.&lt;/p&gt;
&lt;p&gt;Functional tests complete the rest of the inside-of-the-box testing by testing individual
components with related components, in the way they are used in a production
environment.  With these tests, the degree to which these tests are successful is the
ability to inject the project dependencies into one or more of the components being
tested, coupled with the clean division of responsibilities needed for good unit tests.
While using a concept such as the interface concept from Java and C# is not required, it
does allow the injection of dependencies to be performed cleanly and with purpose.&lt;/p&gt;
&lt;p&gt;To enable groups of functional tests to be as independent of the components outside of
their group as possible, &lt;a href="https://en.wikipedia.org/wiki/Mock_object"&gt;mock objects&lt;/a&gt; are
often used to replace concrete classes that are part of your project.  If interfaces are
used in your project to allow for better
&lt;a href="https://en.wikipedia.org/wiki/Dependency_injection"&gt;dependency injection&lt;/a&gt;,
your functional tests can create mock objects that reside with your tests.  This provides
more control and reliability on what changes you are making from the live instance of
the interfaces, for the sake of testing.  If interfaces are not supplied for better
dependency injection, a mocking library such as the Java
&lt;a href="https://site.mockito.org/"&gt;Mockito&lt;/a&gt;
are required to replace test dependencies with reliable objects.&lt;/p&gt;
&lt;h4 id="back-to-our-example"&gt;Back to our example&lt;a class="headerlink" href="#back-to-our-example" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Using the example project as a template, we know from the section on
&lt;a href="https://jackdewinter.github.io/2019/11/10/software-quality-reliability/#Scenario-Tests"&gt;scenario tests&lt;/a&gt;
that we need to test for valid inputs when adding a new contact.  To add
coverage for the component containing the “add a contact” logic as a unit test, it’s
success is determined by how much of the handling the external interface is in the one
component. If that component contains all of the code needed to handle that external
request in one method, it is extremely hard to test that component without bringing in
the other components.  That is definition of a functional test, not a unit test.  As an
alternative, if the validation of the input can be condensed into it’s own component and
removed from that method, that validation component can be unit tested very effectively.&lt;/p&gt;
&lt;p&gt;Applying that refactoring pattern a couple of more times in the right ways, the project’s
ability to be functionally tested increases.  As an added bonus,  depending on how the
refactoring is accomplished, new unit tests can be added based on the refactoring,
gaining measurable confidence on each additional component tested.  &lt;/p&gt;
&lt;p&gt;Using the adding a contact example again, having refactored the input validation to a
validation class could be followed by the following changes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create a new component for the handling of “add a contact” and decouple it from logic of the handling of the external interface&lt;/li&gt;
&lt;li&gt;move the user authentication and authorization logic into it’s own component&lt;/li&gt;
&lt;li&gt;move the persisting of the new contact logic into it’s own component&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From a functional test point of view, each of these refactorings makes it easier to test.
For the first refactoring, instead of having to rely on all functional testing going
through the external interface, which may include costly setup, we can create a local
instance of the new component and test against that.  If interfaces are used for the
remaining two refactorings, then test objects can be used instead of the “live” objects,
otherwise a mocking library can be used to replace those objects with more predictable
objects.&lt;/p&gt;
&lt;h2 id="how-is-each-group-of-tests-measured"&gt;How is each group of Tests Measured?&lt;a class="headerlink" href="#how-is-each-group-of-tests-measured" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;On this winding journey to determine how to measure reliability, I explored the relevant
elements of the four main types of tests.  I believe that I was successful in showing a
clear delineation between the two groups of tests and the benefits each group provides.
To recap, the outside-of-the-box group validates the expectations to be met, matched
against the requirements set out for the project.  The inside-of-the-box group validates
how those exceptions are met, matched against the external interfaces for the project.&lt;/p&gt;
&lt;p&gt;These two distinct foundations are important, as the two distinct groups of tests require
two distinct groups of measurements.&lt;/p&gt;
&lt;p&gt;The first group, scenario tests and end-to-end tests, are measured by scenario coverage.
Scenario coverage measures the number of tests that successfully pass against the total
number of scenario tests and end-to-end tests for that project.  As this group of tests
is measuring the business expectations of the project, this measurement is a simple
fraction: the number of passing tests as the numerator and the number of defined tests
as the denominator.&lt;/p&gt;
&lt;p&gt;The second group, unit tests and functional tests, are measured by source code coverage,
or code coverage for short.  Code coverage can be specified along 6 different axes:
class, method, line, complexity, blocks, and lines.  Different measurement tools will
provide different subsets of those measurements, but in the end they are all relaying
the same thing: the points in the project’s source code that are not properly tested.&lt;/p&gt;
&lt;h2 id="back-to-the-original-question"&gt;Back to the original question&lt;a class="headerlink" href="#back-to-the-original-question" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Does it (the measuring of reliability) have to be a binary choice?&lt;/p&gt;
&lt;p&gt;It depends.&lt;/p&gt;
&lt;p&gt;In an ideal world, the answer to that question is yes, but we do not live in an ideal
world.  In the real world, we have a decision to make for either group of tests on what
is good enough for the project and that group of tests.&lt;/p&gt;
&lt;p&gt;If the suggestions of this article are followed, then a condition of releasing the
project to a production state is 100% scenario coverage.  Anything less than 100% means
that critical use cases for the project are not complete, hence the project itself is not
complete.  &lt;/p&gt;
&lt;p&gt;To achieve the 100% coverage without adding new project code, updated requirements are
needed from the requirements author, say a project manager, to change the composition of
the scenario tests and end-to-end tests.  This may include removing some of these
tests as the release goals for the project are changed.  While changing and removing
goals and their tests, may seem like cheating to some people, the other option is
very risky.&lt;/p&gt;
&lt;p&gt;It should be evident that if a project is released without all scenario tests and
end-to-end tests passing, that team is taking a gamble with their reputation and the
reputation of the project.  It is better to adjust the tests and goals, and communicate
those changes, than to take a risk on releasing something before it meets those goals.&lt;/p&gt;
&lt;p&gt;Following the suggestions of this article for code coverage is a more nuanced goal, and
really does depend on the project and the situation. If architected and designed to
support proper testing from the beginning, I would argue that 95%+ code coverage is easy
and desirable.  If you are adding testing to an already existing project or do not have
the full support of the developers on the project, this number is going to be lower.&lt;/p&gt;
&lt;p&gt;Another factor is the type of project that is being tested and who will use it.  If you
are creating this project to support people inside of your company, it is possible that
one of the requirements is to have a lower initial code coverage target to allow the
project to be used right away and alleviate some internal company pressure.  If the
project is something that will represent you and your company on the international stage,
you will have to balance the time and effort needed to meet a higher bar for code
coverage with the need to get the project out where it can be used.  As with many things,
it is a matter of negotiation and balance between the various requirements.&lt;/p&gt;
&lt;h2 id="what-is-really-important"&gt;What Is Really Important&lt;a class="headerlink" href="#what-is-really-important" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I want to stress that I believe that the important thing is that each project measures
where they are against whatever goals they set for their project. The team doesn’t need
to always maintain a near-100% code coverage measure, but that team needs to know where
they stand.  This will influence and inform the people that author the requirements and
adjust the priorities for the team.  Any negotiations within the team can then cite this
information and use it to help with the balancing act of adding new features, fixing
existing bugs, and enhancing code quality (in this case, increasing code coverage).&lt;/p&gt;
&lt;h2 id="how-to-measure-reliability"&gt;How To Measure Reliability&lt;a class="headerlink" href="#how-to-measure-reliability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To answer the question “Does the software do the task that it is supposed to do?”,
scenario coverage is measured.  Scenario coverage for end-to-end tests and scenario
tests should always be at 100% when a production release of the project is performed.
This measurement is binary.  Until that release (or next production release) is
performed, adding or changing these tests based on the requirements for the next release
will inform the team and any stakeholders of how close the team is to satisfying those
requirements for that release.&lt;/p&gt;
&lt;p&gt;To answer the question “Does the software execute that task in a consistent manner?”,
code coverage is measured.  Code coverage for unit tests and functional tests should
strive for 95% code coverage along all 6 axes with all active tests completing
successfully 100% of the time.  The test completion percentage must be non-negotiable,
but the code coverage percentage must take into account the maturity of the project and
the usage of the project.  This measurement is non-binary.  However, it is important to
know your project’s code coverage measurement, and how it trends over time. While the
measurement is non-binary, it is suggested to create a binary rule that
specifies what the minimum percentage is for each axis, failing the rule if that
specific metric falls below the goal percentage.&lt;/p&gt;
&lt;h2 id="wrapping-it-up"&gt;Wrapping It Up&lt;a class="headerlink" href="#wrapping-it-up" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;By breaking down the types of tests that are expected for a given project, the two
different types of measurements of reliably become more evident.  Scenario coverage
is determined by outlining the major scenarios for using a project and writing
end-to-end tests and scenario tests against them.  Scenario coverage must be a binary
measurement at release time.  Code coverage is determined by using tools to measure
which parts of the code are executed when running functional tests and unit tests.
Code coverage is a non-binary metric that must have a minimum bar for coverage that is
met for the project, and determined on the merits of the project itself.&lt;/p&gt;
&lt;p&gt;By using these two measurements, I hope that I have shown that it is possible to provide
a way to empirically measure reliability.  By having a project be transparent about how
it is reaching those measurements and what they are, any team can provide meaningful and
understandable measurements of reliability.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:defense"&gt;
&lt;p&gt;If asked, I could easily defend the percentages.  For the success case, I would assume that half the 60% number will come from first try successes and half the number will come from success that occurred after people fixed errors returned from the other two tests and resubmitted the data.  While the other two categories are somewhat guesswork, from my experience validation errors are 2-3 times more common than an “existing contact” processing error.  Note that in the absence of real data, these are estimates that do not have to be perfect, just reasonable. &lt;a class="footnote-backref" href="#fnref:defense" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:errorHandling"&gt;
&lt;p&gt;In designing any type of project, you should seek to have clear and consistent interfaces between your project and the users of the project.  An extension of that statement is that any responses you return to your user should be grouped with related responses and returned in a common data structure or UI element to avoid confusion. &lt;a class="footnote-backref" href="#fnref:errorHandling" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Software Quality"></category><category term="measuring software quality"></category><category term="software reliability"></category><category term="end-to-end tests"></category><category term="scenario tests"></category><category term="functional tests"></category><category term="unit tests"></category><category term="code coverage"></category><category term="scenario coverage"></category></entry><entry><title>What is Software Quality?</title><link href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/" rel="alternate"></link><published>2019-09-15T00:00:00-07:00</published><updated>2019-09-15T00:00:00-07:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2019-09-15:/2019/09/15/what-is-software-quality/</id><summary type="html">
&lt;p&gt;When introducing myself to someone professionally, I usually start with the normal
“Hi, my name is …” that is boiler-plated on nametags the world over.  Getting past that
initial point, if the person is so inclined, they ask that always fun lead off question “So,
what do you?”  For me, I …&lt;/p&gt;</summary><content type="html">
&lt;p&gt;When introducing myself to someone professionally, I usually start with the normal
“Hi, my name is …” that is boiler-plated on nametags the world over.  Getting past that
initial point, if the person is so inclined, they ask that always fun lead off question “So,
what do you?”  For me, I always respond with “I am an SDET”&lt;sup id="fnref:SDET"&gt;&lt;a class="footnote-ref" href="#fn:SDET"&gt;1&lt;/a&gt;&lt;/sup&gt;, to which anyone not in the
software industry replies back with “Um.... What is that?”&lt;/p&gt;
&lt;p&gt;Spewing out “It means I am a Software Development Engineer in Test.”, I wait for the response
that most people use: “Oh, so you are a tester.”  Often with gritted teeth, I try and explain
that testing is only a small part of what I do.  If I think they are still listening, I
given them my quick elevator pitch that emphasizes that I focus on helping to produce good
quality software by helping to increase the quality of the teams, the projects, and the
processes that I am tasked to assist with.&lt;/p&gt;
&lt;p&gt;Approximately 60-70% the time I win people over
with the elevator pitch, and a pleasant conversation continues.  The next 20-30% of the time,
usually with people not in the software field, I get blank stares and they fixate on the
“test” in the title rather than the “quality” in my description.  The remaining people are
usually Software Development Engineers or SDEs&lt;sup id="fnref:SDE"&gt;&lt;a class="footnote-ref" href="#fn:SDE"&gt;2&lt;/a&gt;&lt;/sup&gt; that for one reason or another, start to
tune out.&lt;/p&gt;
&lt;p&gt;For the percentage of people that I win over, they seem to understand that I focus on quality,
but the follow up question is almost always: “What does quality software mean to you?”&lt;/p&gt;
&lt;h2 id="where-do-we-start"&gt;Where do we start?&lt;a class="headerlink" href="#where-do-we-start" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For me, I almost always start at the beginning with requirements.  Whether they are
spoken or written down, each project has a set of requirements.  It could be the requirements
are to “explore my ability to use X” or “fix the X in the Y project” or “create a project
that can help me X”, but every project has requirements.&lt;/p&gt;
&lt;p&gt;In the software development industry, requirements are often presented
to teams that are hard to deal with or are left to the developers to write themselves.  This
practice is so prolific that Scott Adam’s Dilbert site has pages and pages of instance where
&lt;a href="https://dilbert.com/search_results?terms=User%20Requirements"&gt;requirements are talked about&lt;/a&gt;.
One example is when a manager talks to their team and
informs them that some process needs to be faster by 5%.  Do they have enough information from
that manager to understand the context of the requirement?  Do they expect that increase by a
specific time to meet their own goals?  What does that requirement look like?  How do they
know when they have achieved it?  Is it achievable?  If it is achievable, how do they measure
progress towards that goal?  These are some of the core questions that I believe need
answering.&lt;/p&gt;
&lt;p&gt;As those questions are at the front of my mind, when someone asks me how I define software quality, the first thing I immediately think back to is a course that I once took on setting
&lt;a href="https://en.wikipedia.org/wiki/SMART_criteria"&gt;S.M.A.R.T. requirements&lt;/a&gt;.
In that class, the main focus was on taking unrefined requirements and curating them to a
point where they could be more readily be acted upon.  The instructor made a very good
argument that each requirement must be Specific, Measurable, Assignable, Realistic, and
Time-Related.&lt;/p&gt;
&lt;p&gt;When it comes to software quality, I believe those same questions needs to be asked with
regards to any of the requirements teams put on their software.  But to ask those questions
properly, we need to have some context in which to ask those questions.  To establish that
context, it is helpful to have some guidelines to provide a framework for the requirements.&lt;/p&gt;
&lt;h2 id="establishing-some-guidelines-the-four-pillars"&gt;Establishing Some Guidelines: The Four Pillars&lt;a class="headerlink" href="#establishing-some-guidelines-the-four-pillars" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A good general article for anyone interested in software quality is the
&lt;a href="https://en.wikipedia.org/wiki/Software_quality#Measurement"&gt;Wikipedia article on Software Quality&lt;/a&gt;.
In fact, when asked by people where to get started in the software quality area, I often refer
them to this article solely because of the excellent diagram in the Measurements section on the
right side of the page.&lt;sup id="fnref:Pillars"&gt;&lt;a class="footnote-ref" href="#fn:Pillars"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The diagram in the Measurements section correlates very closely to what I believe are the four
pillars of software quality: Reliability, Maintainability, Efficiency, and Security.  The
diagram then shows how their pillars relate to other attributes: Application Architecture
Standards, Coding Practices, Complexity, Documentation, Portability, and Technical/Functional
Volumes.  From there, it provides more lists of how to break things down, with many references
to other articles.  In short, it is a great place to start from.&lt;/p&gt;
&lt;h2 id="measuring-software-quality"&gt;Measuring Software Quality&lt;a class="headerlink" href="#measuring-software-quality" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Before proceeding to talk about the pillars themselves, I feel strongly that we need to
discuss the categories that I use for measuring the metrics talked about in the Wikipedia
article.  My intention is that by talking about the metrics before discussing each of the
pillars, you can start building a mental model of how to apply them to your projects as you
are reading about them.  From my point of view, making that mental transition from something
abstract that you read about to something concrete that applies to your work is
essential to serious forward momentum on software quality.&lt;/p&gt;
&lt;p&gt;These metrics typically fall into two categories: seldom violated metrics and positive
momentum metrics.&lt;/p&gt;
&lt;p&gt;The seldom violated metrics category contains rules that define rules that are pivotal to the
quality of your project.  Each rule are a combination of a given metric and a maximum
or minimum weighed against that metric.  As a guideline, teams should only ignore
these rules on a case by case basis after providing a reason that is good, defensible, and
documented.
Examples of such metrics are Service Level Agreements (SLAs), Static Code Analysis
(SCA) results, and Test Failure Rates.  Examples of rules are “the TP99 for the X
API is Y millisecond” or “all PMD warnings (Java SCA tool) must be following with
a minimal of suppressions”.&lt;/p&gt;
&lt;p&gt;Furthermore, to make these rules useful and to keep your
team honest, your team needs to publish the selected metrics, with a description of what the
metrics are, how your team measures those metrics, and why your team is measuring them.&lt;/p&gt;
&lt;p&gt;The positive momentum metrics category is usually reserved for metrics that are being
introduced to an already existing project.  When introducing software quality metrics into an
already existing project, it is not realistic to expect those metrics to be adhered to in an
instant.  It is more realistic to expect positive momentum towards the goal
until the point when your team achieves it, at which point is moves to the desired seldom
violated metrics category.  As such, a measure of the momentum of these metrics is used, and is
hopefully in a positive direction. Similar to the previous category, your team should publish
information about the selected metrics, with the added information on when your team feels
they will translate it from the positive momentum category to the seldom violated category.&lt;/p&gt;
&lt;p&gt;Being consistent on these chosen metrics is very important.  While dropping a metric looks
better on any reporting in the short term, it usually negatively impacts the software quality,
perhaps in a way that is not obvious until later. Adding a new metric will show lower the
measured quality in the short term, but increases the measured quality in the long
term.  Your team can negate the short term impact by paying the immediate cost of making the
new metric a seldom violated metric, but that has to be weighed against the other priorities
for your project.  As with everything, it is a balancing act that needs to be negotiated with
your team.&lt;/p&gt;
&lt;h2 id="exploring-the-four-pillars"&gt;Exploring The Four Pillars&lt;a class="headerlink" href="#exploring-the-four-pillars" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Having established that S.M.A.R.T. requirements and the two categories for metrics
from the previous sections are useful in measuring software quality, the focus of the article
can return to the guidelines: the four pillars.  Each one of these pillars will look at
your software project from a different angle, with the goal of providing a set of data points
to formulate a coherent measurement of software quality for that project.&lt;/p&gt;
&lt;p&gt;In the following sections, I strive to describe each of the four pillars, providing a jumping
off point to another article that describes that pillar in a more comprehensive manner.  I
firmly believe that by providing metrics for each pillar that are specific to your project,
with each of those metrics properly categorized into the two measurement categories documented
above, that your team will take a decent step forward in clearly defining software quality for
your project.&lt;/p&gt;
&lt;h3 id="reliability"&gt;Reliability&lt;a class="headerlink" href="#reliability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The essence of this pillar can be broken down into two questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the software do the task that it is supposed to do?&lt;/li&gt;
&lt;li&gt;Does the software execute that task in a consistent manner?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reliability is one of the areas in which “pure” testing shines.  A lot of the tests that
SDEs, SDETs, and testers are asked to write specifically verify if a given object does
what it is supposed to do.  Unit tests determine whether an individual software unit, such as
a class, performs they way it is supposed to.  Functional tests or integration tests take that
a step higher, determining whether a group of related software units do what they are supposed
to do.  Another step higher are the scenario tests, which determine whether the software
project, as a whole, responds properly to various use cases or scenarios that are considered
critical to its operation.  Finally, end-to-end tests or acceptance tests determine whether or
not a group of projects respond properly from an end user’s perspective.&lt;/p&gt;
&lt;p&gt;This pattern is so widely used, any search for
&lt;a href="https://www.bing.com/images/search?q=test+pyramid"&gt;test pyramid&lt;/a&gt;,
will find many variations of the same theme. Different articles on the subject will stress
different points about the pyramid, but they will all generally look like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="test pyramid" src="https://jackdewinter.github.io/images/quality-1/test-pyramid.png"/&gt;&lt;/p&gt;
&lt;p&gt;This pyramid, or other similar pyramids, are interpreted by authors to indicate a specific
things about the tests, to highlight the position of their article.  Some of these
interpretations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An article on test volume will typically stress that ~70-80% of the tests should be at the unit test level, ~10-15% at the functional test level, ~5-10% at the scenario level, and ~1-3% at the end-to-end level.&lt;/li&gt;
&lt;li&gt;An article on test frequency will typically stress that tests near the bottom of the pyramid should complete within 60 seconds and be executed every time the source code is checked in.  Tests near the top of the pyramid may take minutes or hours and should be executed once a week.&lt;/li&gt;
&lt;li&gt;An article on test fragility will typically stress that tests near the bottom of the pyramid are closer to their components, the expectation is that they will not fail.  Tests near the top of the pyramid require more orchestration between projects and teams, and therefore, are more likely to failure do to environmental or other reasons.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While all of these interpretations have merit, the critical point for me is the issue of
boiling down that information to a small number of bite sized observations that can be easily
measured and communicated. In the upcoming article &lt;code&gt;Software Quality: Reliability&lt;/code&gt;, I will
delve more into breaking the Reliability pillar into S.M.A.R.T. requirements and I provide
suggestions on how it can be measured.&lt;/p&gt;
&lt;h3 id="maintainability"&gt;Maintainability&lt;a class="headerlink" href="#maintainability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The essence of this pillar can be broken down into one question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you are asked to change the software to fix a bug or introduce a new feature, how easy is it to change the software, how many surprises do you expect to encounter, and how confident will you be about the change afterwards?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The best, and most comedic, form of asking this question is captured by this cartoon
from &lt;a href="https://www.osnews.com/"&gt;OSNews&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="WTFs per minute" src="https://mk0osnewswb2dmu4h0a.kinstacdn.com/images/comics/wtfm.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Maintainability is a single pillar that encompasses the most diverse types of processes and
measurements of any of the pillars.  The reason for this is that maintainability is often a
word that is used without a lot of provided context.  For me, a good way to think about
maintainability is that it is the cleanliness of your project.  Different people will have
different experiences, and after asking different people about how “clean” the project is,
the collected answers will almost certainly by different.&lt;/p&gt;
&lt;p&gt;Try this in your office with your colleagues.  Point to a given area of your office and ask
2-5 people how clean a given area, such as your desk is.  Instead of accepting a single
answer, dig in a bit as to why they answered the way they did.  Most likely, you will
get as many distinct answers as people that you talk to.  This exercise illustrates how
hard it is to give a good answer to how maintainable software a given piece of software is.&lt;/p&gt;
&lt;p&gt;The best way to provide metrics for maintainability is usually with various Static Code
Analysis tools.  Almost every mature language has at least one tool to do this, and each tool
usually measures a fair number of metrics.  These metrics will use established (and sometimes
experimental) industry practices to look at the source code of your project and determine
if there are issues that can be addressed.  In addition to those metrics, those same tools
often look for “problematic” and “sloppy” code.  Problematic code is usually some manner of
pattern that a fair number of experts have agreed is a bad thing, such as appending to a
string within a loop.  Sloppy code is usually things like having a variable or a parameter
that is not being used, or a forgotten comment on a public method.&lt;/p&gt;
&lt;p&gt;In addition to Static Code Analysis, teams must continue to strive to have a good set of
documentation on what the project is doing, and regularly maintain that documentation.  While
the “correctness” of the documentation is harder to measure than source code, it is pivotal
for a project. How much of the information on the various projects that your team supports
is in the head of one or two individuals?  What is going to happen if they leave the team
or leave the company.&lt;/p&gt;
&lt;p&gt;Your team should not need volumes of information on every decision that was made, but as a
team, it is imperative to document the major decisions that affect the flow of the project.
It is also a good idea to have solid documentation on building, deploying, and executing
the project. Imagine yourself as a new team member looking at the software project and any
documentation, and honestly ask yourself “How much would I want to run away from that project?”
If the honest answer from each member of the team is something similar to “I’m good”, you
probably have a decent level of documentation.&lt;/p&gt;
&lt;h4 id="a-note-on-static-code-analysis"&gt;A Note On Static Code Analysis&lt;a class="headerlink" href="#a-note-on-static-code-analysis" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Before delving deeper into maintainability, I want to take a minute to talk about Static Code
Analysis.
Typically, Static Code Analysis is used as a gatekeeper for maintainability, and as such,
any suggestions should be strictly followed.  However, Static Code Analysis tends to be an
outlier to the gatekeeper rule in that the metrics need to be “bent” every so often. This
“bending” is accomplished using some form of suppression specified by the Analyzer itself.&lt;/p&gt;
&lt;p&gt;Static Code Analyzers tend to fall into two main categories: style and correctness.&lt;/p&gt;
&lt;p&gt;Any warnings that are generated by a style analyzer should be addressed without fail.
In terms of stylistics, there are very few times where deviating from a common style are
beneficial, and as such should be avoided.  As stylistics can vary from person to
person when writing code, it is useful to supplement the style analyzer with an IDE
plugin that will reformat the source code to meet the team’s stylistics, with the Static
Code Analyzer acting as a backstop in case the IDE formatting fails.&lt;/p&gt;
&lt;p&gt;Warnings generated by correctness analyzers are more likely to require bending.  Most
correctness analyzers are based on rules that are normally correct, but do have exceptions.
As such, your team should deal with these exception by having a follow up rule on when
it is acceptable to suppress the exceptions, and specifically on a case-by-case basis.
It is also acceptable to suppress the exception after generating a future requirement to
address the exception, if your team is diligent on following up with these requests.&lt;/p&gt;
&lt;p&gt;In both cases, it is important to remember that SCAs are used to help your team
keep the project’s maintainability at a healthy level.&lt;/p&gt;
&lt;h4 id="back-to-maintainability"&gt;Back to Maintainability&lt;a class="headerlink" href="#back-to-maintainability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;In the upcoming article &lt;code&gt;Software Quality: Maintainability&lt;/code&gt;, I will delve more into breaking
the Maintainability pillar into S.M.A.R.T. requirements and I provide suggestions on how it can
be measured.  I will do this by presenting the 4-5 metrics that I consider to be useful as
well as both patterns and anti-patterns to avoid. [ED: Need to rephrase that last sentence.]&lt;/p&gt;
&lt;h3 id="efficiency"&gt;Efficiency&lt;a class="headerlink" href="#efficiency" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The essence of this pillar can be broken down into one question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the software execute that task in a timely manner?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similar to my analogy of maintainability being the cleanliness of your software, efficiency
is whether or not your software is executing “fast enough”.  Coming up with an answer to a
question on whether or not something is “fast enough” is usually pretty easy.  But when
you ask for a definition of what “fast enough” means, that is when people start to have issues
coming up with a solid answer.  In my experience, a large part of the reason for that
vagueness is usually not having a good set of requirements.&lt;/p&gt;
&lt;p&gt;As an example, let’s figure out what “fast enough” means for two different video games that
my family plays: Civilization and Rocket League.&lt;/p&gt;
&lt;p&gt;For the game Civilization (in multiplayer mode), the big delays in the game are the human
interactions and decisions required before a player ends their turn.  It is
also very important that all of the information get conveyed between turns so that the
multiplayer server can accurately record actions in a fair and just manner.  For this game,
“fast enough” for the software is largely dwarfed by the delays that the players introduce.
However, if we have a game with 12 players, 2 of them human and the other 10 using the game’s
AI players, then we can start to formulate what “fast enough” is for the AI players.  It
really depends on the context.&lt;/p&gt;
&lt;p&gt;Rocket League is a different story.
&lt;a href="https://en.wikipedia.org/wiki/Rocket_League"&gt;Rocket League&lt;/a&gt;
is a sequel to the game “Supersonic Acrobatic Rocket-Powered Battle-Cars” released in 2008.
In this game, you play a game of arena soccer using rocket powered cars, each match consisting
of a series of games between teams of 1-3 players.  Unless there is a LAN tournament between
professional teams, it is very rare for more than one player to be in the immediate vicinity
of their teammates, and often players are from different states/provinces and even countries.
For the client software on the player’s computers, “fast enough” is measured by latency and
packet loss.  With each player’s action being relayed to the server and then back out to the
other players, any packet loss or increase in latency will impact the server’s reaction to
various inputs from the player’s controllers.  For this type of game, “fast enough” depends
on a good network connection and a server that is able to process many actions per second.&lt;/p&gt;
&lt;p&gt;As you can see from the video game example, efficiency greatly depends on what the requirements
of the software are.  In the upcoming article &lt;code&gt;Software Quality: Efficiency&lt;/code&gt;, I will delve
more into breaking the Efficiency pillar into S.M.A.R.T. requirements and I provide
suggestions on how it can be measured.&lt;/p&gt;
&lt;h3 id="security"&gt;Security&lt;a class="headerlink" href="#security" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The essence of this pillar can be broken down into one question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How easy is it for a third party to perform malicious actions with your software?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That is one dramatic question.  “Perform malicious actions.”  Wow!  I have read all sorts
of articles on various news sites about those, but surely they cannot affect my software?
That is usually one of the first reactions of a lot of software developers.  Just 10
minutes with a security researcher can open your eyes to what is possible.&lt;/p&gt;
&lt;p&gt;To understand this better, pretend that your software project is on a slide, being viewed
through a microscope.  If you look at the slide without the microscope, you just see your
software on the slide, pretty much the same as any other slide.  However, if you increase your
magnification by one order of magnitude, you see that your project includes your source code
and components developed by other people.  You may be following proper security practices, but
did they?&lt;/p&gt;
&lt;p&gt;Another order of magnitude down, and you are looking at the low level instructions for your
project and any included components.  Once the component was assembled, could a third party
have added some malicious code to that component, executing normally until they activate it?
Was that malicious code in their from the beginning?  Or maybe it is a vulnerability at the
source code, machine code, or machine levels?  Someone can make a small change to a component
to utilize that vulnerability with little effort if they know what they are doing.&lt;/p&gt;
&lt;p&gt;Reversing our direction, if we expand outwards instead of inwards, we have containerization.
Containerization solutions, such as &lt;a href="https://www.docker.com/resources/what-container"&gt;Docker&lt;/a&gt;,
provides a complete computing environment to execute your software within.  Popular with back
end development, you encapsulate your software with it’s intended operating system platform,
reducing the number of platform’s you need to design your software for to 1.  But with
containerization, we also have to ask the same questions of the platform as we did with the
software.  How secure is the operating system that the container uses as it’s base?&lt;/p&gt;
&lt;p&gt;In today’s world of software development, where componentization is key, the software you
write is not the only place where security issues can be introduced.  However, there are
proactive steps you can take to reduce the vectors than users can
follow to use your software maliciously.
In the upcoming article &lt;code&gt;Software Quality: Security&lt;/code&gt;, I will delve more into breaking
the Security pillar into S.M.A.R.T. requirements and I provide suggestions on how they it
be measured.&lt;/p&gt;
&lt;h2 id="back-to-requirements"&gt;Back To Requirements&lt;a class="headerlink" href="#back-to-requirements" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Having explored the 4 pillars, it is important to bring the discussion back to the definition
of good requirements.  Using the information from each of the individual pillar articles in
concert with the information on S.M.A.R.T. terminology, your team can request requirements
that are more focused.  As any focused requirements will be Specific (the S. in S.M.A.R.T.),
it is reasonable to expect that any impact on our 4 pillars will be noted.  Asking for this
change will almost guarantee some negotiations with the team’s stakeholders.&lt;/p&gt;
&lt;p&gt;In my experience, when your team asks for more focused goals from your stakeholders, there
will typically be some pushback from those stakeholders at the beginning.  If your team
has had some requirements mishaps in the past, highlight each mishap and how the ensuing
loss of time and focus could have been avoided usually sways stakeholders.  Don’t point
fingers, but simply point out something like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hey, when we did the X requirement, we all had a different idea on what to fix, and as such,it took X hours of meeting and Y hours of coding and testing to figure out it was the wrong thing.  We just want to help tune the requirements process a bit to help everyone try and avoid that waste.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most stakeholders are being asked to have their teams do the maximum amount of work possible
in the shortest amount of time. By asking that question in such simple manner, you are asking
if you can spend a small amount of time up front to hopefully eliminate any such missteps.
Most stakeholders will grab on to that as a way for them to look good and for the team to
look good, a win-win.&lt;/p&gt;
&lt;h3 id="what-will-these-requirements-look-like"&gt;What will these requirements look like?&lt;a class="headerlink" href="#what-will-these-requirements-look-like" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The requirements will typically come in two main categories.  The first category, requirements
focused on fixing bugs or adding features, will typically be the bulk of the requirements.
Each requirement should outline any negative impact it will have on any of the metrics.  If
nothing is added on negative impacts, the assumption is that the impact will be neutral or
positive.&lt;/p&gt;
&lt;p&gt;A good example of this is a requirement to add a new feature to the project.  The requirement
should be clearly stated using S.M.A.R.T. terminology, because it will remove any
ambiguity in the requirements.  As any source code added without tests would impact any
reliability metrics, reliability tests should be added to meet any seldom violated
metrics for your project.  In similar ways for the other 3 pillars, it is assumed that any
source code added will be a step forward or neutral in terms of quality, not backward.&lt;/p&gt;
&lt;p&gt;At some point in your project, you should expect that at least a few of the requirements
will appear in the the second category: requirements specifically targeted at one or more of
the pillars.  These requirements allow your team to focus on some aspect of your project where
your team feels that the quality can be improved.  The big caveat with these
requirements is to be mindful of the Achievable and Time-Related aspects of S.M.A.R.T.
requirements.  Make sure that whatever the goal of these requirements are, they are things
that won’t go on forever and are not pipe dreams.&lt;/p&gt;
&lt;p&gt;A good example of this is wanting to improve the efficiency of your project or processes.
Without a good requirements that is Specific, Achievable and Time-Related, this can go on
forever. A bad requirement would state something like “Make the project build faster”.  A good
requirement might state something like “Reduce the unit test time from 20 seconds to
under 15 seconds”, timeboxed to 4 hours.  The good requirement has good guard rails on it
to keep it from exploding on someone who picks up that work.&lt;/p&gt;
&lt;h2 id="publishing-software-quality"&gt;Publishing Software Quality&lt;a class="headerlink" href="#publishing-software-quality" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Having gone through the previous sections and any related articles, you should have a
better idea on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how to write better requirements to ask for software quality to be improved&lt;/li&gt;
&lt;li&gt;what metrics I recommend to use for each of the four pillars&lt;/li&gt;
&lt;li&gt;how to measure those metrics and integrate them into your projects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using this information as tools, your team can improve the quality of the project at it’s own
pace, be that either an immediate focus or a long term focus for your project.&lt;/p&gt;
&lt;p&gt;For any metrics that are in the seldom violated category, the best way to approach them is
to make them gatekeeper metrics for your project.  It should be possible to execute a great
many of the gatekeeper metrics before a commit happens, which is optimal.  For the remaining
metrics in the seldom violated category and metrics in the the positive momentum category,
your team should publish those metrics with every commit or push, giving the submitter that
needed feedback.&lt;/p&gt;
&lt;p&gt;In addition, publishing the metrics to some kind of data store allows your team to
determine how the project quality is trending over time, allowing any stakeholders or project
members to observe any potential software quality issues and take steps to deal with them.
Even for certain seldom violated metrics, it can be useful to track how they are trending,
even if they are trending above the gatekeeper lines set for the project.&lt;/p&gt;
&lt;p&gt;If your team does not publish those metrics in some form, the only data point they have for
the project is a binary one: it passes or it does not.  From my experience, that binary
metric is often a false positive that burns teams due to a lack of information.&lt;/p&gt;
&lt;h2 id="what-does-software-quality-mean-to-me"&gt;What Does Software Quality Mean To Me?&lt;a class="headerlink" href="#what-does-software-quality-mean-to-me" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Software quality means each software project has a plan.  When requirements come in to the
project, they are detailed using the S.M.A.R.T. terminology.  If not specifically geared
towards a given software quality pillar, each requirement may specify what kind of impact it
has on one or more of the pillars.  If not specified, it is assumed that it has a neutral or
positive effect on all of the software quality pillars.  The goals are also specific, not
overly broad, and realistically achieved within a given time frame.&lt;/p&gt;
&lt;p&gt;Software quality means that metrics are well thought out for each project.  Each metric is
both defensible and reasonable for that project and that team.  Any metrics that are not being
used as gatekeepers are published so they can be tracked over time.  For additional benefit,
non-binary gatekeeper metrics are also published, to further improve the project and the
quality of the project.&lt;/p&gt;
&lt;p&gt;Software quality means ensuring that software projects are reliable.  Projects have well
thought out tests that are performed at many levels to ensure that the project’s components
work together to meet the project requirements as well as verify the correctness of the
components themselves.  These tests are executed frequently, and a large number of them are
used as gatekeepers, trying to ensure that only reliable changes are made to the project.
When a project is released, the scenario coverage is 100% and the code coverage is either
at 100% or whatever percentage the team has negotiated and documented for their project.&lt;/p&gt;
&lt;p&gt;Software quality means ensuring that software projects are maintainable.  This entails
sufficient documentation of project goals, architecture, design, and current state. The
documentation is coupled with Static Code Analysis to measure a number of maintainability
metrics and to gatekeep on most of them, ensuring that the project moves in a positive
direction to a higher quality project.&lt;/p&gt;
&lt;p&gt;Software quality means ensuring that software projects and their processes are efficient.
Team process to administrate and maintain the software and the software itself do not have to
be blindingly fast, but they need to be as efficient as they need to be for that project and
for that team.  They do not need to be fast as lightning, only fast enough for the software
project itself.&lt;/p&gt;
&lt;p&gt;Software quality means ensuring that software projects are secure.  If third party components
are used for the project, those components need to be monitored for vulnerabilities, and
any issues that arise must be addressed quickly.  Steps are taken, at a level that is
appropriate for the type of software project, to reduce the possible ways that an user can use
the software project do something malicious.&lt;/p&gt;
&lt;p&gt;To me, software quality is about the journey, continuously improving quality and showing
that progress, while adding new features and fixing bugs at the same time.&lt;/p&gt;
&lt;h2 id="wrapping-it-up"&gt;Wrapping It Up&lt;a class="headerlink" href="#wrapping-it-up" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To put it succinctly, software quality for a project is about having a common nomenclature
describing the various pillars of quality, having a common way of measuring against each of
those pillars, and the publishing of those measures.  &lt;/p&gt;
&lt;p&gt;Therefore, from my point of view, software quality is not a single metric but a collection of
metrics and a philosophy.  That philosophy is that your team can only really answer that
question by having clearly defined goals for your project and it’s quality metrics, and
steering the project towards those goals.  &lt;/p&gt;
&lt;p&gt;Does every project need to be super high quality? No, not even close.  But I firmly believe
that each project needs to have a solid understanding of what level of software quality they
have in order to negotiate the definition of “good enough” for each project.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:SDET"&gt;
&lt;p&gt;In the United States, where I currently live, I am a Software Development Engineer in Test or SDET.  I do not have an engineering degree.  In any other country, including my native Canada, I am a Software Developer in Test or SDT. &lt;a class="footnote-backref" href="#fnref:SDET" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:SDE"&gt;
&lt;p&gt;In the United States, where I currently live, a Software Development Engineer or SDE is the same as a Software Developer in any other country. &lt;a class="footnote-backref" href="#fnref:SDE" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:Pillars"&gt;
&lt;p&gt;Based on my experience, where the article breaks out &lt;code&gt;Size&lt;/code&gt; as it’s own pillar, I would place it in the Maintainability section. Similarly, while I can understand why they place &lt;code&gt;Indentifying Critical Programming Errors&lt;/code&gt; in its own section, I would most likely fold half of the items into the Maintainability section and half of them into the Reliability section. To be clear, I agree with the content they present, it is just the organization that I disagree with on two small points. &lt;a class="footnote-backref" href="#fnref:Pillars" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Software Quality"></category><category term="measuring software quality"></category><category term="software reliability"></category><category term="software maintainability"></category><category term="software efficiency"></category><category term="software security"></category></entry></feed>