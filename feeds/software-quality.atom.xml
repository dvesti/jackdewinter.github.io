<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jack's Digital Workbench - Software Quality</title><link href="https://jackdewinter.github.io/" rel="alternate"></link><link href="https://jackdewinter.github.io/feeds/software-quality.atom.xml" rel="self"></link><id>https://jackdewinter.github.io/</id><updated>2020-02-03T00:00:00-08:00</updated><entry><title>Markdown Linter - Adding Block Quotes and Lists</title><link href="https://jackdewinter.github.io/2020/02/03/markdown-linter-adding-block-quotes-and-lists/" rel="alternate"></link><published>2020-02-03T00:00:00-08:00</published><updated>2020-02-03T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2020-02-03:/2020/02/03/markdown-linter-adding-block-quotes-and-lists/</id><summary type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Having the leaf blocks mostly in place, as documented
&lt;a href="https://jackdewinter.github.io/2020/01/27/markdown-linter-parsing-normal-markdown-blocks/"&gt;in the last article&lt;/a&gt;, the next items
on the implementation list were the list blocks and the block
quote blocks.  These Markdown blocks, referred to as Container Blocks in the
&lt;a href="https://github.github.com/gfm/#container-blocks"&gt;GitHub Flavored Markdown (GFM) Specification&lt;/a&gt;,
are the more complicated blocks …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Having the leaf blocks mostly in place, as documented
&lt;a href="https://jackdewinter.github.io/2020/01/27/markdown-linter-parsing-normal-markdown-blocks/"&gt;in the last article&lt;/a&gt;, the next items
on the implementation list were the list blocks and the block
quote blocks.  These Markdown blocks, referred to as Container Blocks in the
&lt;a href="https://github.github.com/gfm/#container-blocks"&gt;GitHub Flavored Markdown (GFM) Specification&lt;/a&gt;,
are the more complicated blocks to deal with, as they are capable of containing other
blocks.  As there are
&lt;a href="https://github.github.com/gfm/#phase-1-block-structure"&gt;specific suggestions&lt;/a&gt;
on how to parse these blocks, my confidence took a hit when I started looking at this
section. My viewpoint: if the specification writers thought it was difficult to
implement that they wrote suggestions on how to handle it, it must not be as easy as
the leaf blocks!&lt;/p&gt;
&lt;p&gt;The full record of the work detailed in this article is documented in the
project’s GitHub repository in the commits that occurred between
&lt;a href="https://github.com/jackdewinter/pymarkdown/commit/49407f86564479934ee92fb59597ba830a46fde8"&gt;08 December 2019&lt;/a&gt;
and
&lt;a href="https://github.com/jackdewinter/pymarkdown/commit/24fd972aef725bc55659ad3c2e7c1b93737a1874"&gt;20 December 2019&lt;/a&gt;.  This work includes creating the
scenario tests for all of the Container Blocks as documented in the GFM specification
and implementing the parsing to pass most those tests except for the nested cases.&lt;/p&gt;
&lt;h2 id="container-blocks-leaf-blocks-and-interactions-oh-my"&gt;Container Blocks, Leaf Blocks, and Interactions (Oh My!)&lt;a class="headerlink" href="#container-blocks-leaf-blocks-and-interactions-oh-my" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Before container blocks, parsing was easy.  A block starts, and when the parser
encounters the termination conditions, it ends.  There are a few rules about when
blocks can start and end, such as
&lt;a href="https://github.github.com/gfm/#example-83"&gt;“An indented code block cannot interrupt a paragraph.”&lt;/a&gt;,
but for the most part, there is little interaction between the leaf blocks.  The leaf
blocks are clean and tidy.  Not so much with container blocks.&lt;/p&gt;
&lt;p&gt;Container blocks, by their very definition, contain other blocks, namely leaf blocks and
container blocks.  While this makes certain visual elements easier, this also means
specific rules about what interactions are allowed between the blocks.  On top of that,
as container blocks can contain other container blocks, testing is required to ensure
that an arbitrary number of nested containers is properly supported.&lt;/p&gt;
&lt;p&gt;A great example of nesting container blocks is the Markdown implementation of sublists.
A list containing a list containing a list is simple in Markdown:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="k"&gt;first&lt;/span&gt; &lt;span class="k"&gt;level&lt;/span&gt;
  &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="k"&gt;second&lt;/span&gt; &lt;span class="k"&gt;level&lt;/span&gt;
    &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;third&lt;/span&gt; &lt;span class="k"&gt;level&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That example is not a single list, but 3 separate lists.  The &lt;code&gt;first level&lt;/code&gt; list is the
first level list, containing the list &lt;code&gt;second level&lt;/code&gt;, which contains the list
&lt;code&gt;third level&lt;/code&gt;.  And while sublists are a simple case of container blocks, more complex
cases are possible, such as this one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="k"&gt;first&lt;/span&gt; &lt;span class="k"&gt;level&lt;/span&gt;
  &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="o"&gt;```&lt;/span&gt;&lt;span class="nb"&gt;text&lt;/span&gt;
    &lt;span class="n"&gt;my&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;
    &lt;span class="o"&gt;```&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This list is similar to the first list, except it contains a fenced code block as the
contained block.  Both of these examples are just a few of the possibilities of how
container blocks can contain other blocks.  Looking through the specification, I
quickly lost count of the number of combinations possible.&lt;/p&gt;
&lt;h2 id="enter-lazy-continuations"&gt;Enter Lazy Continuations&lt;a class="headerlink" href="#enter-lazy-continuations" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If the interactions between container blocks and the blocks they contain wasn’t a fun
enough exercise in mental agility, enter lazy continuations.  From the GitHub Flavored
Markdown (GFM) Specification’s
&lt;a href="https://github.github.com/gfm/#block-quotes"&gt;block quotes section&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Laziness&lt;/strong&gt;. If a string of lines Ls constitute a block quote with contents Bs, then the result of deleting the initial block quote marker from one or more lines in which the next non-whitespace character after the block quote marker is paragraph continuation text is a block quote with Bs as its content.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and from the &lt;a href="https://github.github.com/gfm/#list-items"&gt;list items section&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Laziness&lt;/strong&gt;. If a string of lines Ls constitute a list item with contents Bs, then the result of deleting some or all of the indentation from one or more lines in which the next non-whitespace character after the indentation is paragraph continuation text is a list item with the same contents and attributes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Basically, what they are both saying is that if a paragraph has been started with block
quotes or within a list AND if a line is clearly a continuation of a paragraph, then it
is valid to remove some or all of the container block markers.  For a more concrete
example, &lt;a href="https://github.github.com/gfm/#example-211"&gt;example 211&lt;/a&gt; has the following
Markdown:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;
&lt;span class="n"&gt;baz&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which is parsed the same as if the following Markdown was written as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;bar&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;baz&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After reading those sections and letting them sink in, my confidence took a dip.  This
was not going to be an easy concept to get right.  But the sooner I dealt with those
scenarios, the sooner I could try and implement them the right way.  So I went forward
with the implementation phase of the container blocks.&lt;/p&gt;
&lt;h2 id="getting-down-to-work-the-easy-scenarios"&gt;Getting Down to Work - The Easy Scenarios&lt;a class="headerlink" href="#getting-down-to-work-the-easy-scenarios" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I often recommend to friends and
co-workers that taking a break and doing something unconnected to the “chore” helps
your mind get things together.  As such, before getting started on this work, I
decided to walk our dog for a while and let some of these concepts mull around in my
head. I am not sure if it was the exercise or the change in scenery, but it helped to
clear the cobwebs from my head and helped me to see things about the project more
clearly.&lt;/p&gt;
&lt;p&gt;The big thing that it accomplished was to help me cleanly separate out the easy tasks
from the more difficult tasks. The easy tasks? Simple block quotes and simple lists,
including sub-lists.  The difficult tasks?  Lazy continuations and mixed container
types.  I remember feeling that taking this time helped my confidence on the project,
as I was taking simple steps to understand where the difficulties were most likely to
show up.  This process also allowed me to think about those hard issues a bit while
implementing the easier features.  While I wasn’t devoting any serious time to the
more complicated features, it was good to just have my mind aware of which sections of
code that I was going to need to keep flexible going forward.&lt;/p&gt;
&lt;p&gt;Keeping this in mind, I started with block quotes, adding the block quote test cases to
&lt;code&gt;test_markdown_block_quotes.py&lt;/code&gt;, disabling any tests that I figured were not in the
easy category.  I then proceeded to implement the code, in the same way as detailed in
the
&lt;a href="https://jackdewinter.github.io/2020/01/27/markdown-linter-parsing-normal-markdown-blocks/"&gt;prior article on leaf blocks&lt;/a&gt;.
Implementing the easy scenario tests for the block quotes was a decent sized task,
mostly completed during two days on a weekend where I had some time.  This also included
fixing scenario tests in 6 other test files that has block quotes in their scenarios.&lt;/p&gt;
&lt;p&gt;Working on the basic list items over the next week, by the middle of the next weekend
they were completed, in a similar fashion to how the block quotes were completed: new
scenario tests were added, the easy ones were then tested, enabled, and verified for
completion, and the more difficult ones were disabled.  Similar to the block quotes,
getting these right took roughly a week, and that work also had impact on scenario
tests other than the ones I added.&lt;/p&gt;
&lt;p&gt;During this process, I believed I found the parsing of lists more difficult.  Thinking
about the
implementation in hindsight, I believe it was mostly due to their parsing requirements.
The fact is that block quotes have a single character &lt;code&gt;&amp;gt;&lt;/code&gt; to consider for parsing,
while the lists can be unordered and start with the &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;*&lt;/code&gt; character or the
lists can be ordered and start with a number and the &lt;code&gt;)&lt;/code&gt; or &lt;code&gt;.&lt;/code&gt; or character.  In
addition, for ordered lists, there is also the parsing of the start number and how to
interpret it.  Looking at the two blocks that way, block quote blocks seem a lot easier
to me.&lt;/p&gt;
&lt;p&gt;However, now that I have had a bit of time since that code was written, I believe that
those two features were more closer in difficulty that I initially thought.  Having
implemented both block quotes and lists, I think that they both had something that was
difficult that needed overcoming.  Since I have done a lot of parsers in my past, the
number of variations in parsing the lists were immediately noticeable to me, while the
block quotes were pretty easy to parse.  Balancing that out, once parsed the lists were
easy to coordinate, while the block quotes took a bit more finessing to get right.
In the end, I believe it was a pretty event effort to get both done properly.&lt;/p&gt;
&lt;p&gt;At least until nested mixed container blocks.&lt;/p&gt;
&lt;h2 id="nested-and-mixed-containers"&gt;Nested and Mixed Containers&lt;a class="headerlink" href="#nested-and-mixed-containers" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Nested container blocks, specifically mixed nested container blocks, is where things
got messy.  To be 100% honest, I am pretty sure I didn’t get everything right with the
implementation, and I already have plans to rewrite this logic. More on that later.&lt;/p&gt;
&lt;p&gt;I started implementing these features knowing that they probably made up the remaining
10% of the scenarios. I also figured that to handle these specific scenarios properly
would require as much time and effort as the prior 90% of the scenarios.  This was not
really a surprise, as in software development getting a project to the 70-90% finished
mark is almost always the easy part.&lt;/p&gt;
&lt;p&gt;Over the next week’s work, I reset my fork of the code back to it’s initial state 3 or 4
times.  In each case, I just got to a point where I either hit a block in going forward,
I wasn’t happy and confident about the solution, or both.  In one of those cases, the
code was passing the scenario tests that I was trying to enable, but it just didn’t feel
like I could extend it to the next scenario.  I needed to be honest with myself and
make an honest determination of how good the code I just wrote was.&lt;/p&gt;
&lt;p&gt;In the end, I completed some of the sublists and nested block quotes, requiring only 4
scenario tests to be disabled or skipped.  The ones that were disabled were the 10% of
the 10%, the cases where there were 3 or more levels of block quotes and lists mixed
together. I was not really happy with it, but after a week, I knew I needed to move on
with the project.  Grudgingly, I acknowledged that I would need to rewrite this later.&lt;/p&gt;
&lt;h2 id="why-rewrite-already"&gt;Why Rewrite Already?&lt;a class="headerlink" href="#why-rewrite-already" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I am very confident that I coded the easy level cases correctly, as I have solid
scenario tests, and a decent volume of them, to test the various use cases.  For the
medium difficulty cases, such as a container within a container, I have a decent amount
of confidence that the scenario tests are capturing most of the permutations.  It is
the more complicated cases that I really am not confident about.  And when I say I am
not confident, it is not that I am not sure if it is handling the test properly: that
is a binary thing.  The test is passing, or the test is failing, and thus disabled.  I
m not confident that all of those tests work for all use cases like that the scenario
tests represent.&lt;/p&gt;
&lt;p&gt;Part of any project is learning what works and what doesn’t work.  As I started
looking at implementing
&lt;a href="https://github.github.com/gfm/#example-237"&gt;example 237&lt;/a&gt;, I read the
following paragraph located right before the example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is tempting to think of this in terms of columns: the continuation blocks must be indented at least to the column of the first non-whitespace character after the list marker. However, that is not quite right. The spaces after the list marker determine how much relative indentation is needed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It was then that I was pretty sure I had coded the container blocks in terms of columns
instead of spaces.  Add that to the list of rewrites needed.&lt;/p&gt;
&lt;p&gt;The other category where my confidence is not high is with multiple levels of mixed
container blocks.  Once I complete the rewrite above, I can properly evaluate how well
I can nest the containers, but at the moment, that is not high.  At that point,
example 237 will be a good scenario test to determine how well I have those set up.
Having taken some time to really evaluate the code and the scenario tests, I just have
a suspicion that there is at least 1-2 bugs in the code that I wrote.  For now, that is
on my list of possible rewrites, with a medium to high probability of being needed.&lt;/p&gt;
&lt;p&gt;The saving grace for both of these scenarios that I believe need rewrites?  Their
frequency.  The scenarios for blocks, leaf blocks and container blocks, comprise
about half of the specification, ending with
&lt;a href="https://github.github.com/gfm/#example-306"&gt;example 306&lt;/a&gt;.  According to my test failure
report, only 4 of the list block tests had to be marked as skipped, hence they were not
passing.  At approximately 1.3% of the total scenarios, it is not a big impact.  In
writing this block, I have used lists frequently, block quotes sporadically, and block
quotes with lists even less.  I am not sure if my writing is representative of
everyone’s writing, but at least at the moment, &lt;/p&gt;
&lt;h2 id="what-was-my-experience-so-far"&gt;What Was My Experience So Far?&lt;a class="headerlink" href="#what-was-my-experience-so-far" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;All of the leaf blocks were finished in about a week.  The easy and medium cases for
the container blocks were finished about a week.  The hard cases for the container
blocks… not finished after a week, but close.&lt;/p&gt;
&lt;p&gt;Was I disappointed?  Sure.  But in comparison to other issues I have had with projects,
this was not even near the top 20 in terms of disappointment.  To be honest, in terms
of how projects have gone for me over the years, this has been a decent project to
work on.  Every project has it’s issues, and this was just the set of issues that
happened to occur on this project.&lt;/p&gt;
&lt;p&gt;I know it may sound a bit silly, but me and my immediate family have a saying we like
to repeat when things get tough: “Stuff&lt;sup id="fnref:notStuff"&gt;&lt;a class="footnote-ref" href="#fn:notStuff"&gt;1&lt;/a&gt;&lt;/sup&gt; happens, pick yourself up, dust
yourself off, and figure out what to do next.”  The disabled tests happened, so I took
some time to find my focus, and came up with a plan to deal with it.  Not a great
plan, but it meant I could go forward with the remaining scenarios and circle back
once I accumulated more experience with the parser.&lt;/p&gt;
&lt;p&gt;Sure there already was some
&lt;a href="https://en.wikipedia.org/wiki/Technical_debt"&gt;technical debt&lt;/a&gt;
for this project, but other than that, I believe it is going well.  At this point it
was just before Christmas, and I had a Markdown parser that was coming along pretty
well.  My confidence in the implemented leaf blocks was high, as was my confidence in
the easy 90% of the container block implementation.  The more difficult 10% of the
container blocks was still undecided, but I had a plan to deal with it going forward.
While not a sterling situation, it was definitely a good position for me to be in.&lt;/p&gt;
&lt;h2 id="what-is-next"&gt;What is Next?&lt;a class="headerlink" href="#what-is-next" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Before I took some time to improve my PyScan tool, I worked on adding HTML block
support for the PyMarkdown project.  As HTML in Markdown has some funny logic associated
with it, the next article will be devoted entirely to the HTML blocks.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:notStuff"&gt;
&lt;p&gt;When my kids were younger, I did indeed use the word “stuff”.  As my kids got older, we changed that word to another one that also starts with “s”.  The actual word that we now use should be easy to figure out! &lt;a class="footnote-backref" href="#fnref:notStuff" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Software Quality"></category><category term="markdown linter"></category></entry><entry><title>Markdown Linter - Parsing Normal Markdown Blocks</title><link href="https://jackdewinter.github.io/2020/01/27/markdown-linter-parsing-normal-markdown-blocks/" rel="alternate"></link><published>2020-01-27T00:00:00-08:00</published><updated>2020-01-27T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2020-01-27:/2020/01/27/markdown-linter-parsing-normal-markdown-blocks/</id><summary type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;With the &lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;project requirements&lt;/a&gt;,
the &lt;a href="https://jackdewinter.github.io/2019/12/16/markdown-linter-setting-up-parser-tests/"&gt;test framework&lt;/a&gt;,
and the &lt;a href="https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/"&gt;test strategy&lt;/a&gt; in place,
it was time to start working on the most frequently used and easy-to-parse Markdown
items.  These Markdown blocks, referred to as Leaf Blocks in the
&lt;a href="https://github.github.com/gfm/#leaf-blocks"&gt;GitHub Flavored Markdown (GFM) Specification&lt;/a&gt;,
are the root of many Markdown …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;With the &lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;project requirements&lt;/a&gt;,
the &lt;a href="https://jackdewinter.github.io/2019/12/16/markdown-linter-setting-up-parser-tests/"&gt;test framework&lt;/a&gt;,
and the &lt;a href="https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/"&gt;test strategy&lt;/a&gt; in place,
it was time to start working on the most frequently used and easy-to-parse Markdown
items.  These Markdown blocks, referred to as Leaf Blocks in the
&lt;a href="https://github.github.com/gfm/#leaf-blocks"&gt;GitHub Flavored Markdown (GFM) Specification&lt;/a&gt;,
are the root of many Markdown documents and have the virtue of being easy to parse.
With small exceptions, each of the Leaf Blocks is self contained.  For the most part,
those exceptions arise in how the Leaf Blocks interact with each other.
In all cases, this interaction is small and does not require complicated logic to
understand.&lt;/p&gt;
&lt;p&gt;The full record of the work detailed in this article is documented in the
project’s GitHub repository in the commits that occurred between
&lt;a href="https://github.com/jackdewinter/pymarkdown/commit/4ad7ebd46d7651d20a8b470b4d287e49cebfff75"&gt;30 November 2019&lt;/a&gt;
and
&lt;a href="https://github.com/jackdewinter/pymarkdown/commit/aee7a3def2dffc8531814a8b87365068d07730dc"&gt;05 December 2019&lt;/a&gt;.  This work includes creating the
scenario tests for all of the Leaf Blocks as documented in the GFM specification
and implementing the parsing to pass all of those tests except for the Link Reference
Definitions, HTML Blocks, and Tables.&lt;/p&gt;
&lt;p&gt;While the documentation of what needed to be done (GFM Specification) and what was done
(GitHub commits) is pretty straightforward, the “hows” and “whys” of what I implemented
is worth talking about.  The process that I followed for the implementation of the Leaf
Blocks did not uncover any development issues during implementation.  However, without
giving too much away, the same process applied to other block types (to be talked about
in future articles) did uncover issues that were not so easy to resolve.  As there were
complications that arose with those feature implementations, I wanted to provide a
consistent documentation of the process from the beginning, to provide a complete
picture of how things progressed.   I firmly believe that it is always good to show the
complete story of what happened, and not only one side of the story. So let’s go!&lt;/p&gt;
&lt;h2 id="moving-forward-with-implementation"&gt;Moving Forward With Implementation&lt;a class="headerlink" href="#moving-forward-with-implementation" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Even though the first commit for processing Markdown elements is on 30 November 2019, my
work on implementing them started on 25 November 2019.  Based on the test framework and
strategy documented in previous articles, the first thing to do was to write the
scenario tests cases, even if most of those tests were initially disabled or skipped.
This was easily done by annotating each test function with &lt;code&gt;@pytest.mark.skip&lt;/code&gt;. Once
I implemented the code to satisfy a given test, I removed that skip annotation for
that specific test.  While I would made modifications on how I disabled tests later on,
this was a good point for me to start off at.&lt;/p&gt;
&lt;h2 id="what-was-the-workflow"&gt;What Was the Workflow?&lt;a class="headerlink" href="#what-was-the-workflow" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;From the outset, the basic implementation workflow was as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;figure out the next section to work on&lt;/li&gt;
&lt;li&gt;figure out the next section-feature to implement&lt;/li&gt;
&lt;li&gt;enable the relevant tests for that section-feature&lt;/li&gt;
&lt;li&gt;add or change the code in &lt;code&gt;tokenized_markdown.py&lt;/code&gt; to implement that feature&lt;/li&gt;
&lt;li&gt;execute all enabled tests, with special attention to the feature added in item 4.&lt;/li&gt;
&lt;li&gt;if there were any test errors; debug, fix and go back to &lt;code&gt;item 4.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;stage the changes in the project before&lt;/li&gt;
&lt;li&gt;if there are more features in the current section, go back to &lt;code&gt;item 2.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;verify each test case’s input and output against the specification&lt;/li&gt;
&lt;li&gt;if any verification errors are found; debug, fix and go back to &lt;code&gt;item 4.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;if there are any leaf block sections left to work on, go back to &lt;code&gt;item 1.&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It wasn’t really glamourous, but it worked well.  Looking closely at the list, it is
easy for me to see why… I took an agile approach without really being aware of it.
According to the Wikipedia article on
&lt;a href="https://en.wikipedia.org/wiki/Agile_software_development#Agile_software_development_practices"&gt;Agile Software Development&lt;/a&gt;,
there are a number of good practices that I was following.  Because I was doing testing
as I went, the is a good argument to be made that I was practicing
&lt;a href="https://en.wikipedia.org/wiki/Agile_testing"&gt;Agile Testing&lt;/a&gt; and
&lt;a href="https://en.wikipedia.org/wiki/Test-driven_development"&gt;Test Driven Development&lt;/a&gt;.
As the tests are also the acceptance criteria for this stage of the project,
&lt;a href="https://en.wikipedia.org/wiki/Acceptance_test-driven_development"&gt;Acceptance Test Driven Development&lt;/a&gt;
could also be tacked on to those two Agile practices.  Finally, as the workflow is
iterative by it’s very nature, the workflow also qualifies as
&lt;a href="https://en.wikipedia.org/wiki/Iterative_and_incremental_development"&gt;Iterative and Incremental Development&lt;/a&gt;.
All in all, I see a number of solid agile patterns within the workflow.&lt;/p&gt;
&lt;p&gt;Agile aspirations aside, the real test of this workflow is that it works for me and
works well.  I was able to stick to the process pretty easily. It very nicely
compartmentalized my work into nice iterations that were easy for me to keep in my
head. It was also simple enough that if I needed to refocus myself, I just had to
figure out where I was in the workflow and where I was in the specification, and I
was able to get back to work!  In addition, I feel that if I had performed this
development as part of a team, the frequent commits and complete with enabled tests
would enable me to share my progress with the rest of the team, and solicit their
feedback in a quick and iterative manner.&lt;/p&gt;
&lt;p&gt;More importantly, at no point in the development practice did I feel that I bit off more
than I could handle.  Of course there were times where I was wondering how long it was
going to take me and how I would handle some features… I am only human!  But the agile
nature of how the workflow is structured kept me grounded and focused on the feature
that was in front of me.  I just reminded myself to keep that focus, and feature by
feature, the foundations of the parser came together.&lt;/p&gt;
&lt;p&gt;In the end, this workflow wasn’t about being agile or taking easy to implement steps.
It is about finding something that works well for the team… namely me.&lt;/p&gt;
&lt;h2 id="how-did-things-progress"&gt;How Did Things Progress?&lt;a class="headerlink" href="#how-did-things-progress" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The order in which things are tackled is important.  Doing the big
stuff at the start of the project sometimes pays off, but it can often be demoralizing.
Doing the small stuff first can lay some great foundations, but miss the larger target
due to the smaller focus.  To accomplish this for the PyMarkdown project, I broke
this part of the project down into 4 groups of Markdown elements.  Each group of
Markdown elements that were handled added new information to the stream of tokens
that were being generated by the parser, allowing for future examination.  It was
very important to me to ensure that the token stream was kept working and moving
forwards at all times.&lt;/p&gt;
&lt;h3 id="group-1-foundational-elements"&gt;Group 1: Foundational Elements&lt;a class="headerlink" href="#group-1-foundational-elements" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The first group that I worked on were the rudimentary elements of blank lines,
paragraphs, and thematic breaks.  This was a good first
group to work on, as these were all common Markdown elements that people use, and are
foundational to the rest of the work.  As such, they were good confidence boosters for
the tribulations that I expected that would occur later with the more complicated
elements.&lt;/p&gt;
&lt;p&gt;The only real issue that I had with this first group was due to my lack of confidence
about the Markdown specification itself.  From my days on the
&lt;a href="https://www.ietf.org/"&gt;Internet Engineering Task Force&lt;/a&gt;,
I am used to clear grammar specifications written in
&lt;a href="https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form"&gt;Backus-Naur form&lt;/a&gt;.
However, this specification has no such representation and is written mainly as a
series of use cases and text to describe each use case.  It took me a while to see that
what I perceived initially as a downfall was actually a bonus.  Instead of having to
search for examples or to make them up myself, they were already provided.  Once I got
used to that concept, my confidence increased and I started to implement each test more
quickly than the last one.&lt;/p&gt;
&lt;p&gt;While it didn’t seem like much at the time, at this point the parser was capable of handling the following Markdown:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;captured&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;paragraph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="o"&gt;***&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h4 id="group-1-sidebar-tabs"&gt;Group 1 Sidebar: Tabs&lt;a class="headerlink" href="#group-1-sidebar-tabs" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;I started to tackle the GFM specification decision that any tab character is
considered to be replaced with exactly 4 space characters.  For the most part, this had
little bearing on the foundational elements, but the subject of
&lt;a href="https://www.bing.com/search?q=tabs+vs+spaces"&gt;tabs versus spaces&lt;/a&gt; has ignited
&lt;a href="https://www.reddit.com/r/programming/comments/3xbyh6/the_software_development_holy_wars_part_i_the/"&gt;programming holy wars&lt;/a&gt;
that last to this day.  I thought it was useful and prudent to deal with it
and get it out of the way early.&lt;/p&gt;
&lt;p&gt;Smartly, Markdown avoids these arguments with a strong statement that 1 tab character
equals 4 space characters, and a decent argument to reinforce that the decision is the
right one. With the exception of the indented code block, every Markdown element is
only recognized if it starts with less than 4 spaces.  An indented code block line is
only recognized if it starts with 4 spaces.  Therefore, a shortcut for any indented
code block is to start the line with 1 tab character, due to it’s 1:4 mapping.  To be
honest, I feel this is brilliant in it’s simplicity.&lt;/p&gt;
&lt;h3 id="group-2-headers"&gt;Group 2: Headers&lt;a class="headerlink" href="#group-2-headers" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The next group that I tackled were the header markers, referred to in the specification
as the &lt;code&gt;setext&lt;/code&gt; and &lt;code&gt;atx&lt;/code&gt; elements.  Weird names though they are, they are the up to 6
&lt;code&gt;#&lt;/code&gt; characters at the start of the line, or the &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;=&lt;/code&gt; characters underlining
text from a previous paragraph.  While the &lt;code&gt;atx&lt;/code&gt; elements (the &lt;code&gt;#&lt;/code&gt; characters) was
pretty straight forward, the ‘underlining’ aspect of the &lt;code&gt;setext&lt;/code&gt; element made it
interesting.  As that element essentially makes the last paragraph a heading, I had to
search backwards in the list of generated tokens for the first time.&lt;/p&gt;
&lt;p&gt;It was also at this point that I decided to perform some refactoring to better
handle string processing.  The simple truth about any parser is that it requires
gratuitous amounts of “string fiddling” &lt;sup id="fnref:stringFiddle"&gt;&lt;a class="footnote-ref" href="#fn:stringFiddle"&gt;1&lt;/a&gt;&lt;/sup&gt;.  Most efficient parsers work
aggressively to parse their documents in a way that minimizes the number of actual
strings created while parsing.  A good example of efficient “string fiddling” can be
seen in the following example of parsing the sentence &lt;code&gt;I have a black dog&lt;/code&gt;.  When
parsing out the word &lt;code&gt;black&lt;/code&gt;, the most optimal parsers will find the index of the &lt;code&gt;b&lt;/code&gt;
in &lt;code&gt;black&lt;/code&gt;, then find the space character after the &lt;code&gt;k&lt;/code&gt;, using the language’s
&lt;code&gt;substring&lt;/code&gt; function  and those two indexes to create a single string with &lt;code&gt;black&lt;/code&gt; in
it.  Less optimal parsers will find the &lt;code&gt;b&lt;/code&gt;
append it to the end of an empty string (creating a new string with &lt;code&gt;b&lt;/code&gt;), then find
the &lt;code&gt;l&lt;/code&gt; character and appended it, etc.  This can easily cause 6 strings to be created
during the parsing of the word &lt;code&gt;black&lt;/code&gt;, when only 1 is needed.  As some of the
Markdown documents that the parser will handle are large, it is important to remember
optimizations like this as features are added.&lt;/p&gt;
&lt;p&gt;Keeping this in mind, I started looking for “string fiddling” patterns that looked ripe
for refactoring.  The most obvious one was the
&lt;code&gt;determine_whitespace_length&lt;/code&gt; function that took care of any tabs in the input data.
While I would rip this out later, opting instead to do a simple search-and-replace for
tabs at the start of parsing, the &lt;code&gt;determine_whitespace_length&lt;/code&gt; function kept things
manageable for tabs characters.  There were also the &lt;code&gt;extract_whitespace*&lt;/code&gt; functions for
extracting whitespace and the &lt;code&gt;collect_while_character&lt;/code&gt; function for collecting data
for a string while the input was a given character.  Taking a couple of peeks ahead in
the specification, it was easy to see that moving the code into those functions was going to pay off.&lt;/p&gt;
&lt;p&gt;When it comes down to it, there were no real issues that I experienced with the
headers.  My confidence was still building from the foundational group above, but there
was nothing weird or challenging that I did not handle with a bit of serious thought
and planning.&lt;/p&gt;
&lt;p&gt;At this point, the parser was capable of handling the following Markdown elements:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;Markdown&lt;/span&gt;

&lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;captured&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;paragraph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;But&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;also&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;
&lt;span class="c1"&gt;-------------------&lt;/span&gt;

&lt;span class="o"&gt;***&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="group-3-indented-and-fenced-code-blocks"&gt;Group 3: Indented and Fenced Code Blocks&lt;a class="headerlink" href="#group-3-indented-and-fenced-code-blocks" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Marching right along, indented and fenced code blocks were next on the list.  Both are
used to denote sections of text that are to be represented literally, but one is easier
and one is more flexible.  The indented code blocks require 4 space characters (or a
tab character) at the start of the line to denote the block, and text is presented
plainly.  However, the fenced code blocks start and end with an equal number of &lt;code&gt;`&lt;/code&gt;
or &lt;code&gt;~&lt;/code&gt; characters and include provisions for naming the type of text used within the
code block.  This naming allows processors to specify a given style to apply to the
code block, allowing processors and style sheets to ‘colorize’ the text according to
the the specified type name.&lt;/p&gt;
&lt;p&gt;This grouping was pretty easy to process, adding the &lt;code&gt;extract_until_whitespace&lt;/code&gt; function
to the growing list of helper functions.  The interesting part to the code blocks was
that I needed to add extra processing of normal text to handle the text within the code
blocks.  Prior to these code blocks, any text that did not fall into one of the other
categories was simply wrapped in a paragraph.  Both of these blocks have specific end
conditions, and until those end conditions are met, the collection continues.  This
meant adding extra code at the start of line parsing to determine if it was within one
of the code blocks.  If the end condition was met, then the end block token was emitted,
and if not, a text block would be emitted without further parsing.&lt;/p&gt;
&lt;p&gt;It was at this point that I started seeing the intertwining nature of some of the use
cases.  An indented code block cannot interrupt a paragraph, but a fenced code block
can.  So when looking for the indented code block, I had to explicitly disallow one
from starting if the block currently being process was a paragraph.  While this was
only a small case, it became very obvious to me from a quick scan over the specification
that this type of pattern was going to repeat more than once.  As such, I started
moving the start and stop logic into their own functions, whether they required it or
not.  This improved the readability, and enabled me to get a better view on what was
being handled and where.&lt;/p&gt;
&lt;p&gt;At this point, the parser was capable of handling the following Markdown elements:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;Markdown&lt;/span&gt;

&lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;captured&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;paragraph&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="o"&gt;```&lt;/span&gt;&lt;span class="n"&gt;Python&lt;/span&gt;
    &lt;span class="n"&gt;rt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="ss"&gt;"1:"&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;```&lt;/span&gt;

&lt;span class="n"&gt;But&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;also&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;
&lt;span class="c1"&gt;-------------------&lt;/span&gt;

    &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="n"&gt;block&lt;/span&gt;

&lt;span class="o"&gt;***&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please note that the fenced code block specifies &lt;code&gt;python&lt;/code&gt; as it’s type, allowing the
colorization of the text with the assumption that the code block is Python code.&lt;/p&gt;
&lt;h3 id="group-4-stopping-at-a-good-place"&gt;Group 4: Stopping At a Good Place&lt;a class="headerlink" href="#group-4-stopping-at-a-good-place" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Sometimes it makes sense to march forward without much attention to the surroundings,
and sometimes it makes sense to stop at a good place along the way.  In taking a quick
look at HTML blocks, I figured they were going to be tricky, and I had the same
determination with the table element.  Taking a look at the link reference definitions,
I noticed that they required inline expansion of text within the blocks, something that
I wasn’t even remotely close to yet.  These three leaf blocks were in the final group:
the To Be Done Later group.&lt;/p&gt;
&lt;p&gt;To ensure that I had a good place to come back to when I was ready for the each of these
blocks, I made sure to go through and implement, verify, and then disable each
test for every leaf block.&lt;/p&gt;
&lt;p&gt;Depending on the leaf block, I handled the disabling of the tests differently. To
properly deal with the link reference definitions, I needed the inline processing
capabilities that I knew were many weeks away.  As such, I kept those tests disabled
in the previous documented way of using the &lt;code&gt;@pytest.mark.skip&lt;/code&gt; annotation.  This was
a big shout out to myself that these were going to need to be completed after almost
everything else.&lt;/p&gt;
&lt;p&gt;In the case of any other of the leaf node tests, I captured the
current tokens emitted for that case and placed them in the corresponding test.  While
it might seem weird, my belief was that by testing each test case this way, I would
increase overall coverage and possibly hit edge cases not currently documented in an
use case.  It also meant that once I started implementing the HTML blocks and table
blocks, those tests would just start failing in predictable fashion.&lt;/p&gt;
&lt;h2 id="what-was-my-experience-so-far"&gt;What Was My Experience So Far?&lt;a class="headerlink" href="#what-was-my-experience-so-far" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It is always easier to look back and see what worked and what did not work, than to
observe it at the time.  With only a few issues, I personally felt like I dodged a
lot of pain due to the specification and planning.  While BNF grammars are easy to
implement, the general rule is to “be strict in what you generate and lenient in what
you accept”.  As such, coming up with “valid” parse cases is a task that takes a long
time to complete.  By having the acceptable test cases as part of the core
specification, the time that I would normally spend in the development and testing phase
was greatly reduced.  True, it took me a while to get used to it, but when I did, it
just worked and worked well.&lt;/p&gt;
&lt;p&gt;One of the practices that I engaged in during the development of the parser is to
liberally spread around &lt;code&gt;print&lt;/code&gt; statements as I went.  As I was adding these statements,
my dominant thought was to collect enough information to determine which pieces of
information were the most relevant for log messages to be added later.  However,
as I proceeded, that information also had the additional benefits of being immensely
helpful to debug any parsing issues, and indispensable in the verification of the code
itself.  While I know I need to remove those statements or convert them before the
project is completed, their presence is indeed beneficial.&lt;/p&gt;
&lt;p&gt;All in all, I think I had a great start to an interesting project and learned a bit
in the process… and learning is always good!&lt;/p&gt;
&lt;h2 id="what-is-next"&gt;What is Next?&lt;a class="headerlink" href="#what-is-next" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Next up on the list is adding block quote and list support to the parser.  Stay tuned!&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:stringFiddle"&gt;
&lt;p&gt;I remember this term being used all the way back to my university days.  The closest I have been able to come to a definition is the Oxford dictionary’s definition: touch or fidget with something in a restless or nervous way.  Perhaps this is alluding to amount of work to get most string operations “just right”? &lt;a class="footnote-backref" href="#fnref:stringFiddle" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Software Quality"></category><category term="markdown linter"></category></entry><entry><title>Clarity Through The Summarizing of Test Measurements</title><link href="https://jackdewinter.github.io/2020/01/20/clarity-through-the-summarizing-of-test-measurements/" rel="alternate"></link><published>2020-01-20T00:00:00-08:00</published><updated>2020-01-20T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2020-01-20:/2020/01/20/clarity-through-the-summarizing-of-test-measurements/</id><summary type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As part of the process of
&lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;creating a Markdown Linter&lt;/a&gt;
to use with my personal website, I firmly believe that it is imperative that I have
solid testing on the linter and the tools necessary to test the linter.  In previous
articles, I talked about the framework I use …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As part of the process of
&lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;creating a Markdown Linter&lt;/a&gt;
to use with my personal website, I firmly believe that it is imperative that I have
solid testing on the linter and the tools necessary to test the linter.  In previous
articles, I talked about the framework I use to
&lt;a href="https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/"&gt;scenario test Python scripts&lt;/a&gt; and
how my current PyTest setup
&lt;a href="https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/"&gt;produces useful test reports&lt;/a&gt;,
both human-readable and machine-readable.  These two things allow me to properly
test my Python scripts, to collect information on the tests used to verify those
scripts, and to determine how well the collection of tests covers those scripts.&lt;/p&gt;
&lt;p&gt;While the human-readable reports are very useful for digging into issues, I often find
that I need a simple and concise “this is where you are now” summary that gives me the
most pertinent information from those reports.  Enter the next tool in my toolbox, a
Python script that summarizes information from the machine-readable reports,
unimaginatively called &lt;code&gt;PyScan&lt;/code&gt;.  While it is simple tool, I constantly use this tool
when writing new Python scripts and their tests to ensure the development is going in
the direction that I want to.  This article describes how I use the tool and how it
provides a benefit to my development process.&lt;/p&gt;
&lt;h2 id="why-not-discuss-the-script-itself"&gt;Why Not Discuss The Script Itself?&lt;a class="headerlink" href="#why-not-discuss-the-script-itself" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When coming up with the idea for this article, I had two beneficial paths
available: focus on the code behind the PyScan tool or focus on the usage of the PyScan
tool.  Both paths have merit and benefit,
and both paths easily provide enough substance for a full article.  After a lot of
thought, I decided to focus on the usage of this tool instead of the code itself.  I
made this decision primarily due to my heavy use of the PyScan tool and it’s
significant benefit to my development process.&lt;/p&gt;
&lt;p&gt;I rely on the PyScan to give me an accurate summary of the tests used to verify any
changes along with the impact on code coverage for each of those changes.  While I
can develop without PyScan, I find that using PyScan immediately increases my
confidence in each change I make.  When I make a given type of change to either the
source code or the test code, I expect a related side-effect to appear in the test
results report and the test coverage report.  By having PyScan produce summaries of the
test results and test coverage, each side-effect is more visible, therefore
adding validation that the changes made are the right changes.&lt;/p&gt;
&lt;p&gt;In the end, the choice became an easy one: focus on the choice with the most positive
impact.  I felt that documenting how I use this tool satisfied that requirement with
room to spare.  I also felt that if any readers are still interested in looking at the
code behind the script, it’s easy enough to point them to the project’s
&lt;a href="https://github.com/jackdewinter/pyscan"&gt;GitHub repository&lt;/a&gt; and make sure it is well
documented.&lt;/p&gt;
&lt;h2 id="setting-up-pyscan-for-its-own-project"&gt;Setting Up PyScan For It’s Own Project&lt;a class="headerlink" href="#setting-up-pyscan-for-its-own-project" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Based on the setup from
&lt;a href="https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/"&gt;the last article&lt;/a&gt;, the PyTest command
line options &lt;code&gt;--junitxml=report/tests.xml&lt;/code&gt; and &lt;code&gt;--cov-report xml:report/coverage.xml&lt;/code&gt;
place the &lt;code&gt;tests.xml&lt;/code&gt; file and the &lt;code&gt;coverage.xml&lt;/code&gt; file in the &lt;code&gt;report&lt;/code&gt; directory.
Based on observation, the &lt;code&gt;tests.xml&lt;/code&gt; file is in a JUnit XML format and the
&lt;code&gt;coverage.xml&lt;/code&gt;
file is in a Cobertura XML format.  The format of the &lt;code&gt;tests.xml&lt;/code&gt; is pretty obvious from
the command line flag required to generate it.  The format of the &lt;code&gt;coverage.xml&lt;/code&gt; file
took a bit more effort, but the following line of the file keyed me to it’s format:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;&amp;lt;!-- Based on https://raw.githubusercontent.com/cobertura/web/master/htdocs/xml/coverage-04.dtd --&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From within the project’s root directory, the main script is located at &lt;code&gt;../main.py&lt;/code&gt;.
Since the project uses &lt;code&gt;pipenv&lt;/code&gt;, the command line to invoke the script is
&lt;code&gt;pipenv run python pyscan/main.py&lt;/code&gt; and invoking the script with the &lt;code&gt;--help&lt;/code&gt; option
gives us the options that we can use.  Following the information from the help text,
the command line that I use from the project’s root directory is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pipenv run python pyscan/main.py --junit report/tests.xml --cobertura report/coverage.xml
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With everything set up properly, the output from that command looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Class Name                     Total Tests   Failed Tests   Skipped Tests
----------------------------  ------------  -------------  --------------
test.test_coverage_profiles              2              0               0
test.test_coverage_scenarios            12              0               0
test.test_publish_scenarios              9              0               0
test.test_results_scenarios             19              0               0
test.test_scenarios                      1              0               0
---                                     --              -               -
TOTALS                                  43              0               0

Test Coverage Summary
---------------------

Type           Covered   Measured   Percentage
------------  --------  ---------  -----------
Instructions       ---        ---        -----
Lines              505        507        99.61
Branches           158        164        96.34
Complexity         ---        ---        -----
Methods            ---        ---        -----
Classes            ---        ---        -----
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="before-we-continue"&gt;Before We Continue…&lt;a class="headerlink" href="#before-we-continue" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To complete my setup, there are two more things that are needed.  The first thing is
that I primarily execute the tests from a simple Windows script called &lt;code&gt;ptest.cmd&lt;/code&gt;.
While there is a lot of code in the &lt;code&gt;ptest.cmd&lt;/code&gt; script to handle errors and options,
when the script is boiled down to it’s bare essence, the script runs tests and reports
on those tests as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pipenv run pytest
pipenv run python pyscan/main.py --only-changes --junit report/tests.xml --cobertura=report/coverage.xml
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Note&lt;/p&gt;
&lt;p&gt;I also have a Bash version called &lt;code&gt;ptest.sh&lt;/code&gt; which I have experimented with locally, but is not checked in to the project.  If you are interested in this script, please let me know in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Setting up a script like &lt;code&gt;ptest&lt;/code&gt; keeps things simple and easy-to-use.  One
notable part of the script is that there is a little bit of logic in the script to not
summarize any coverage if there are any issues running the tests under PyTest.  Call me
a purist, but if the tests fail to execute or are not passing, any
measurements of how well the tests cover the code are moot.&lt;/p&gt;
&lt;p&gt;The other thing that I have setup is a small change to the command line for PyScan.  In
the “bare essence” text above, after the text &lt;code&gt;pyscan/main.py&lt;/code&gt;, there is a new option
used for PyScan: the &lt;code&gt;--only-changes&lt;/code&gt; option.  By adding the &lt;code&gt;--only-changes&lt;/code&gt; option,
PyScan restricts the output to only those items that show changes.  If no changes are
detected, it displays a simple line stating that no changes have been observed.  In the
case of the above output, the output with this new option is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Test coverage has not changed since last published test coverage.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To me, this gives a very clear indication that things have not changed.  In the
following sections, I go through different cases and explain what changes I made and
what effects I expect to see summarized.&lt;/p&gt;
&lt;h2 id="introducing-changes-and-observing-behavior"&gt;Introducing Changes and Observing Behavior&lt;a class="headerlink" href="#introducing-changes-and-observing-behavior" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For this section of the article, I temporarily added a “phantom” feature called
“nothing” to PyScan.  This feature is facilitated by two code changes.
In the &lt;code&gt;__parse_arguments&lt;/code&gt; function, I added the following code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;"--nothing"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;dest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"do_nothing"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"store_true"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"only_changes"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and in the &lt;code&gt;main&lt;/code&gt; function, I changed the code as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;        &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__parse_arguments&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;do_nothing&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"noop"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that this feature is only present for the sake of these examples, and is not in
the project’s code base.&lt;/p&gt;
&lt;h3 id="adding-new-code"&gt;Adding New Code&lt;a class="headerlink" href="#adding-new-code" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;When I added the above code for the samples, the output that I got after running
the tests was:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     507 (+2)   511 (+4)  99.22 (-0.39)
Branches  159 (+1)   166 (+2)  95.78 (-0.56)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Based on the introduced changes, this output was expected.  In the &lt;code&gt;Measured&lt;/code&gt; column,
4 new lines were added (1 in &lt;code&gt;__parse_arguments&lt;/code&gt; and 3 in &lt;code&gt;main&lt;/code&gt;) and the
&lt;code&gt;if args.do_nothing:&lt;/code&gt; line added 2 branches (1 for True and one for False). In the
&lt;code&gt;Covered&lt;/code&gt; column, without any tests to exercise the new code, 2 lines are
covered by default (1 in &lt;code&gt;__parse_arguments&lt;/code&gt; and 1 in &lt;code&gt;main&lt;/code&gt;) and 1 branch is covered
by default (the False case of &lt;code&gt;if args.do_nothing:&lt;/code&gt;).&lt;/p&gt;
&lt;h3 id="adding-a-new-test"&gt;Adding a New Test&lt;a class="headerlink" href="#adding-a-new-test" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Having added source code to the project, I added a test to address the new code.  To
start, I added this simple test function to the &lt;code&gt;test_scenarios.py&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_nothing&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This change is just a stub for a test function, so the expected change is that the
number of tests for that module increase and there is no change in coverage.  This
effect is born out by the output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Class Name            Total Tests   Failed Tests   Skipped Tests
-------------------  ------------  -------------  --------------
test.test_scenarios        2 (+1)              0               0
---                       --                   -               -
TOTALS                    44 (+1)              0               0

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     507 (+2)   511 (+4)  99.22 (-0.39)
Branches  159 (+1)   166 (+2)  95.78 (-0.56)
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="populating-the-test-function"&gt;Populating the Test Function&lt;a class="headerlink" href="#populating-the-test-function" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now that a stub for the test is in place and registering, I added a real body to the
test function as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_nothing&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;

    &lt;span class="c1"&gt;# Arrange&lt;/span&gt;
    &lt;span class="n"&gt;executor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MainlineExecutor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;suppplied_arguments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"--nothing"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;expected_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"""noop&lt;/span&gt;
&lt;span class="s2"&gt;"""&lt;/span&gt;
    &lt;span class="n"&gt;expected_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;
    &lt;span class="n"&gt;expected_return_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="c1"&gt;# Act&lt;/span&gt;
    &lt;span class="n"&gt;execute_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invoke_main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;suppplied_arguments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Assert&lt;/span&gt;
    &lt;span class="n"&gt;execute_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assert_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;expected_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_return_code&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The code that I added at the start of this section is triggered by the command line
argument &lt;code&gt;--nothing&lt;/code&gt;, printing the simple response text &lt;code&gt;noop&lt;/code&gt;, and returning a return
code of 1 .  This test code was crafted to trigger that code and to verify the expected
output.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Class Name            Total Tests   Failed Tests   Skipped Tests
-------------------  ------------  -------------  --------------
test.test_scenarios        2 (+1)              0               0
---                       --                   -               -
TOTALS                    44 (+1)              0               0

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     509 (+4)   511 (+4)  99.61 ( 0.00)
Branches  160 (+2)   166 (+2)  96.39 (+0.04)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Based on the output from the test results summary, the test does verify that once
triggered, the code is working as expected.  If there was any issue with the test,
the summary would include the text &lt;code&gt;1 (+1)&lt;/code&gt; in the &lt;code&gt;Failed Tests&lt;/code&gt; column to denote
the failure.  As that text is not present, it is safe to assume that both tests in
the &lt;code&gt;test.test_scenarios&lt;/code&gt; module succeeded.  In addition, based on the output from the
test coverage summary, the new code added 4 lines and 2 branches to the code base, and
the new test code covered all of those changes.&lt;/p&gt;
&lt;h3 id="establishing-a-new-baseline"&gt;Establishing a New Baseline&lt;a class="headerlink" href="#establishing-a-new-baseline" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;With the new source code and test code in place, I needed to publish the results and
set a new baseline for the project.  To do this with the &lt;code&gt;ptest&lt;/code&gt; script, I invoked the
following command line:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ptest -p
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Within this &lt;code&gt;ptest&lt;/code&gt; script, the &lt;code&gt;-p&lt;/code&gt; option was translated into the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pipenv run python pyscan/main.py --publish
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When executed, the &lt;code&gt;publish/coverage.json&lt;/code&gt; and &lt;code&gt;publish/test-results.json&lt;/code&gt; files were
updated with the current summaries.  Following that point, when the script was run, it
reverts back to the original output of:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Test coverage has not changed since last published test coverage.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This process can be repeated at any time to establish a solid baseline that any new
changes can be measured against.&lt;/p&gt;
&lt;h3 id="refactoring-code-my-refactoring-process"&gt;Refactoring Code - My Refactoring Process&lt;a class="headerlink" href="#refactoring-code-my-refactoring-process" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In practice, I frequently do “cut-and-paste” development during my normal development
process.  However, I do this with a strict rule that I follow: “2 times on the
fence, 3 times refactor, clean up later”.  That rule break down as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if I cut-and-paste code once, I then have 2 copies, and I should consider refactoring unless I have a good reason to delay&lt;/li&gt;
&lt;li&gt;if I cut-and-paste that code again, I then have 3 copies, and that third copy must be into a function that the other 2 copies get merged into&lt;/li&gt;
&lt;li&gt;when I have solid tests in place and I am done with primary development, go back
to all of the cases where I have 2 copies and condense them if beneficial&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My rationale for this rule is as follows.&lt;/p&gt;
&lt;p&gt;When you are creating code, you want the
ideas to flow free and fast, completing a good attempt at meeting your current goal
in the most efficient way possible.  While cut-and-paste as a long term strategy is not
good, I find that in the short term, it helps me in creating a new function, even if
that function is a copy of something done before.  To balance that, from experience, if
I have pasted the same code twice (meeting the criteria for “3 times refactor”), there
is a very good chance that I will use that code at least one more time, if not more.  At
that point, it makes more sense to refactor the code to encapsulate the functionality
properly before the block of code becomes to unwieldly.&lt;/p&gt;
&lt;p&gt;Finally, once I have completed the creation of the new source code, I go back and
actively look for cases where I cut-and-pasted code, and if it is worth it to refactor
that code, with a decision to refactor if I am on the fence.  At the very least,
refactoring code into a function almost always makes the code more readable and
maintainable.  Basically, by following the above rule for refactoring, I almost always
change the code in a positive manner.&lt;/p&gt;
&lt;p&gt;The summaries provided to me from PyScan help me with this refactoring in a big way.
Most of the time, the main idea with refactoring is to change the code on the “inside”
of the program or script without changing the “outside” of the program or script.  If
any changes are made to the “outside”, they are usually small changes with very
predictable impacts.  The PyScan summaries assist me in ensuring that any changes to the
outside of the script are kept small and manageable while also measuring the
improvements made to the inside of the script.  Essentially, seeing both summaries
helps me keep the code refactor of the script very crisp and on course.&lt;/p&gt;
&lt;h3 id="refactoring-code-leveraging-the-summaries"&gt;Refactoring Code - Leveraging The Summaries&lt;a class="headerlink" href="#refactoring-code-leveraging-the-summaries" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A good function set of functions for me to look at for clean-up refactoring were the
&lt;code&gt;generate_test_report&lt;/code&gt; and &lt;code&gt;generate_coverage_report&lt;/code&gt; functions.  When I wrote those
two functions, I wasn’t sure
how much difference I was going to have between those two functions, so did an initial
cut-and-paste (see “2 times on the fence”) and started making changes.  As those parts
of PyScan are now solid and tested, I went back (see “clean up later”) and compared
the two functions to see what was safe to refactor.&lt;/p&gt;
&lt;p&gt;The first refactor I performed was to extract the xml loading logic into a new
&lt;code&gt;__load_xml_docment&lt;/code&gt; function.  While I admit I didn’t get it right the first time, the
tests kept me in
check and made sure that, after a couple of tries, I got it right.  And when I say
“tries”, I mean that I made a change, ran &lt;code&gt;ptest&lt;/code&gt;, got some information, and diagnosed
it… all within about 30-60 seconds per iteration.  In the end, the summary looked like
this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Type        Covered   Measured     Percentage
--------  ---------  ---------  -------------
Lines     499 (-10)  501 (-10)  99.60 (-0.01)
Branches  154 ( -6)  160 ( -6)  96.25 (-0.14)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As expected, the refactor eliminated both lines of code and branches, with the measured
values noted in the summary.&lt;/p&gt;
&lt;p&gt;The second refactor I made was to extract the summary file writing logic into a new
&lt;code&gt;__save_summary_file&lt;/code&gt; function.  I followed a similar pattern to the refactor for
&lt;code&gt;__load_xml_docment&lt;/code&gt;, but there was a small difference.  In this case, I observed that
for a specific error case, one function specified &lt;code&gt;test coverage&lt;/code&gt; and the other function
specified &lt;code&gt;test summary&lt;/code&gt;.  Seeing as consistent names in output is always beneficial,
I decided to change the error messages to be consistent with each other.  The
&lt;code&gt;test coverage&lt;/code&gt; name for the first function remained the same, but the &lt;code&gt;test summary&lt;/code&gt;
name was changed to &lt;code&gt;test report&lt;/code&gt;, with the text &lt;code&gt;summary&lt;/code&gt; added in the refactored
function.&lt;/p&gt;
&lt;p&gt;At this point, I knew that one test for each of the test results scenarios and test
coverage scenarios was going to fail, but I knew that it would fail in a very specific
manner.  Based on the above changes, the text &lt;code&gt;Project test summary file&lt;/code&gt; for the
results scenario test should change to &lt;code&gt;Project test report summary file&lt;/code&gt; and the text
&lt;code&gt;Project test coverage file&lt;/code&gt; for the coverage scenario test should change to
&lt;code&gt;Project test coverage summary file&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When I ran the tests after these changes, there were indeed 2 errors, specifically
in the tests I thought they would show up in.  Once those 2 tests were changed to
reflect the new consistent text, the tests were ran again and produced the following
output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Test results have not changed since last published test results.

Test Coverage Summary
---------------------

Type        Covered   Measured     Percentage
--------  ---------  ---------  -------------
Lines     491 (-18)  493 (-18)  99.59 (-0.01)
Branches  152 ( -8)  158 ( -8)  96.20 (-0.18)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once again, the output matched my expectations.  While it wasn’t a large number of code
or branches, an additional 8 lines and 2 branches were refactored.&lt;/p&gt;
&lt;h3 id="determining-additive-test-function-coverage"&gt;Determining Additive Test Function Coverage&lt;a class="headerlink" href="#determining-additive-test-function-coverage" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are times after I have written a series of tests where I wonder how much actual
coverage a given test contributes to the overall test coverage percentage. As test
coverage is a collaborative effort of all of the tests, a single number that identifies
the amount of code covered by a single test is not meaningful.  However, a meaningful
piece of information is what unique coverage a given test contributes to the collection
of tests as a whole.&lt;/p&gt;
&lt;p&gt;To demonstrate how I do this, I picked one of the tests that addresses one of the error
conditions, the &lt;code&gt;test_summarize_cobertura_report_with_bad_source&lt;/code&gt; function in the
&lt;code&gt;test_coverage_scenarios.py&lt;/code&gt; file.  Before I
changed anything, I made sure to publish the current state to use it as a baseline. To
determine the additive coverage this test provides, I simply changed it’s name to
&lt;code&gt;xtest_summarize_cobertura_report_with_bad_source&lt;/code&gt;.  As the &lt;code&gt;pytest&lt;/code&gt; program only
matches on functions that start with &lt;code&gt;test_&lt;/code&gt;, the function was then excluded from the
tests to be executed.&lt;/p&gt;
&lt;p&gt;Upon running the &lt;code&gt;ptest&lt;/code&gt; script, I got the following output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Class Name                     Total Tests   Failed Tests   Skipped Tests
----------------------------  ------------  -------------  --------------
test.test_coverage_scenarios       11 (-1)              0               0
---                                --                   -               -
TOTALS                             43 (-1)              0               0

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     507 (-2)        511  99.22 (-0.39)
Branches  159 (-1)        166  95.78 (-0.60)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Interpreting this output, given what I documented earlier in this article, was pretty
easy.  As I “disabled”
one of the coverage scenario tests in the &lt;code&gt;test_coverage_scenarios.py&lt;/code&gt; file, the summary
reports one less test in &lt;code&gt;test.test_coverage_scenarios&lt;/code&gt; as expected.  That disabled
test added 2 lines of coverage and 1 branch of coverage to overall effort, coverage
that was now being reported as missing.  As this test was added specifically to test a
single error case, this was expected.&lt;/p&gt;
&lt;p&gt;If instead I disable the &lt;code&gt;xtest_junit_jacoco_profile&lt;/code&gt; test in the
&lt;code&gt;test_coverage_profiles.py&lt;/code&gt; file, I get a different result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Test Results Summary
--------------------

Class Name                    Total Tests   Failed Tests   Skipped Tests
---------------------------  ------------  -------------  --------------
test.test_coverage_profiles        1 (-1)              0               0
---                               --                   -               -
TOTALS                            43 (-1)              0               0

Test Coverage Summary
---------------------

Type       Covered   Measured     Percentage
--------  --------  ---------  -------------
Lines     501 (-8)        511  98.04 (-1.57)
Branches  152 (-8)        166  91.57 (-4.82)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Like the previous output, the disabled test is showing up as being removed, but there
is a lot more coverage that was removed.  Strangely enough, this was also expected.  As
I also use PyScan to summarize test results from Java projects I work on, I used all 6
coverage measurements available from Jacoco &lt;sup id="fnref:jacoco"&gt;&lt;a class="footnote-ref" href="#fn:jacoco"&gt;1&lt;/a&gt;&lt;/sup&gt; as a baseline for the 2
measurements generated by PyTest for Python coverage.  With a quick look at the
&lt;code&gt;report/coverage/pyscan_model_py.html&lt;/code&gt; file, this was indeed the reason for the
difference, with the test exercising 4 additional paths in each of the serialization
and deserialization functions. Basically, four paths of one line each, times two (one
for serialization and one for deserialization), and the 8 lines/branches covered is
explained.&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping Up&lt;a class="headerlink" href="#wrapping-up" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I believe that making my decision to talk about how I use my PyScan tool to summarize
test results and test coverage was the right choice.  It is difficult for me to
quantize exactly how much benefit PyScan has provided to my development process, but it
is easily in the very positive to indispensable category.  By providing a quick summary
on the test results file and the test coverage file, I can ensure that any changes I
make are having the proper effects on those two files at each stage of the change that
I am making.  I hope that by walking through this process and how it helps me, it will
inspire others to adopt something similar in their development processes.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:jacoco"&gt;
&lt;p&gt;For an example Jacoco HTML report that shows all 6 coverage measurements, check out &lt;a href="https://www.jacoco.org/jacoco/trunk/coverage/"&gt;the report trunk coverage for Jacoco&lt;/a&gt;. &lt;a class="footnote-backref" href="#fnref:jacoco" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Software Quality"></category><category term="pytest"></category><category term="scenario testing"></category></entry><entry><title>Measuring Testing in Python Scripts</title><link href="https://jackdewinter.github.io/2020/01/13/measuring-testing-in-python-scripts/" rel="alternate"></link><published>2020-01-13T00:00:00-08:00</published><updated>2020-01-13T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2020-01-13:/2020/01/13/measuring-testing-in-python-scripts/</id><summary type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As part of the process of
&lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;creating a Markdown Linter&lt;/a&gt;
to use with my personal website, I firmly believe that it is imperative that I have
solid testing on that linter and the tools necessary to test the linter.  In my
previous article on
&lt;a href="https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/"&gt;Scenario Testing Python Scripts&lt;/a&gt;,
I …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As part of the process of
&lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;creating a Markdown Linter&lt;/a&gt;
to use with my personal website, I firmly believe that it is imperative that I have
solid testing on that linter and the tools necessary to test the linter.  In my
previous article on
&lt;a href="https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/"&gt;Scenario Testing Python Scripts&lt;/a&gt;,
I described the in-process framework that I use for testing Python scripts from within
PyTest.  That framework ensures that I can properly test Python scripts from the
start of the script, increasing my confidence that they are tested properly.&lt;/p&gt;
&lt;p&gt;To properly figure out how my tests are doing and what their impact is, I turned on a
number of features that are available with PyTest.  The features either make testing
easier or measure the impact of those tests and relay that information. This article
describes my PyTest configuration and how that configuration provides a benefit to my
development process.&lt;/p&gt;
&lt;h2 id="adding-needed-packages-to-pytest"&gt;Adding Needed Packages to PyTest&lt;a class="headerlink" href="#adding-needed-packages-to-pytest" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There are four main Python packages that I use in conjunction with PyTest.  The
&lt;code&gt;pytest-console-scripts&lt;/code&gt; package is the main one, allowing PyTest to be invoked
from the command line.  Since I am in favor of automating process where possible, this
is a necessity.  From a test execution point of view, the &lt;code&gt;pytest-timeout&lt;/code&gt; is
used to set a timeout on each test, ensuring that a single runaway test does not cause
the set of tests to fail to complete.  For reporting, the &lt;code&gt;pytest-html&lt;/code&gt; package is
useful for creating an HTML summary of the test results.  The &lt;code&gt;pytest-cov&lt;/code&gt; package adds
coverage of the source code, with reporting of that coverage built in.  I have found
that all of these packages help me in my development of Python scripts, so I highly
recommend these packages.&lt;/p&gt;
&lt;p&gt;Depending on the Python package manager and environment in use, there will be slightly
different methods to install these packages.  For plain Python this is usually:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install pytest-console-scripts&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.20 pytest-cov&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.8.1 pytest-timeout&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.3.3 pytest-html&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.0.1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As I have used &lt;code&gt;pipenv&lt;/code&gt; a lot in my professional Python development, all of my personal
projects use it for setting up the environment and it’s dependencies.  Similar to the
line above, to install these packages into &lt;code&gt;pipenv&lt;/code&gt; requires executing the following
line in the project’s directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pipenv install pytest-console-scripts&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;.20 pytest-cov&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.8.1 pytest-timeout&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.3.3 pytest-html&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;.0.1
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="configuring-pytest-for-those-packages"&gt;Configuring PyTest For Those Packages&lt;a class="headerlink" href="#configuring-pytest-for-those-packages" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Unless information is provided on the command line, PyTest will search for a
configuration file to use.  By default, &lt;code&gt;setup.cfg&lt;/code&gt; is the name of the configuration
file it uses.  The following fragment of my &lt;code&gt;setup.cfg&lt;/code&gt; file takes care of the
configuration for those PyTest packages.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[tool:pytest]
testpaths=./test
cache_dir=./build/test/.pytest_cache
junit_family=xunit2
addopts=--timeout=10 --cov --cov-branch --cov-fail-under=90 --strict-markers -ra --cov-report xml:report/coverage.xml --cov-report html:report/coverage --junitxml=report/tests.xml --html=report/report.html
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While all configuration is important, the following sections are most important in the
setting up of PyTest for measuring the effects of testing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;testpaths=./test&lt;/code&gt; - relative path where PyTest will scan for tests&lt;/li&gt;
&lt;li&gt;&lt;code&gt;addopts/--junitxml&lt;/code&gt; - creates a junit-xml style report file at given path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;addopts/--cov&lt;/code&gt; - record coverage information for everything&lt;/li&gt;
&lt;li&gt;&lt;code&gt;addopts/--cov-branch&lt;/code&gt; - enables branch coverage&lt;/li&gt;
&lt;li&gt;&lt;code&gt;addopts/--cov-report&lt;/code&gt; - types of report to generate and their destination paths&lt;/li&gt;
&lt;li&gt;&lt;code&gt;default/--cov-config&lt;/code&gt; - configuration file for coverage, defaulting to &lt;code&gt;.coveragerc&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order, the first two configuration items tells PyTest where to look for tests to
execute and where to place the JUnit-styled XML report with the results of each test.
The next three configuration items turn on coverage collection, enable
branch coverage, and specifies what types of coverage reports to produce and where to
place them.  Finally, because the &lt;code&gt;--cov-config&lt;/code&gt; is not set, the default location for
the coverage configuration file is set to &lt;code&gt;.coveragerc&lt;/code&gt;.  &lt;/p&gt;
&lt;p&gt;For all of my projects, the default &lt;code&gt;.coveragerc&lt;/code&gt; that I use, with a small change to
the &lt;code&gt;source=&lt;/code&gt; line is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[run]
source = pyscan

[report]
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover

    # Don't complain about missing debug-only code:
    def __repr__
    if self\.debug

    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run:
    if 0:
    if __name__ == .__main__.:
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To be honest, this &lt;code&gt;.coveragerc&lt;/code&gt; template is something I picked up somewhere, but it
works, and works well for my needs.  The exclude lines work in all case that I have
come across, so I haven’t touched them in the 2+ years that I have been writing code in
Python.&lt;/p&gt;
&lt;h2 id="benefits-of-this-configuration"&gt;Benefits Of This Configuration&lt;a class="headerlink" href="#benefits-of-this-configuration" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Given the setup from the last section, there are two main benefits that I get from this
setup.  The first benefit is machine readable XML information generated for the test
results and the test coverage.  While this is not immediately consumable in it’s
current form, that data can be harvested in the future to provide concise information
about what has been tested.&lt;/p&gt;
&lt;p&gt;The second benefit is to provide human readable information about the tests that have
been executed.  The HTML file located at &lt;code&gt;report/report.html&lt;/code&gt; relays the results of the
last series of tests while the HTML file located at &lt;code&gt;report/coverage/index.html&lt;/code&gt; relays
the coverage information for the last series of tests.  Both of these pieces of
information are useful for different reasons.&lt;/p&gt;
&lt;p&gt;In the case of the test results HTML, the information presented on the test results page
is mostly the same information as is displayed by PyTest when executed on the command
line.  Some useful changes are present, such as seeing all of the test information at
once, instead of just a &lt;code&gt;.&lt;/code&gt; for a successful test, a &lt;code&gt;F&lt;/code&gt; for a failed test, and so on.
I have found that having this information available on one page allows me to more
quickly debug an issue that is affecting multiple tests, instead of scrolling through
the command line output one test at a time.&lt;/p&gt;
&lt;p&gt;In the case of the test coverage HTML, the information presented on this page is
invaluable.  For each source file in the Python project being tested, there is a page
that clearly shows which lines of each Python script are exercised by the tests,  By
using these pages as a guide, I can determine what tests I need to add to ensure that
the scripts are properly covered.&lt;/p&gt;
&lt;p&gt;By using these two tools together, I can quickly determine what tests to add, and when
tests fail, I can determine why they failed and look for patterns in the failures.  This
enables me to quickly figure out where the blind spots are in my testing, and to address
them quickly.  This in turn can help me to figure out the best way to improve the
quality of the project I am working on.&lt;/p&gt;
&lt;p&gt;If this finds an issue with an existing requirement, that requirement can be adjusted
or a new requirement added to fulfil the deficiency.  If the requirements were all
right and the code it was testing was incorrect, that code can be addressed.  If
the coverage page shows that code was written but not tested, a new test function can
be introduced to cover that scenario.  Each observation and its appropriate action
work to improve the quality of the software project.&lt;/p&gt;
&lt;h2 id="what-was-accomplished"&gt;What Was Accomplished&lt;a class="headerlink" href="#what-was-accomplished" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This article showed how to setup PyTest using a configuration file.  With that
configuration file, it was set up to provide timeouts for tests, provide output on
the test results, and provide a coverage report of how well the tests covered the
scripts under test.  This was all accomplished to better understand the impact of tests
on a project and provide better information on how they succeed (test coverage) or fail
(test results).  By understanding this information, the quality of the software
can be measured and improved on if needed.&lt;/p&gt;
&lt;h2 id="what-is-next"&gt;What Is Next?&lt;a class="headerlink" href="#what-is-next" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the next article, I will briefly describe the PyScan tool I have written, and how it
takes the XML information generate by the &lt;code&gt;--junitxml=report/tests.xml&lt;/code&gt; option and the
&lt;code&gt;--cov-report xml:report/coverage.xml&lt;/code&gt; option and produces concise summaries of that
information.  I will also give a number of examples of how I use this information during
my development of Python projects.&lt;/p&gt;</content><category term="Software Quality"></category><category term="pytest"></category><category term="scenario testing"></category></entry><entry><title>Scenario Testing Python Scripts</title><link href="https://jackdewinter.github.io/2020/01/06/scenario-testing-python-scripts/" rel="alternate"></link><published>2020-01-06T00:00:00-08:00</published><updated>2020-01-06T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2020-01-06:/2020/01/06/scenario-testing-python-scripts/</id><summary type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As part of the process of
&lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;creating a Markdown Linter&lt;/a&gt;
to use with my personal website, I firmly believe that it is imperative that I have
solid testing on that linter and the tools necessary to test the linter.  This testing
includes executing those Python tool scripts from start …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As part of the process of
&lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;creating a Markdown Linter&lt;/a&gt;
to use with my personal website, I firmly believe that it is imperative that I have
solid testing on that linter and the tools necessary to test the linter.  This testing
includes executing those Python tool scripts from start to finish and verifying that
everything is working properly.  From my experience, one of the most efficient ways to
scenario test the project’s Python scripts is to use an in-process framework for
running Python scripts.&lt;/p&gt;
&lt;p&gt;Because of the way that Python works, it is very feasible to scenario test the Python
scripts using the in-process framework which I describe in this article.  To show
how the framework works in practice, I reference my
&lt;a href="https://github.com/jackdewinter/pyscan"&gt;PyScan project&lt;/a&gt; to
illustrate how I use this framework to test the scenarios in that project.
Specifically, I talk about the
&lt;a href="https://github.com/jackdewinter/pyscan/blob/master/test/pytest_execute.py"&gt;pytest_execute.py file&lt;/a&gt;
which contains the bulk of the code I use to write scenario tests with.&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id="determine-the-requirements"&gt;Determine the Requirements&lt;a class="headerlink" href="#determine-the-requirements" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As with most of my projects, the first thing I do for any new project is to cleanly
determine and document the requirements for the project.  Even though this project is
a single component used to test the tools and other components, I feel strongly that it
is still important to follow those guidelines to ensure the right component is built in
the right way.&lt;/p&gt;
&lt;p&gt;The basic requirements are pretty easy to define for this in-process test component:
execute the Python script independently and capture all relevant information about it’s
execution, verifying that information against expected values.  The devil is in the
details however.  I believe that a good definition of “execute the Python script” must
include the ability to set the current working directory and arguments for the command
line. For a good definition of “capture all relevant information”, I believe the
requirements must include capturing of the script’s return code as well as any output
to standard out (stdout) and standard error (stderr).  As this component executes the
script in-process, any attempts to exit the script prematurely must be properly
captured, and the state of the test must be returned to what it was at the beginning of
the test. Finally, to satisfy the “verifying” requirement, the component must have easy
to use comparison functions, with informative output on any differences that arise
during verification.&lt;/p&gt;
&lt;p&gt;Finding a balance between too many bulky requirements and too few lean requirements is
a tough balance to achieve.  In this case, I feel that I have achieved that balance by
ensuring all of the major parts of the requirements are specified at a high enough level
to be able to communicate clearly without ambiguity.  Here’s hoping I get the balance
right!&lt;/p&gt;
&lt;h2 id="capture-relevant-information"&gt;Capture Relevant Information&lt;a class="headerlink" href="#capture-relevant-information" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The first thing to take care of is a class that will contain the information to satisfy
the “capture all relevant information” requirement above.  As the requirement specifies
the 3 things that need to be captured, all that is left to do is to create a class to
encapsulate these variables as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;InProcessResult&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Class to provide for an encapsulation of the results of an execution.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_code&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std_out&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std_err&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;return_code&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std_out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std_out&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std_err&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std_err&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="executing-the-script"&gt;Executing the Script&lt;a class="headerlink" href="#executing-the-script" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Now that there is an object to collect the information about the script’s execution, a
simple function is needed to collect that information.  In the &lt;code&gt;InProcessExecution&lt;/code&gt;
base class, the &lt;code&gt;invoke_main&lt;/code&gt; function serves this purpose.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;invoke_main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        Invoke the mainline so that we can capture results.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;

        &lt;span class="n"&gt;saved_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SystemState&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="n"&gt;std_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;std_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;returncode&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std_output&lt;/span&gt;
            &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;std_error&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
            &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_main_name&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;chdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;execute_main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;SystemExit&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;this_exception&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;returncode&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;handle_system_exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;this_exception&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std_error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;returncode&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;handle_normal_exception&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;finally&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;saved_state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;InProcessResult&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;returncode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;std_error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Before changing any of the existing system values, changes that by their very nature
are be made across the entire Python interpreter, the original values of those system
values are kept safely in an instance of the the &lt;code&gt;SystemState&lt;/code&gt; class in the
&lt;code&gt;saved_state&lt;/code&gt; variable.  As I want to ensure that the saved system state is reverted
back to regardless of what happens, a try-finally block is used to ensure that the
&lt;code&gt;saved_state.restore&lt;/code&gt; function is called to restore the system back to it’s original
state.&lt;/p&gt;
&lt;p&gt;Once the system state is safely stored away, changes to those system values can be made.
Instances of the &lt;code&gt;StringIo&lt;/code&gt; class are used to provide alternative streams for stdout
and stderr.  A new array is assigned to &lt;code&gt;sys.argv&lt;/code&gt;, either an empty array if no
arguments are provided or a copy of the provided array if provided.  To the start of
that array is inserted the name of the main script, to ensure that libraries expecting
a properly formatted array of system arguments are happy.  Finally, if an alternate
working directory is provided to the function, the script changes to that directory.&lt;/p&gt;
&lt;p&gt;To reiterate, the reason it is acceptable to make all of these changes to the system
state is that we have a safe copy of the system state stored away that we will revert
to when this function completes.&lt;/p&gt;
&lt;p&gt;After the &lt;code&gt;execute_main&lt;/code&gt; function is called to execute the script in the specified
manner, there are three possibilities that the function needs to capture the
information for. In the case of a normal fall-through execution, the &lt;code&gt;returncode = 0&lt;/code&gt;
statement at the start of the try-finally block sets the return code.  If a
&lt;code&gt;SystemExit&lt;/code&gt; exception is thrown, the &lt;code&gt;handle_system_exit&lt;/code&gt; function does a bit of
process to figure out the return code based on the contents of the exception.  Finally,
if the execution is terminated for any other exception, the &lt;code&gt;handle_normal_exception&lt;/code&gt;
makes sure to print out decent debug information and sets the return code to 1.  In all
three cases, the collected values for stdout and stderr are collected, combined with
the return code determined earlier in this paragraph, and a new instance of the
&lt;code&gt;InProcessResult&lt;/code&gt; class is returned with these values.&lt;/p&gt;
&lt;h2 id="verifying-actual-results-against-expected-results"&gt;Verifying Actual Results Against Expected Results&lt;a class="headerlink" href="#verifying-actual-results-against-expected-results" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When I started with the &lt;code&gt;assert_results&lt;/code&gt; function, it was only 3 statements in quick
succession: 3 assert statements asserting that the actual values for stdout, stderr and
the return code matched the expected values.  However, as I started using that function,
it was quickly apparent that when something did fail, there was a certain amount of
repetitive debugging that I performed to determine why the assert was triggered.  At
first I added some extra information to the assert statements, and that worked for the
return code.  But there were still two issues.&lt;/p&gt;
&lt;p&gt;The first issue was that, in the case where all 3 expected values were different than
the actual values, it took 3 iterations of cleaning up the test before it passed.  Only
when I cleared up the first failure did I see the second failure, and only after the
second failure was dealt with did I see the third.  While this was workable, it was far
from efficient.  The second issue was that if there were any differences with the
contents of the stdout or stderr stream, the differences between the expected value and
the actual value were hard to discern by just looking at them.&lt;/p&gt;
&lt;p&gt;To address the first issue, I changed the simple &lt;code&gt;assert_results&lt;/code&gt; function to the
following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;assert_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_code&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        Assert the results are as expected in the "assert" phase.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;

        &lt;span class="n"&gt;stdout_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assert_stream_contents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"stdout"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std_out&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;stderr_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assert_stream_contents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;"stderr"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std_err&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stderr&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;return_code_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assert_return_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;return_code&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error_code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;combined_error_msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;stdout_error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;combined_error_msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;combined_error_msg&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stdout_error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;stderr_error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;combined_error_msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;combined_error_msg&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stderr_error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;return_code_error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;combined_error_msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;combined_error_msg&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;return_code_error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;combined_error_msg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;"Either stdout, stderr, or the return code was not as expected.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
            &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;combined_error_msg&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The key to resolving the first issue is in capturing the information about all
differences that occur, and then asserting only once if any differences are encountered.
To accomplish this, several comparison functions are required that capture individual
asserts and relay that information back to the &lt;code&gt;assert_results&lt;/code&gt; function where they
can be aggregated together.  It is these comparison functions that are at the heart
of the &lt;code&gt;assert_results&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The easiest
of these comparison functions is the &lt;code&gt;assert_return_code&lt;/code&gt; function, which simply
compares the actual return code and the expected return code.  If there is any
difference, the error message for the assert statement is descriptive enough to provide
a clear indication of what the difference is.  That raised &lt;code&gt;AssertionError&lt;/code&gt; is then
captured and returned from the function so the &lt;code&gt;assert_results&lt;/code&gt; function can report on
it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="nd"&gt;@classmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;assert_return_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;cls&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual_return_code&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_return_code&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        Assert that the actual return code is as expected.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;

        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;actual_return_code&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;expected_return_code&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="s2"&gt;"Actual error code ("&lt;/span&gt;
                &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;actual_return_code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;") and expected error code ("&lt;/span&gt;
                &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expected_return_code&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;") differ."&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;AssertionError&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;ex&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ex&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;A slightly more complicated function is the &lt;code&gt;assert_stream_contents&lt;/code&gt; comparison
function. To ensure
that helpful information is returned in the assert failure message, it checks to see if
the &lt;code&gt;expected_stream&lt;/code&gt; is set and calls &lt;code&gt;compare_versus_expected&lt;/code&gt; if so.  (More about
that function in a minute.)  If not set, the assert used clearly states that the stream
was expected to be empty, and the actual stream is not empty.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;assert_stream_contents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual_stream&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_stream&lt;/span&gt;
    &lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        Assert that the contents of the given stream are as expected.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;

        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;expected_stream&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compare_versus_expected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;stream_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual_stream&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_stream&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;actual_stream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getvalue&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="s2"&gt;"Expected "&lt;/span&gt;
                    &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;stream_name&lt;/span&gt;
                    &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;" to be empty. Not:&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
                    &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;actual_stream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getvalue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                    &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;AssertionError&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;ex&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ex&lt;/span&gt;
        &lt;span class="k"&gt;finally&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;actual_stream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Addressing the second issue with the initial &lt;code&gt;assert_results&lt;/code&gt; function, the differences
between the two streams being difficult to discern, is the &lt;code&gt;compare_versus_expected&lt;/code&gt;
function.  My first variation on this function simply used the statement
&lt;code&gt;assert actual_stream.getvalue() != expected_text&lt;/code&gt;, producing the same assert result,
but lacking in the description of why the assert failed.  The second variation of this
function added a better assert failure message, but left the task of identifying the
difference between the two strings on the reader of the failure message.  The final
variation of this function uses the &lt;code&gt;difflib&lt;/code&gt; module and the &lt;code&gt;difflib.ndiff&lt;/code&gt; function to
provide a detailed line-by-line comparison between the actual stream contents and the
expected stream contents.  By using the &lt;code&gt;difflib.ndiff&lt;/code&gt; function in this final
variation, the assert failure message now
contains a very easy to read list of the differences between the two streams.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;difflib&lt;/span&gt;

    &lt;span class="nd"&gt;@classmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compare_versus_expected&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="bp"&gt;cls&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual_stream&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_text&lt;/span&gt;
    &lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;        Do a thorough comparison of the actual stream against the expected text.&lt;/span&gt;
&lt;span class="sd"&gt;        """&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;actual_stream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getvalue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;expected_text&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;difflib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndiff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;expected_text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitlines&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;actual_stream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getvalue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitlines&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;diff_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;stream_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;" not as expected:&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;diff_values&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;---&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="using-it-all-together"&gt;Using it all together&lt;a class="headerlink" href="#using-it-all-together" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To start using the work that completed in the sections above, a proper subclass of the
&lt;code&gt;InProcessExecution&lt;/code&gt; class is required.  Because that class is an abstract base class,
a new class &lt;code&gt;MainlineExecutor&lt;/code&gt; is required to resolve the &lt;code&gt;execute_main&lt;/code&gt; function and
the &lt;code&gt;get_main_name&lt;/code&gt; function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MainlineExecutor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;InProcessExecution&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;resource_directory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getcwd&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="s2"&gt;"test"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"resources"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resource_directory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resource_directory&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;execute_main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;PyScan&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_main_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;"main.py"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;MainlineExecutor&lt;/code&gt; class implements those two required functions.  The
&lt;code&gt;get_main_name&lt;/code&gt; function returns the name of the module entry point for the project.
This name is inserted into the array of arguments to ensure that any functions based
off of the command line &lt;code&gt;sys.argv&lt;/code&gt; array resolves properly.  The &lt;code&gt;execute_main&lt;/code&gt;
function implements the actual code to invoke the main entry point for the script.  In
the case of the PyScan project, the entry point at the end of the &lt;code&gt;main.py&lt;/code&gt; script is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"__main__"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;PyScan&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Therefore, the contents of the &lt;code&gt;execute_main&lt;/code&gt; function is &lt;code&gt;PyScan().main()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In addition to those two required functions, there is some extra code in the constructor
for the class.  Instead of recomputing the resource directory in each test that requires
it, the &lt;code&gt;MainlineExecutor&lt;/code&gt; class computes it in the constructor to keep the test
functions as clean as possible.  While this is not required when subclassing from
&lt;code&gt;InProcessExecution&lt;/code&gt;, it has proven very useful in practice.&lt;/p&gt;
&lt;p&gt;To validate the use of the &lt;code&gt;MainlineExecutor&lt;/code&gt; class with the project, I created a
simple scenario test to verify that the version of the scanner is correct.  This is
very simple test, and verifying that the framework passes such a simple test increases
the confidence in the framework itself.  At the start of the scenario test, the
&lt;code&gt;executor&lt;/code&gt; variable is created and assigned an instance of our new class
&lt;code&gt;MainlineExecutor&lt;/code&gt; as well as specify that the arguments to
use for the script as &lt;code&gt;["--version"]&lt;/code&gt;. in the array &lt;code&gt;suppplied_arguments&lt;/code&gt;  In keeping
with the Arrange-Act-Assert pattern, I then specify the expected behaviors for stdout
(in &lt;code&gt;expected_output&lt;/code&gt;), stderr (in &lt;code&gt;expected_error&lt;/code&gt;), and the return code from the
script (in &lt;code&gt;expected_return_code&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Having set everything up in the Assert section of the test, the Act section simply
invokes the script using the &lt;code&gt;executor.invoke_main&lt;/code&gt; function with the
&lt;code&gt;suppplied_arguments&lt;/code&gt; variable assigned previously, and collect the results.  Once
collected, the &lt;code&gt;execute_results.assert_results&lt;/code&gt; function verifies those actual results
against the expected results, asserting if there are differences.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_get_summarizer_version&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Make sure that we can get information about the version of the summarizer.&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;

    &lt;span class="c1"&gt;# Arrange&lt;/span&gt;
    &lt;span class="n"&gt;executor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MainlineExecutor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;suppplied_arguments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"--version"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;expected_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"""&lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="s2"&gt;main.py 0.1.0&lt;/span&gt;
&lt;span class="s2"&gt;"""&lt;/span&gt;
    &lt;span class="n"&gt;expected_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt;
    &lt;span class="n"&gt;expected_return_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="c1"&gt;# Act&lt;/span&gt;
    &lt;span class="n"&gt;execute_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;executor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invoke_main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;suppplied_arguments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Assert&lt;/span&gt;
    &lt;span class="n"&gt;execute_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assert_results&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;expected_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_return_code&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="what-does-using-this-look-like"&gt;What Does Using This Look Like?&lt;a class="headerlink" href="#what-does-using-this-look-like" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In terms of writing scenario tests, the tests are usually as simple to write as the
&lt;code&gt;test_get_summarizer_version&lt;/code&gt; function in the last section.  If there are parts of the
output that have a non-constant value, such as the full path of the directory in which
the test is executed in, the &lt;code&gt;expected_output&lt;/code&gt; variable would have to be set to
compensate for that variability, but that is an expected complexity.&lt;/p&gt;
&lt;p&gt;For the PyScan project, a quick scan of the 
&lt;a href="https://github.com/jackdewinter/pyscan/blob/master/test/test_scenarios.py"&gt;PyScan test_scenarios.py file&lt;/a&gt; reveals that for this project, the non-constant values most often
occur with failure messages, especially ones that relay path information in their
failure messages.  When that happens, such as with the
&lt;code&gt;test_summarize_junit_report_with_bad_source&lt;/code&gt; test function, that extra complexity
is not overwhelming and does not make the test function unreadable.&lt;/p&gt;
&lt;p&gt;In terms of the test output for a passing test, there is no difference.  If executing
&lt;code&gt;pipenv run pytest&lt;/code&gt; produced a &lt;code&gt;.&lt;/code&gt; for a successful test before, it remains a &lt;code&gt;.&lt;/code&gt; now.
The big difference is in what is displayed when there is a difference in the test
output.  &lt;/p&gt;
&lt;p&gt;In the case where there is a single character difference in the test output, such as
changing the expected output for the &lt;code&gt;test_get_summarizer_version&lt;/code&gt; test to
&lt;code&gt;main.py 0.1.1&lt;/code&gt;, the output below
clearly shows where the actual output and expected output differ.  Note that in these
comparisons, the line that starts with the &lt;code&gt;-&lt;/code&gt; character is the expected output and
the line that starts with the &lt;code&gt;+&lt;/code&gt; character is the actual output.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;E       AssertionError: Either stdout, stderr, or the return code was not as expected.
E
E       stdout not as expected:
E       ---
E       - main.py 0.1.1
E       ?             ^
E
E       + main.py 0.1.0
E       ?             ^
E
E       ---
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the case where a line in the test output is completely different, such as changing
the expected output to &lt;code&gt;This is another line&lt;/code&gt;, the output below clearly reflects that
difference:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;E       AssertionError: Either stdout, stderr, or the return code was not as expected.
E
E       stdout not as expected:
E       ---
E       - This is another line
E       + main.py 0.1.0
E       ---
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, in the case where the actual output contains either more lines or less lines
that the expected output, such as adding the line &lt;code&gt;This is another line&lt;/code&gt; to the
expected output, the output below clearly shows that difference.  In this example, as
the first line is at the start of both the actual output and expected output, it is
shown without any prefix to the line.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;E       AssertionError: Either stdout, stderr, or the return code was not as expected.
E
E       stdout not as expected:
E       ---
E         main.py 0.1.0
E       - This is another line
E       ---
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="summary"&gt;Summary&lt;a class="headerlink" href="#summary" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;While the &lt;code&gt;pytest_execute.py&lt;/code&gt; file that I use as the base for my scenario tests isn’t
rocket science, it is invaluable to me in creating simple, easy-to-read scenario tests.
At the heart of the module is the base requirement (as stated above) to execute the
Python script independently, capture all relevant information about it’s execution,
and then verifying that information against expected values.  Based on my experience
and evolution of this module, I believe that it handily satisfies the requirements
with ease.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;To keep things simple for the article, the &lt;code&gt;additional_error&lt;/code&gt; parameter from a number of the functions has been removed.  This parameter is used in the PyMarkdown project and will be documented as part of my articles on that project. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Software Quality"></category><category term="pytest"></category><category term="scenario testing"></category></entry><entry><title>Have a Happy Winter Holiday 2019</title><link href="https://jackdewinter.github.io/2019/12/29/have-a-happy-winter-holiday-2019/" rel="alternate"></link><published>2019-12-29T00:00:00-08:00</published><updated>2019-12-29T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2019-12-29:/2019/12/29/have-a-happy-winter-holiday-2019/</id><summary type="html">&lt;p&gt;I just wanted to take a quick minute and wish everyone a  happy winter holiday season
as 2019 winds to a close.  When I resume posts in the new year, I will be trying to
publish weekly posts on Mondays instead of Sundays, and see how that goes.&lt;/p&gt;
&lt;p&gt;Safe travels …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just wanted to take a quick minute and wish everyone a  happy winter holiday season
as 2019 winds to a close.  When I resume posts in the new year, I will be trying to
publish weekly posts on Mondays instead of Sundays, and see how that goes.&lt;/p&gt;
&lt;p&gt;Safe travels, and well wishes.&lt;/p&gt;</content><category term="Software Quality"></category></entry><entry><title>Markdown Linter - Parser Testing Strategy</title><link href="https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/" rel="alternate"></link><published>2019-12-22T00:00:00-08:00</published><updated>2019-12-22T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2019-12-22:/2019/12/22/markdown-linter-parser-testing-strategy/</id><summary type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the previous articles in this series, I discussed the requirements for the Markdown
linter that I am writing.  From a development point of view, the main requirement is
the need for an accurate stream of tokens emitted by the parser.  Due to the absence of
any Markdown-to-token parsers …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the previous articles in this series, I discussed the requirements for the Markdown
linter that I am writing.  From a development point of view, the main requirement is
the need for an accurate stream of tokens emitted by the parser.  Due to the absence of
any Markdown-to-token parsers out there, I need to write a new parser that outputs an
accurate stream of tokens instead of a stream of HTML text. With the last article
showing the patterns I am using to test the parser, it is now time to figure out a set
of good strategies for the project, to ensure I can complete it without losing my
confidence (and sanity).&lt;/p&gt;
&lt;h2 id="why-is-strategy-important-when-testing"&gt;Why Is Strategy Important When Testing?&lt;a class="headerlink" href="#why-is-strategy-important-when-testing" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;When my son was younger, like most boys in his age group, he loved playing with LEGO and
he loved the idea of robots.  I mean, come on!  I am a lot older than him and I still
like LEGO and the idea of robots!  Anyhow, at his school they advertised for 5th grade
students that were interested in participating in a local
&lt;a href="http://firstlegoleague.org/"&gt;FIRST Lego League&lt;/a&gt; robotics team. From the first mention
of it, he was hooked.  As they needed some parents to help out, I participated with him
as a coach.  That position was a very rewarding, very humbling, and very frustrating
experience. Rewarding because I got to help 5th graders learn a little taste of what I
did everyday at work.  Humbling because the look in the kid’s eyes when they really
understood something reminded me of the benefits of being a coach.  Frustrating because
of almost all of the rest of the time between those two types of moments.&lt;/p&gt;
&lt;p&gt;I am not sure which parent, coach, or teacher helped me with a little gem of wisdom,
but I remember it as clear as day:  People have problems moving boulders, people
have success moving pebbles.  The idea behind that phrase is that if a team is
confronted with a problem, it is like encountering a boulder that you need to move out
of the way. Upon seeing a big boulder, many people take a look at it and say something
similar to “Wow! That is too big to move!”  But if you take that boulder and break it
down into smaller rocks, such as pebbles, many people will just laugh with ease at
moving those rocks, even if they have to do it one at a time.  In a similar fashion,
breaking down a big problem into smaller problems is a necessity in problem solving a
situation.  The boulders-to-pebbles phrase is a phrase I still use to this day when
coaching people in both my professional and personal lives.&lt;/p&gt;
&lt;p&gt;Writing a parser that handles anything more significant than a single line of text is
definitely “a boulder”.  I have been writing parsers for the better part of 25 years,
and those parsers are still boulders to me. However, I know from experience that
breaking down that “boulder-sized” task into more “pebble-sized” tasks works and works
well.  So here are the various items of my strategy for this project.&lt;/p&gt;
&lt;h2 id="strategy-0-define-and-execute-testing-linting-and-formatting"&gt;Strategy 0: Define and Execute Testing, Linting, and Formatting&lt;a class="headerlink" href="#strategy-0-define-and-execute-testing-linting-and-formatting" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For me this is a strategy that I bring to almost every project, with very few
exceptions.  I always start with some kind of workflow template that I apply to the
project that performs formatting of the source code, linting of the source code, and
executes the testing framework.  Since I am a stickler for this approach, the setup for
this workflow usually takes 5 minutes or less, as I usually have at least one example
project lying around.  By consistently executing this workflow before committing any
changes, I keep the quality reasonably high as I go.&lt;/p&gt;
&lt;p&gt;Knowing that I had this framework in place for the Markdown parser was a godsend.  My
preference is to find frequent small break points during the implementation of a
feature, and to use those points to run the workflow.  For me, it increases my
confidence that I am either establishing a new “last known good point” or that I need
to retrace my steps to the last known good point to address an issue.  That confidence
helps me go forward with a positive attitude.&lt;/p&gt;
&lt;h2 id="strategy-0a-suppress-major-issues-until-later"&gt;Strategy 0A: Suppress Major Issues Until Later&lt;a class="headerlink" href="#strategy-0a-suppress-major-issues-until-later" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This may seem like somewhat of a counter to Strategy 0, but I see it more of allowing
the project to grow, but being reminded that there is work to do.  Minor issues such
as stylistics and documentation are handled right away, as they have a direct impact
on the maintainability of the code as it moves forward.  Major issues usually involve
a larger amount of code, and changing that much code usually has a fair amount of side
effects unless you work to prevent those side effects.&lt;/p&gt;
&lt;p&gt;Major issues are usually of the “too many/much” type, such as “too much complexity”,
“too many statements”, or “too many boolean statements”.  When I get to a really good
and stable point in the project, I know I will deal with these.  If I deal with the
issues before I get to such a point, I am taking a chance that I won’t have the
stability to make the change, while limiting and dealing with any potential side effects
in a clean and efficient manner.&lt;/p&gt;
&lt;p&gt;What is a good and stable point? For me, such a point has to have two dominant
characteristics.  The first is that I need to have a solid collection of tests in place
that I can execute.  These tests make sure that any refactoring doesn’t negatively
affect the quality of the code. The second characteristic is that the source code for
the project is at a point where there is a large degree of confidence that the
code in the section that I want to refactor is very solid and very well defined.  This
ensures that I can start looking for commonalities and efficiencies for refactoring
that will enhance the source code, but not prematurely.&lt;/p&gt;
&lt;h2 id="strategy-1-break-tests-and-development-into-task-groups"&gt;Strategy 1: Break Tests and Development Into Task Groups&lt;a class="headerlink" href="#strategy-1-break-tests-and-development-into-task-groups" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Following the principle of keeping things at a good size, don’t plan the entire project
out ahead of time, but make sure to break things down into the groups of tasks that are
needed as you need them.  Following an agile approach, make sure you have a good idea of
what needs to be done for a given task group, and don’t worry about any more details of
it until you need to.  And when you reach that point, reverify the tasks before going
forward and flushing out the details.&lt;/p&gt;
&lt;p&gt;For this parser, the
&lt;a href="https://github.github.com/gfm"&gt;GitHub Flavored Markdown specification&lt;/a&gt; delineates it’s
groups by the features in Markdown that are implemented.  Aligning the groups specified
in that document with the groups for tests and development was a solid choice from a
tracking point of view.  One of the reasons that I feel this worked well is because
these feature groups have anywhere between 1 and 50 examples in each group.  While some
of the larger ones were a tiny bit too big, for the most part it was a manageable
number of scenarios to handle in each group.&lt;/p&gt;
&lt;h2 id="strategy-2-organize-those-task-groups-themselves"&gt;Strategy 2: Organize Those Task Groups Themselves&lt;a class="headerlink" href="#strategy-2-organize-those-task-groups-themselves" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Once the task groups have been identified, take a step back and organize those task
groups themselves.  There are almost always going to be task groups that have a natural
affinity to be with similar task groups, so do so.  By doing similar tasks in
groups, it will help identify refactorings that can be accomplished later, as well as
the efficiency benefits from repeating similar processes.  Especially with a larger
project, those little efficiency benefits can add up quickly.&lt;/p&gt;
&lt;p&gt;As with the previous strategy, the GitHub Flavored Markdown specification comes to the
rescue again.  There are some implementation notes near the end of the specification
that provide some guidance on grouping.  The groups that I recognized were container
blocks, normal blocks, and inline parsing.  Normal blocks are the foundation of the
parsing, so it made sense to schedule those first.  Container blocks (lists and block
quotes) add nesting requirements, so I scheduled those second.  Finally, once all of
the block level tasks are done, inline parsing (such as for emphasis) can be performed
on text blocks derived at after the processing of the normal and container blocks.
After re-reading the end of the specification, the example that they gave seemed to
indicate that as well, so I was probably on a decent path.&lt;/p&gt;
&lt;h2 id="strategy-3-kiss"&gt;Strategy 3: K.I.S.S.&lt;a class="headerlink" href="#strategy-3-kiss" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As I mentioned in the last article, I am a big proponent of the
&lt;a href="https://en.wikipedia.org/wiki/KISS_principle"&gt;K.I.S.S principle&lt;/a&gt;.
While I usually arrive at an end project that has lots of nice classes and functions,
worrying about that at an early stage can often be counter productive.  Even if it means
doing ugly string manipulations with variable names that you know you will change, that
approach can often lead to cleaner code faster.  Worry about getting the logic and the
algorithms right first, and then worry about making it “look pretty”.&lt;/p&gt;
&lt;p&gt;A good example of this is my traditional development practice of giving variables and
functions “garbage names” until I am finished with a set of functions.  Yes, that means
during development I have variable names like “foobar”, “abc”, “sdf”, and “ghi”, just to
name a few of them.  When I am creating the function, I maintain a good understanding of
what the variables are doing, and I want to concentrate on the logic.  Once the logic
is solid, I can then rename the variables to a descriptive name that accurately
reflects it’s purpose and use.&lt;/p&gt;
&lt;p&gt;I am not sure if this process works for everyone, but for me, not focusing on the names
helps me focus on the logic itself.  I also find that having a “naming pass” at the
function when I am done with the work helps me to give each variable a more meaningful
name before I commit the changes.  Once again, this is one of my development practices
that helps boost my productivity, and I acknowledge it might not work for everyone.&lt;/p&gt;
&lt;p&gt;For the parser, I employed this strategy whole-heartedly.  The first couple of groups of
work on the parser were performed by dealing with strings, with the only class for the
parser being the single class containing the parsing logic.  Once I got to a good point
(see above), I moved a number of the parsing functions and html functions into their
own static helper modules.  Up until that point, it was just simpler to be creative
with the logic in a raw form.  After that point, it made more sense to identify and
solidify the logic that encapsulated some obvious patterns, moving those algorithms
into their own classes for easy identification.&lt;/p&gt;
&lt;p&gt;As with many things, finding the right points to perform changes like this are difficult
to describe.  I can only say that “it felt like the right time for that change”.  And as
I commit and stage code frequently, if I made a mistake, I could easily rewind and
either retry the change, or abandon it altogether.&lt;/p&gt;
&lt;h2 id="strategy-4-use-lots-of-debug-output"&gt;Strategy 4: Use Lots of Debug Output&lt;a class="headerlink" href="#strategy-4-use-lots-of-debug-output" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;There is a phrase that we use at work called “TTR” or Time-To-Resolution.
This is usually measured as the time taken from knowing that you have a problem until
the time that the problem is resolved and it’s solution is published.  Added during
development and debugging, spurious debug output can help provide a journal or log of
what is happening in the project, allowing for a more comprehensive comparison of the
 output of a passing test with the output of a failing test at the same time.  &lt;/p&gt;
&lt;p&gt;To be clear, using a debugger to load the code and step through it as it executes is
another way to debug the code.  In fact, in a fairly decent number of situations I
recommend that.  However, I find that the downside is that I don’t get to see the
flow through the code in the same way as with lots of debug statements.  As with a
lot of things, determining the balance between debug output and using a debugger
will differ for individual developers and for individual projects.&lt;/p&gt;
&lt;p&gt;Another benefit of the debug output approach is the transition from debug output to
logging.  Once the project has been sufficiently stabilized and completed, one of the
tasks that
arises is usually to output useful log messages at various points throughout the code.
I personally find that a certain percentage of the debug output that was good enough to
emit during development can become quality log messages with only small changes.&lt;/p&gt;
&lt;p&gt;The parser development definitely benefitted from this strategy.  Within a given task
group, there were often two Markdown patterns that were almost the same.  Sometimes it
looked like they should being parsed differently and sometimes I couldn’t figure out
why they weren’t parsed differently.  By examining the debug output for both cases,
I was able to verify whether or not the correct paths were followed, and if not, where
the divergences occurred.  Sure, the debug was cryptic and most of it never made it in
the final version of the parser. But when I needed to debug or verify during
development, it was invaluable.&lt;/p&gt;
&lt;h2 id="strategy-5-run-tests-frequently"&gt;Strategy 5: Run Tests Frequently&lt;a class="headerlink" href="#strategy-5-run-tests-frequently" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Don’t only run tests when a set of changes is ready to commit, run those tests
frequently during the development of each task.  If done properly, most tests are there
to verify things are as they should be, and to warn of changes or situations that fall
outside of the requirements. If something is wrong, it is better to look through the
last feature added to determine what the problem is, rather than trying to determine
which of the last 5 features introduced that bad behavior.  Therefore, by executing the
tests frequently, either the confidence that the project is working properly increases
or there are early and frequent indications that something is wrong.&lt;/p&gt;
&lt;p&gt;During the development of the parser, the tests were instrumental in making sure that
I knew what features were “locked down” and which features needed work.  By keeping
track of that when adding a new feature, I could easily see when work on a new feature
caused a previously completed feature to fail it’s tests.  At that point, I knew I
didn’t have the right solution, but I also had confidence that the changes were small
enough to handle.&lt;/p&gt;
&lt;p&gt;Also, as the specification is large, there were often cases that were present but not
always spelled out in the documentation as well as they could have been.  However, time
and time again, the saving grace for the specification were the examples, now scenarios
and scenario tests in my project, sterling examples of what to expect.  And as I took
care to make sure they ran quickly, I was able to run all of the scenario tests in less
than 10 seconds.  For me, taking 10 seconds to ensure things were not broken was well
worth the cost.&lt;/p&gt;
&lt;h2 id="strategy-6-do-small-refactors-only-at-good-points"&gt;Strategy 6: Do Small Refactors Only At Good Points&lt;a class="headerlink" href="#strategy-6-do-small-refactors-only-at-good-points" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;While this strategy may look like a repeat of
&lt;a href="https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/#strategy-0a-suppress-major-issues-until-later"&gt;Strategy 0A: Suppress Major Issues Until Later&lt;/a&gt;,
the scope for this strategy is on a smaller, more local level.  Where Strategy 0A talks
about refactoring major issues later, there are often obvious minor refactors that can
be done at a local level.  These changes are often done right after a function is
written to fulfil a feature and rarely includes more than one function.  A good example
of this is taking a function that performs a given action twice with small variations
and rewriting that function by encapsulating that repeated action into it’s own
well-named function.  &lt;/p&gt;
&lt;p&gt;While such refactors almost always improve the code, care must be taken to strike a
good balance between making each method more readable and trying to optimize the
function ahead of time.  For myself, it is often more efficient for me to see the raw
code to recognize patterns from rather than already refactored code.  Unless I am the
author of the refactored code, I find that I don’t see the same patterns as with the
raw code.  As with many things, “Your Mileage May Vary”.&lt;/p&gt;
&lt;p&gt;When implementing the parser, this strategy was effectively applied at the local
level to improve readability and maintainability.  There were quite a few cases where
the logic to detect a given case and the processing of that case were complicated.
By assigning the detection of a given case to one function and the processing of that
case to another function, the border between the two concepts was enhanced, making the
calling function more readable.  As this kind of refactoring occurred at the local
level, it employed this strategy quite effectively.&lt;/p&gt;
&lt;h3 id="how-did-this-help"&gt;How Did This Help?&lt;a class="headerlink" href="#how-did-this-help" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;For one, I had a plan and a strategy to deal with things.  As always, something would
happen during development which would require me to re-assess something. Given the
above strategy, I had confidence that I would be able to deal with it, adjusting the
different parts of the project as I went.&lt;/p&gt;
&lt;p&gt;Basically, I took a boulder (writing a parser) and not only broke it down into pebbles
(tasks needed to write the parser), but came up with a set of rules (strategy) on what
to do if I found some rocks that were previously unknown or larger than a pebble.  As
I mentioned at the start of the article, it’s a fairly simple bit of wisdom that I was
taught, but what a gem it is!&lt;/p&gt;
&lt;h2 id="what-comes-next"&gt;What Comes Next?&lt;a class="headerlink" href="#what-comes-next" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the next article, I take the requirements, scenarios,
and strategies and put them together to start writing the parser.  As one of the
test groups that I came up with was normal Markdown blocks, I will describe how I
implemented those blocks as well as the issues I had in doing so cleanly.&lt;/p&gt;</content><category term="Software Quality"></category><category term="markdown linter"></category></entry><entry><title>Markdown Linter - Setting Up Parser Tests</title><link href="https://jackdewinter.github.io/2019/12/16/markdown-linter-setting-up-parser-tests/" rel="alternate"></link><published>2019-12-16T00:00:00-08:00</published><updated>2019-12-16T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2019-12-16:/2019/12/16/markdown-linter-setting-up-parser-tests/</id><summary type="html">
&lt;h2 id="sidebar"&gt;Sidebar&lt;a class="headerlink" href="#sidebar" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;My apologies for this being a day or two later that usual.  My son brought home a
cold that knocked the stuffing out of me, I needed to take some personal time to ensure
I was feeling better before writing.  Thanks for your patience.&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As a reminder of …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="sidebar"&gt;Sidebar&lt;a class="headerlink" href="#sidebar" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;My apologies for this being a day or two later that usual.  My son brought home a
cold that knocked the stuffing out of me, I needed to take some personal time to ensure
I was feeling better before writing.  Thanks for your patience.&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As a reminder of the requirements from the
&lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;last article&lt;/a&gt;,
the big bullet-point items are:
command line driven, GitHub Flavored Markdown (for now), and preserving all tokens.  To
make sure I have a solid set of goals to work towards, setting these requirements as
part of the project was pivotal.  Now that I have that as a touchstone, I need to move
forward with defining how to progress with the testing of the parser at the core of
the linter.&lt;/p&gt;
&lt;h2 id="why-write-a-parser"&gt;Why Write a Parser?&lt;a class="headerlink" href="#why-write-a-parser" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In taking a look at the kind of rules that linters support, I have observed that there
are typically two categories of rules: general rules and grammar rules.  For general
rules such as “tabs should not be used”, it is easy to look at any line in the
document being scanned and look for a tab character.  For grammatical rules such
as “headings should always be properly capitalized”, that scan is more difficult.
The most difficult part of that rule is identifying whether or not any given piece
of text is considered part of a header, thus engaging the rest of the rule.&lt;/p&gt;
&lt;p&gt;From experience, to properly determine which part of grammar maps to which part of text
requires a capable parser, written to the specifications of the language to be parsed.
Based on my research from the
&lt;a href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/"&gt;last article&lt;/a&gt;,
all of the parsers that I found only translated Markdown into HTML, not any
intermediate form.  Since I need a clean stream of tokens before translation to HTML,
the only option is to write my own parser which will output a clean stream of parsed
Markdown tokens.&lt;/p&gt;
&lt;p&gt;As I am writing my own parser, I need to have a good set of tests to ensure
that the parser works properly.  But where to start?&lt;/p&gt;
&lt;h2 id="where-to-start-with-the-tests"&gt;Where To Start With The Tests?&lt;a class="headerlink" href="#where-to-start-with-the-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Referring back to my article on &lt;a href="https://jackdewinter.github.io/2019/11/10/software-quality-reliability/"&gt;software reliability&lt;/a&gt;,
the 2 main types of tests that I need to decide on are scenario tests and unit tests.
In a nutshell, the purpose of a scenario test is to test the input and outputs of the
project and the purpose of a unit test is to test a specific function of a specific
components of the project.  Getting a hold of how to balance the quantity of tests that
I need to write between the two of these types is my first priority.&lt;/p&gt;
&lt;p&gt;As one of the initial requirements is to support the
&lt;a href="https://github.github.com/gfm"&gt;GitHub Flavored Markdown specification&lt;/a&gt;, it is useful to
note that the specification itself has 637 individual examples.  Each example provides
for the input, in Markdown, and the output, in HTML.  While the output is not at the
token level needed to satisfy my project’s third requirement, it should be close
enough.  In looking at each of these examples, I need a solid set of rules that I
can apply to the tokens to get them from my desired token-based output to a HTML-based
output that matches the examples.  It is reasonable to collect these rules as I go when
I develop the various types of elements to be parsed.  If I tried to do them to far
ahead of time, it would invariably lead to a lot of rework.  Just in time is the way to
go for these rules.&lt;/p&gt;
&lt;p&gt;Taking another looking at the types of tests that I need to write, I realized that this
project’s test viewpoint was inverted from the usual ratio of scenario tests to unit
tests.  In most cases, if I have anything more than 20-30 scenario tests, I would think
that I have not properly scoped the project.  However, with 637 scenarios already
defined for me, it would be foolish not to write at least one scenario test for each of
those scenarios, adding extra scenario tests and supportive unit tests where needed.
In this case, it makes more sense to focus on the scenario tests as the major set of
tests to write.&lt;/p&gt;
&lt;p&gt;The balance of scenario tests to unit tests?&lt;/p&gt;
&lt;p&gt;Given 637 scenarios ready to go, I need to create at least 637 scenario tests.
For those scenario tests, experimenting with the first couple of scenario tests to
find a process that worked seemed to be the most efficient way forward.  Given a simple
and solid template for every scenario test, I had a lot of confidence to then use that
template for each scenario test that I tackled.&lt;/p&gt;
&lt;p&gt;And the unit tests?  In implementing any parsing code, I knew that I needed helper
functions that parsed a specific type of foundational thing, like a tag in an HTML
block or skipping ahead over any whitespace.  The unit tests are used to verify
that those kind of foundational functions are operating properly, ensuring that the
rest of the code can depend on those foundations with confidence.  As an added bonus,
more combinations of the various sequences to parse could be tested without inflating
the number of scenario tests.&lt;/p&gt;
&lt;p&gt;Ground rules set?   Check.  On to the first scenario test.&lt;/p&gt;
&lt;h2 id="starting-with-the-first-scenario-test"&gt;Starting With the First Scenario Test&lt;a class="headerlink" href="#starting-with-the-first-scenario-test" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;While it might not seem correct, starting with example number 189, the first test I
did write was for
&lt;a href="https://github.github.com/gfm/#example-189"&gt;GitHub Flavored Markdown example 189&lt;/a&gt;,
the first example
included in the specification for the paragraph blocks.  After solidly reading the
specification, the general rule seemed to be that if it doesn’t fit into any other
category, it is a paragraph.  If everything is going to be a paragraph until the other
features are written, I felt that starting with the default case was the right choice.&lt;/p&gt;
&lt;p&gt;After a number of passes at cleaning up the test for this first case, it boiled down to
the following Python code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;https://github.github.com/gfm/#paragraphs&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pymarkdown.tokenized_markdown&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TokenizedMarkdown&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;assert_if_lists_different&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_paragraph_blocks_189&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Test case 189:  simple case of paragraphs&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;

    &lt;span class="c1"&gt;# Arrange&lt;/span&gt;
    &lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TokenizedMarkdown&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;source_markdown&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"""aaa&lt;/span&gt;

&lt;span class="s2"&gt;bbb"""&lt;/span&gt;
    &lt;span class="n"&gt;expected_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="s2"&gt;"[para:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[text:aaa:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[end-para]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[BLANK:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[para:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[text:bbb:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[end-para]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Act&lt;/span&gt;
    &lt;span class="n"&gt;actual_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source_markdown&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Assert&lt;/span&gt;
    &lt;span class="n"&gt;assert_if_lists_different&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expected_tokens&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual_tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="breaking-down-the-scenario-test"&gt;Breaking Down the Scenario Test&lt;a class="headerlink" href="#breaking-down-the-scenario-test" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It might be a lot to take in all at once, so let’s break it down step by step.&lt;/p&gt;
&lt;h3 id="start-of-the-module"&gt;Start of the Module&lt;a class="headerlink" href="#start-of-the-module" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The start of the module needs to perform two important tasks: provide useful
documentation to someone examining the tests and import any libraries needed.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;https://github.github.com/gfm/#paragraphs&lt;/span&gt;
&lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pymarkdown.tokenized_markdown&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TokenizedMarkdown&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;.utils&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;assert_if_lists_different&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The most useful and relevant information about the module that I was able to think of
was the actual source for the test cases themselves.  That being the case, I felt that
including the URI to the specific section in the
&lt;a href="https://github.github.com/gfm"&gt;GitHub Flavored Markdown specification&lt;/a&gt; was the
right choice for the module documentation.  For anyone reading the tests, it provides a
solid reference point that answers most of the questions about why the tests are there
and whether or not the tests are relevant.&lt;/p&gt;
&lt;p&gt;Next are the import statements.  The first one statement imports the
&lt;code&gt;TokenizedMarkdown&lt;/code&gt; class, a class that I set up to handle the parsing.  Initially this
class was a quick and simple skeleton class, especially for the first paragraph case.
However, it provided the framework for me to support more use cases while maintaining
a uniform interface. The second import statement is used to include a function that
provides a good comparison of the contents of the list returned from the &lt;code&gt;transform&lt;/code&gt;
function of the &lt;code&gt;TokenizedMarkdown&lt;/code&gt; class and a simple text list of the expected
tokens.  &lt;/p&gt;
&lt;h3 id="arrange-the-data-for-the-test"&gt;Arrange The Data For The Test&lt;a class="headerlink" href="#arrange-the-data-for-the-test" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;From all of the useful pieces of information that I have learned about testing, the
most useful bits about actually writing tests are the K.I.S.S. principle and the use of
the Arrange-Act-Assert pattern.  The
&lt;a href="https://en.wikipedia.org/wiki/KISS_principle"&gt;K.I.S.S principle&lt;/a&gt; constantly reminds me
to not overcomplicate things, reducing the tests to what is really relevant for that
thing or task.  The
&lt;a href="https://docs.telerik.com/devtools/justmock/basic-usage/arrange-act-assert"&gt;Arrange-Act-Assert pattern&lt;/a&gt;
reminds me that when writing tests, each test I write breaks down into setup, action,
and verification (with cleanup occasionally being added if needed). As such, I always
start writing my tests by adding a comment for each of those sections, with the rest
of the function blank.  Once there, it’s easy to remember which parts of the tests
go where!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_paragraph_blocks_189&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;"""&lt;/span&gt;
&lt;span class="sd"&gt;    Test case 189:  simple case of paragraphs&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;

    &lt;span class="c1"&gt;# Arrange&lt;/span&gt;
    &lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TokenizedMarkdown&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;source_markdown&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"""aaa&lt;/span&gt;

&lt;span class="s2"&gt;bbb"""&lt;/span&gt;
    &lt;span class="n"&gt;expected_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="s2"&gt;"[para:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[text:aaa:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[end-para]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[BLANK:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[para:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[text:bbb:]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;"[end-para]"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;Arrange&lt;/code&gt; part of this test is simple, consisting mostly of easy-to-read
assignments.  The object to test needs to be setup in a way that it is completely
enclosed within the test function.  The tokenizer object with no options is assigned to
the &lt;code&gt;tokenizer&lt;/code&gt;, so a simple assignment takes care of it’s setup.  The &lt;code&gt;source_markdown&lt;/code&gt;
variable is setup within Python’s
&lt;a href="https://docs.python.org/3/tutorial/introduction.html#strings"&gt;triple-quotes&lt;/a&gt;
to preserve newlines and provide an accurate look at the string being fed to the
tokenizer.  This string is copied verbatim from the example represented by the function,
in this case &lt;a href="https://github.github.com/gfm/#example-189"&gt;example 189&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The final setup, the array assigned to the &lt;code&gt;expected_tokens&lt;/code&gt; variable, takes a bit more
work.  When I wrote these, I sometimes wrote the expect tokens ahead of time, but more
often than not used a known “bad” set of tokens and adjusted the tokens as I went.&lt;/p&gt;
&lt;h3 id="act-tokenize-and-assert-verify-results"&gt;Act (Tokenize) and Assert (Verify Results)&lt;a class="headerlink" href="#act-tokenize-and-assert-verify-results" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;With all of the work on the setup of the tests, the Act and Assert parts of the test
are very anticlimactic.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="c1"&gt;# Act&lt;/span&gt;
    &lt;span class="n"&gt;actual_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;source_markdown&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Assert&lt;/span&gt;
    &lt;span class="n"&gt;assert_if_lists_different&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expected_tokens&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;actual_tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the information that was established in the Arrange section of the test, the
Act section simply applies the input (&lt;code&gt;source_markdown&lt;/code&gt;) to the object to test
(&lt;code&gt;tokenizer&lt;/code&gt;) and collects the output in &lt;code&gt;actual_tokens&lt;/code&gt;.  The Assert section then
takes the output tokens and compares them against the expected list of tokens in
&lt;code&gt;expected_tokens&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id="why-not-use-pure-test-driven-development"&gt;Why Not Use Pure Test Driven Development?&lt;a class="headerlink" href="#why-not-use-pure-test-driven-development" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In a normal project, I usually follow
&lt;a href="https://en.wikipedia.org/wiki/Test-driven_development"&gt;Test Driven Development&lt;/a&gt;
practices quite diligently, either writing the tests first and code second, or writing
both tests and code at the same time.  As this was my first version of my first
Markdown parser, I was aware that I was going to be adapting the tokens and token
formats as I went, eventually arriving at a set of tokens that worked for all scenarios.
Knowing that this churn was part of the development process for this project, I decided
that a true Test Driven Development process would not be optimal.&lt;/p&gt;
&lt;p&gt;For this project, it was very useful to adjust the process.  The balance that I struck
with myself was to make sure that as I coded the parser to respond to a given scenario,
I adjusted the tokens assigned to the &lt;code&gt;expected_tokens&lt;/code&gt; variable based on the example’s
HTML output for the equivalent scenario test.  This process gave me the confidence to
know that as I made tests pass by enabling the code behind the scenario, each individual
passing test was both moving towards a fully functioning parser and protecting the work
that I had already done in that direction.&lt;/p&gt;
&lt;p&gt;To be clear, as I copied the template over, I adjusted the function name, the
function’s doc-string, and the Markdown source text based on the scenario test that I
was implementing.  The list of tokens in &lt;code&gt;expected_tokens&lt;/code&gt; were then populated with
a “best guess” before I started working on the code to make that scenario pass.
In a microscopic sense, as I updated the test and the test tokens before starting on
the code, I was still adhering to Test Driven Development on a scenario-by-scenario
level.&lt;/p&gt;
&lt;p&gt;To me, this was a good balance to strike, evaluating the correct tokens as I went
instead of trying to work out all 637 sets of tokens ahead of time.  &lt;/p&gt;
&lt;h3 id="how-did-this-help"&gt;How Did This Help?&lt;a class="headerlink" href="#how-did-this-help" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Getting a good process to deal with the large bulk of scenario tests was a welcome
relief.  While I still needed to create a strategy to deal with that bulk of scenario
tests I would need to write (see the
&lt;a href="https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/"&gt;next article&lt;/a&gt;
for details on that), I had a
solid template that was simple (see K.I.S.S. principle), easy to follow (see
Arrange-Act-Assert pattern), and would scale.  This was indeed something that I
was able to work with.&lt;/p&gt;
&lt;h2 id="what-about-the-unit-tests"&gt;What About the Unit Tests?&lt;a class="headerlink" href="#what-about-the-unit-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Compared to the scenario tests, writing unit tests for the parser’s foundation
functions was easy.  In each case, there is a function to test with a very cleanly
specified interface, providing for a clean definition of expected input and output.&lt;/p&gt;
&lt;h2 id="what-comes-next"&gt;What Comes Next?&lt;a class="headerlink" href="#what-comes-next" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the &lt;a href="https://jackdewinter.github.io/2019/12/22/markdown-linter-parser-testing-strategy/"&gt;next article&lt;/a&gt;,
I look at the work that needs to
be done and come up with general strategies that I use to implement the parser
required for the linter.  With the specification’s 637 examples as a base for the
scenario tests, good planning is needed to ensure the work can progress forward.&lt;/p&gt;</content><category term="Software Quality"></category><category term="markdown linter"></category></entry><entry><title>Markdown Linter - Collecting Requirements</title><link href="https://jackdewinter.github.io/2019/12/08/markdown-linter-collecting-requirements/" rel="alternate"></link><published>2019-12-08T00:00:00-08:00</published><updated>2019-12-08T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2019-12-08:/2019/12/08/markdown-linter-collecting-requirements/</id><summary type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;My website is now up and running, even though in my mind it took forever.  To make sure
everything was “just so”, I went through each article with a fine-toothed comb multiple
times, each with a slightly different thing I was looking for.  In the end, it worked
out …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;My website is now up and running, even though in my mind it took forever.  To make sure
everything was “just so”, I went through each article with a fine-toothed comb multiple
times, each with a slightly different thing I was looking for.  In the end, it worked
out, but I wished I could have automated at least some of that work and reduced the time
it took to do it.  And I also have a lingering question of whether or not I got
everything, or did I miss something out?&lt;/p&gt;
&lt;h2 id="what-is-a-linter"&gt;What Is a Linter?&lt;a class="headerlink" href="#what-is-a-linter" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A long time ago, when I first heard the term “lint”, I thought someone was referring to
the stuff that you find in the clothes dryer trap that you need to clean out.  According
to &lt;a href="https://en.wikipedia.org/wiki/Lint_(software)"&gt;Wikipedia&lt;/a&gt;, my guess was close.
Similar to the “undesirable bits of fiber and fluff found in sheep’s wool” from the
Wikipedia article, software linters are used to detect undesirable practices and
patterns in the objects they scan.  Once pointed out, the development team can then
decide whether or not to address the issue or ignore the issue.&lt;/p&gt;
&lt;h2 id="doing-my-research"&gt;Doing My Research&lt;a class="headerlink" href="#doing-my-research" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I started looking around, and even though there are a number of Markdown-to-HTML command
line programs out there, finding a solid Markdown linter was another story.  I did find
a couple of starts at making one, but not a finished one that I could use.  The only
exception was the NPM-based
&lt;a href="https://marketplace.visualstudio.com/items?itemName=DavidAnson.vscode-markdownlint"&gt;Markdownlint&lt;/a&gt;
by David Anson.  This VSCode plugin is pretty much a standard for anyone creating
content in Markdown using VSCode, with well over 1.3 million downloads as of the writing
of this article.  By default, as you save articles, this linter executes and produces a
list of issues in the &lt;code&gt;Problems&lt;/code&gt; section at the bottom of the VSCode editor.&lt;/p&gt;
&lt;p&gt;This tool is indeed handy while writing an article, but the act of verifying multiple
articles becomes a bit of chore.  My general process was to open a document I wanted to
inspect, make a small whitespace changes, save the file, and examine the &lt;code&gt;Problems&lt;/code&gt;
section to see what the linter came up with.  Two things were more annoying about this
process that others.  The first issue is that any issue for any file that is open is
displayed in that section.  If I wanted to be efficient, it meant closing every other
file and just working on a single file at a time.  The second issue is that other
plugins write their problems there as well.  As a lot of my content is technical, there
are a fair number of spelling issues that arise that I need to ignore.  Once again,
neither one of these issues is a bad thing, just annoying.&lt;/p&gt;
&lt;h2 id="what-are-the-requirements"&gt;What Are The Requirements?&lt;a class="headerlink" href="#what-are-the-requirements" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Doing some thinking about this during the couple of weeks that I worked on the website,
a fairly decent set of requirements crystalized:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;must be able to see an accurate tokenization of the markdown document before translating to HTML&lt;ul&gt;
&lt;li&gt;working with an accurate tokenization remedies any translation problems instead of translating from HTML&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;all whitespace must be encoded in that token stream as-is&lt;ul&gt;
&lt;li&gt;for consistency, want an exact interpretation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;initial tokenization for GitHub Flavored Markdown only, add others later&lt;ul&gt;
&lt;li&gt;initial tests against the &lt;a href="https://github.github.com/gfm/"&gt;GitHub Flavored Markdown specs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;plans to later add other flavors of parser&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;must be able to provide a consistent lexical scan of the Markdown document from the command line&lt;ul&gt;
&lt;li&gt;clean feedback on violations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;extending the base linting rules should require very little effort&lt;ul&gt;
&lt;li&gt;clear support for adding custom linting rules.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;written in Python&lt;ul&gt;
&lt;li&gt;good cross-platform support&lt;/li&gt;
&lt;li&gt;same language as Pelican, used as the
  &lt;a href="https://jackdewinter.github.io/2019/08/18/static-websites-choosing-a-static-web-site-generator/"&gt;Static Site Generator&lt;/a&gt; for my website&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While there are only 5 requirements, they are important.  The first two requirements
speak to reliability: the parsed Markdown tokens should be complete. The third
requirement is for stability: write against one specification with a solid set of test
cases before moving on to others.  The fourth requirement is all about usability: the
linter can be run from any appropriate command line.  Finally, the fifth requirement is
about extensibility:  add any needed custom rules.&lt;/p&gt;
&lt;p&gt;From my point of view, these requirements help me visualize a project that will help me
maintain my website by ensuring that any articles that I write conform to a simple set
of rules.  Those rules can be checked by a script before I commit them, without having
to load up a text editor.  Simple.&lt;/p&gt;
&lt;h2 id="why-is-this-important-to-me"&gt;Why Is This Important To Me?&lt;a class="headerlink" href="#why-is-this-important-to-me" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Writing this section, it took me a couple of tries to word this properly.  In the end, I
settled on a basic phrase:  It is a tool that I can use to make a software project
better.&lt;/p&gt;
&lt;p&gt;In other parts of my professional life, I take a look at things such as a Java project,
and try and improve the quality of that project.  The input is mainly Java source code
and the output is mainly JAR files that are executed by a JVM.  My website is no
different.  Remove Java source code and insert Markdown documents.  Remove JAR files
executed by a JVM and insert HTML files presented by a browser.  There are a few
differences between the two types of projects, but in all of the important ways, they
are the same.&lt;/p&gt;
&lt;p&gt;I took the time to manually scan each article for my website multiple times before I
did my website’s soft release.  To me, it just makes sense that there should be an
easier way to perform that process.  Easier in terms of time, and easier in terms of
consistency.  Unless I am missing something out there in the Internet, the only project
that came close to fulfilling my requirements was &lt;code&gt;Markdownlint&lt;/code&gt;, and it still had some
things missing.  I came to the realization that to be able to lint a Markdown file
against a set of rules, I was going to have to write my own Markdown parser.&lt;/p&gt;
&lt;p&gt;In the last couple of decades of professional life, I have written many parsers, so that
part of the project doesn’t scare me.  Due to the great works of the people at the
&lt;a href="https://github.github.com/gfm/"&gt;GFM site&lt;/a&gt;, there we a solid number of test cases that I
can test the parser against.  The extensibility issue would make me look at different
ways to integrate code into my tool, so a plus there.  All in all, a decent number of
things I have to get right, but nothing too far out of my field of experience.&lt;/p&gt;
&lt;p&gt;Sure it would be hard in places… but also a challenge!  Just the kind of thing I like!&lt;/p&gt;
&lt;h2 id="what-comes-next"&gt;What Comes Next?&lt;a class="headerlink" href="#what-comes-next" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In the &lt;a href="https://jackdewinter.github.io/2019/12/16/markdown-linter-setting-up-parser-tests/"&gt;next article&lt;/a&gt;, I start breaking down the
requirements for the Markdown parser and document how I will setup the tests for it.
As I am parsing a well-known format with varying implementations already available, it
is important to stay focused on one implementation and have a solid set of tests to
ensure I don’t go backwards in my development.&lt;/p&gt;</content><category term="Software Quality"></category><category term="markdown linter"></category></entry><entry><title>Software Quality: Reliability</title><link href="https://jackdewinter.github.io/2019/11/10/software-quality-reliability/" rel="alternate"></link><published>2019-11-10T00:00:00-08:00</published><updated>2019-11-10T00:00:00-08:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2019-11-10:/2019/11/10/software-quality-reliability/</id><summary type="html">
&lt;p&gt;In the main article titled &lt;a href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"&gt;What is Software Quality?&lt;/a&gt;, I
took a high level look at what I believe are the 4 pillars of software quality.  This article
will focus specifically on the Reliability pillar, with suggestions on how to measure
Reliability and how to write good requirements for this …&lt;/p&gt;</summary><content type="html">
&lt;p&gt;In the main article titled &lt;a href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"&gt;What is Software Quality?&lt;/a&gt;, I
took a high level look at what I believe are the 4 pillars of software quality.  This article
will focus specifically on the Reliability pillar, with suggestions on how to measure
Reliability and how to write good requirements for this pillar.&lt;/p&gt;
&lt;h2 id="introduction"&gt;Introduction&lt;a class="headerlink" href="#introduction" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;From the main article on
&lt;a href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/"&gt;What is Software Quality?&lt;/a&gt;,
the essence of this pillar can be broken down into two questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the software do the task that it is supposed to do?&lt;/li&gt;
&lt;li&gt;Does the software execute that task in a consistent manner?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This article will take an in-depth look at common types of tests, discussing how those
tests can help us gather the information necessary to answer those questions.  At
the end of this article, the section
&lt;a href="https://jackdewinter.github.io/2019/11/10/software-quality-reliability/#how-to-measure-reliability"&gt;How To Measure Reliability&lt;/a&gt; will use that information to provide a cohesive answer
to those questions.&lt;/p&gt;
&lt;h2 id="how-does-testing-help-measure-reliability"&gt;How Does Testing Help Measure Reliability?&lt;a class="headerlink" href="#how-does-testing-help-measure-reliability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;As discussed in the main article’s section on
&lt;a href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/#Reliability"&gt;Reliability&lt;/a&gt;,
many articles on testing and reliability refer to a test pyramid that defines the 4
basic types of reliability tests: unit tests, functional/integration tests, scenario
tests, and end-to-end tests.  While those articles often have slightly different takes
on what the pyramid represents, a general reading of most of those articles leaves me
with the opinion that each test in each section of the pyramid must pass every time.
With tests and reliability being closely related, it is easy for me to draw the
conclusion that if tests must pass every time, then reliability is a binary choice:
they all pass and the project is reliable, or one or more fail and the project is not
reliable.&lt;/p&gt;
&lt;p&gt;As such, my main question is: Does it have to be a
binary choice?  Are the only two choices that either all tests did pass or all tests did
not pass? If the answer to that question is a binary answer, then the answer is simple:
it is either 100% reliable or 0% reliable.  More likely, there are other answers that
will give use a better understanding of how to measure reliability and how to interpret
those measurements.&lt;/p&gt;
&lt;h2 id="can-we-identify-groups-of-tests"&gt;Can We Identify Groups of Tests?&lt;a class="headerlink" href="#can-we-identify-groups-of-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Before determining whether or not reliability is a binary choice, I feel that it is important
to make some foundational decisions on how to measure reliability based on the types of tests
that are already identified.  To aid in making those decisions, it helps to examine the four
categories of tests, looking for groupings between them.&lt;/p&gt;
&lt;p&gt;&lt;img alt="test pyramid" src="https://jackdewinter.github.io/images/quality-1/test-pyramid.png"/&gt;&lt;/p&gt;
&lt;p&gt;Using the definitions established in the main article, unit tests are used to test
the reliability of individual software components and functional tests are used to test the
reliability of more than one of those components working together.  Both of these categories
are used to determine the reliability of the components themselves, and not their objectives.
As such, they make for a good grouping as they have a common responsibility: technical
reliability.&lt;/p&gt;
&lt;p&gt;Observing the scenario tests and end-to-end tests through a similar lens, those tests are used to
determine whether or not the software project meets its business requirements.  The end-to-end tests are often a set of
tests that are very narrow and deep of purpose.   At a slightly lower level, the scenario
tests provide extra support to those end-to-end tests by breaking those “bulky” end-to-end
tests into more discrete actions matched to the overall business use cases for the project.
A good grouping for these tests is by what they: business reliability.&lt;/p&gt;
&lt;p&gt;Another way to think about it is to view the groups of tests in terms of whether or not they
are inside or outside of the
&lt;a href="https://www.techopedia.com/definition/3552/black-box-testing"&gt;black box&lt;/a&gt;
that is the software project.  The first group of tests verify the inside of that black box,
ensuring that all of the technical requirements or “what needs to be done to meet
expectations” are met.  The second group of tests verify the outside of that black box,
ensuring that all of the business requirements or “what is expected of the project” are met.&lt;/p&gt;
&lt;p&gt;[Add picture of pyramid showing inside and outside?]&lt;/p&gt;
&lt;h2 id="give-me-an-example"&gt;Give Me an Example&lt;a class="headerlink" href="#give-me-an-example" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For the follow sections, I use the example of a simple project that uses a data store to
keep track of contact information. By providing a simple example that most developers have
encountered before, my hope is that it will make it easier for the reader to picture the
different types of tests and how they will interact with their team’s project.  As I
examine each type of tests, I try and explain my thinking on what I write and how
I write it for that group of tests, hoping to guide others on making better decisions
for their testing strategy.&lt;/p&gt;
&lt;p&gt;Note that I do not believe that the definition of “data store” is relevant to the example,
therefore the definition of “data store” is left up to the reader’s imagination and
experience.&lt;/p&gt;
&lt;h3 id="end-to-end-tests"&gt;End-To-End Tests&lt;a class="headerlink" href="#end-to-end-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Starting at the top of test pyramid, each end-to-end test needs to be a solid,
representative test of the main focus of the project itself.  These tests are usually a
small set of tests meant as a solid litmus test on whether the software project is
reliably meeting the requirements of the project.  In forming the initial end-to-end
tests, my intent is to start with a focus on positive cases which occur more than
60% of the time.&lt;/p&gt;
&lt;p&gt;For the example project, I started with a test to successfully add a new contact. As a
nice bonus, starting with that test allowed me to add the remove, list, and update
end-to-end tests, as they all need to add a new contact as a foundation of each of those
3 individual tests. Given my experience measuring quality, I believe that all of those
tests together provide that check with confidence for the example project.  If I had
found out
that the number of end-to-end tests I needed was more than a handful of tests, I would
have then examined the requirements and try to determine if the project had too many
responsibilities.  Doing this exercise with a new project often helps me figure out if
the project is properly scoped and designed, or if it requires further refinement.&lt;/p&gt;
&lt;p&gt;Having identified the end-to-end tests for a project and assuming that no further
refinement is necessary, I rarely write source code for these tests right away.  Most
of the time I just add some simple documentation to the project outlined in
&lt;a href="https://en.wikipedia.org/wiki/Pseudocode"&gt;pseudocode&lt;/a&gt; to capture that information.  I
find that the main benefit of doing this in the early stages is to provide a
well-defined high level goal that myself and my team can work towards. Even having rough
notes on what the test will eventually look like can help the team work towards that
goal of a properly reliable project.&lt;/p&gt;
&lt;h3 id="scenario-tests"&gt;Scenario Tests&lt;a class="headerlink" href="#scenario-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Still on the outside of the box, I then add a number of scenario tests to expand on the
scope of each of end-to-end tests.  For these tests, I focus on
&lt;a href="https://en.wikipedia.org/wiki/Use_case"&gt;use cases&lt;/a&gt;
that the user of the project will experience in typical scenarios. The intent here is to
identify the scenario tests that collectively satisfy 90% or more of the projected
business use cases for a given slice of the project.&lt;/p&gt;
&lt;p&gt;For the example project, adding a test to verify that I can successfully add a contact
was the first scenario test that I added.  I then added a scenario for the negative use
case of adding a contact and being told there are invalid fields in my request and a
third for a contact name that already existed.  Together, these scenarios met my bar for
the “add a contact” slice of the scenarios for the project.&lt;/p&gt;
&lt;p&gt;It is important to remember that these are tests that are facing the user and systems
they interact with. Unless there is a very strong reason to, I try and avoid scenario
tests that depend on any specific state of the project unless the test explicitly sets
that state up.  From my experience, such a dependency on external setup of state is very
fragile and hard to maintain.  It also raises the question on whether or not it is a
realistic or valuable test if that setup is not something that the project itself sets
up.&lt;/p&gt;
&lt;h4 id="why-only-those-3-scenario-tests"&gt;Why only those 3 scenario tests?&lt;a class="headerlink" href="#why-only-those-3-scenario-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Here is a simple table on what types of scenario tests to add that I quickly put
together for that project.  The estimates are just that, examples,  but helped me
determine if I hit the 90% mark I was aiming for.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Percentage&lt;/th&gt;
&lt;th&gt;Scenario&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Success&lt;/td&gt;
&lt;td&gt;60%&lt;/td&gt;
&lt;td&gt;Add a contact successfully to the project.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bad/Invalid Data&lt;/td&gt;
&lt;td&gt;25%&lt;/td&gt;
&lt;td&gt;Add an invalid contact name and validate that a ValidateError response is returned.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Processing Error&lt;/td&gt;
&lt;td&gt;10%&lt;/td&gt;
&lt;td&gt;Add an contact name for an already existing contact and validate that a ProcessingError response is returned.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I sincerely believe that between those 3 scenario tests, I can easily defend that they
represent 90%+ of the expected usage of the project for the specific task of adding a
contact. While the percentages in the table are
&lt;a href="https://en.wikipedia.org/wiki/Scientific_wild-ass_guess"&gt;swags&lt;/a&gt;
that seem to be “plucked out of thing air”, I believe they can be reasonably
defended&lt;sup id="fnref:defense"&gt;&lt;a class="footnote-ref" href="#fn:defense"&gt;1&lt;/a&gt;&lt;/sup&gt;.  This defense only needs to be reasonable enough to get the project
going. Once the project is going, real data can be obtained by monitoring and more
data-driven percentages can be used, if desired.&lt;/p&gt;
&lt;h4 id="how-did-i-get-there"&gt;How did I get there?&lt;a class="headerlink" href="#how-did-i-get-there" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;From experience, there are typically 4 groups of action results, and therefore,
scenarios: the action succeeded, the action failed due to bad data, the action failed
due to a processing error, or the action failed due to a system error.&lt;/p&gt;
&lt;p&gt;The first scenario test represents the first category.  Unless there was a good reason to
show another successful “add” use case, I will typically stick with a single “add” test.
As the goal is to achieve 90% of the typical use cases for the project, unless a
variant of that success case is justified by it’s potential contribution towards the 90%
total, it can be better performed by other tests.  In addition, tests on variations of
the input data are better performed by unit tests and functional tests, where executing
those tests have lower setup costs and lower execution costs.&lt;/p&gt;
&lt;p&gt;The second scenario test is used to satisfy the second group of tests where the data is
found to be bad or invalid. In general, I use these to test that there is consistent
error handling &lt;sup id="fnref:errorHandling"&gt;&lt;a class="footnote-ref" href="#fn:errorHandling"&gt;2&lt;/a&gt;&lt;/sup&gt; on the boundary between the user and the project.  At
this level, I ideally need only one or two tests to verify that any reporting of bad or
invalid data is being done consistently. By leaving the bulk of the invalid testing to
unit testing and/or functional testing, I can simulate many error conditions and check
them for consistent output at a low execution cost.  To be clear, if possible I try and
verify the general ability that consistent error handling is in place and not that a
specific instance of error is being reported properly.&lt;/p&gt;
&lt;p&gt;The third scenario test is used to verify the third group of tests where data is valid
but fails during processing.  Similar to the second group of tests, there is an
assumption that the reporting of processing errors should be done consistently.  However,
as most processing errors result due to a sequence of actions originating from the user,
representative types of processing errors should be tested individually.  The key to this
type of scenario tests is to represent processing errors that will help the group of
scenario tests hit that 90% mark.  Relating this to the example project, getting a
“already add a record with that name” response from the project is something that would
occur with enough frequency to qualify in my books.&lt;/p&gt;
&lt;p&gt;From experience, the fourth group of tests, testing for system errors, rarely makes it
to the level of a scenario test.  In this example, unless a system error is so
consistent that it was estimated to occur more than 10% of the time, a higher priority
is placed on the other types of responses.&lt;/p&gt;
&lt;p&gt;One of the exceptions to these generic rules are when a business requirement exists to
provide extra focus on a given portion of the interface.  These requirements are often
added to a project based on a past event, either in the project or in a related project.
As the business owners have taken the time to add the business requirement due to its
perceived priority, it should have a scenario test to verify that requirement is met.&lt;/p&gt;
&lt;p&gt;In the contact manager example, I made a big assumption that unless there were
requirements that stated otherwise, the data store is local and easy to reach.  If
instead we are talking about a project where the data is being collected on a mobile
device and relayed to a server, then a test in this last group of system errors would
increase in value.  The difference that this context introduces is that it is expected
that project will fail to reach the data store on a frequent basis, and hence, providing
a scenario for that happening helps us reach that 90% goal.&lt;/p&gt;
&lt;h3 id="commonalities-between-end-to-end-tests-and-scenario-tests"&gt;Commonalities between End-to-end tests and scenario tests&lt;a class="headerlink" href="#commonalities-between-end-to-end-tests-and-scenario-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;While I took the long way around describing end-to-end tests and scenario tests, I
believe the journey was worth it.  These two types of tests test against the external
surface of the project, together painting a solid picture of what that external surface
will look like once the project is done.  For both of those tests, the project needs
clear business requirements on what benefit it provides to the user, which will be
highlighted by translating the requirements into the various tests.  By including either
actual data (for existing projects) or projected data (for new projects) on the usage
patterns for that project, the requirements can be prioritized to ensure the most
frequently used requirements are more fully tested.&lt;/p&gt;
&lt;p&gt;For each of those requirements and goals, the team can then set goals for the project
based on those documented requirements.  By codifying those goals and requirements with
end-to-end and scenario tests, you firm up those goals into something concrete.  Those
actions allow the team to present a set of tests or test outlines to the authors of the
requirements, validating that things are going in the right direction before writing too
much source code or setting up of interfaces with the user.  That communication and
changing the course before writing code can save a team hours, days, or weeks,
depending on any course changes discovered.&lt;/p&gt;
&lt;p&gt;What happens if the requirements change?  The project has a set of tests that
explicitly test against the outside of the box, and informs the team on what changes
will be needed if that requirement change is applied to the project.  At the very least,
it starts a conversation with the author of the requirement about what the external
surface of the project will look like before and after the change.  With that
conversation started, the team can have a good understanding of how things will change,
with some level of confidence that the change is the change specified by the
requirements author.&lt;/p&gt;
&lt;h3 id="unit-tests-and-functional-tests"&gt;Unit Tests and Functional Tests&lt;a class="headerlink" href="#unit-tests-and-functional-tests" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Transitioning to inside of the black box, unit tests and functional tests are more
understood by developers and more frequently used than end-to-end tests or scenario
tests. The unit tests isolate a single component (usually a class) and attempt to test
that each interface of that component and is functioning properly.  The functional tests
do the same thing, but with a single group of components that work together as a single
component rather than a single component itself.&lt;/p&gt;
&lt;p&gt;From an implementation point of view, the main difference is in how these tests are
created. Unit tests, as they are testing a single component, should only contain a
project reference to the one component being tested.  If the components are created
properly and have a good separation from the rest of the project, this should be
achievable for a good number of
components for the project, especially the support components.  Therefore, the degree to
which these tests are successful is determined by the amount of clean division of
responsibilities the project has between it’s components.&lt;/p&gt;
&lt;p&gt;Functional tests complete the rest of the inside-of-the-box testing by testing individual
components with related components, in the way they are used in a production
environment.  With these tests, the degree to which these tests are successful is the
ability to inject the project dependencies into one or more of the components being
tested, coupled with the clean division of responsibilities needed for good unit tests.
While using a concept such as the interface concept from Java and C# is not required, it
does allow the injection of dependencies to be performed cleanly and with purpose.&lt;/p&gt;
&lt;p&gt;To enable groups of functional tests to be as independent of the components outside of
their group as possible, &lt;a href="https://en.wikipedia.org/wiki/Mock_object"&gt;mock objects&lt;/a&gt; are
often used to replace concrete classes that are part of your project.  If interfaces are
used in your project to allow for better
&lt;a href="https://en.wikipedia.org/wiki/Dependency_injection"&gt;dependency injection&lt;/a&gt;,
your functional tests can create mock objects that reside with your tests.  This provides
more control and reliability on what changes you are making from the live instance of
the interfaces, for the sake of testing.  If interfaces are not supplied for better
dependency injection, a mocking library such as the Java
&lt;a href="https://site.mockito.org/"&gt;Mockito&lt;/a&gt;
are required to replace test dependencies with reliable objects.&lt;/p&gt;
&lt;h4 id="back-to-our-example"&gt;Back to our example&lt;a class="headerlink" href="#back-to-our-example" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Using the example project as a template, we know from the section on
&lt;a href="https://jackdewinter.github.io/2019/11/10/software-quality-reliability/#Scenario-Tests"&gt;scenario tests&lt;/a&gt;
that we need to test for valid inputs when adding a new contact.  To add
coverage for the component containing the “add a contact” logic as a unit test, it’s
success is determined by how much of the handling the external interface is in the one
component. If that component contains all of the code needed to handle that external
request in one method, it is extremely hard to test that component without bringing in
the other components.  That is definition of a functional test, not a unit test.  As an
alternative, if the validation of the input can be condensed into it’s own component and
removed from that method, that validation component can be unit tested very effectively.&lt;/p&gt;
&lt;p&gt;Applying that refactoring pattern a couple of more times in the right ways, the project’s
ability to be functionally tested increases.  As an added bonus,  depending on how the
refactoring is accomplished, new unit tests can be added based on the refactoring,
gaining measurable confidence on each additional component tested.  &lt;/p&gt;
&lt;p&gt;Using the adding a contact example again, having refactored the input validation to a
validation class could be followed by the following changes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create a new component for the handling of “add a contact” and decouple it from logic of the handling of the external interface&lt;/li&gt;
&lt;li&gt;move the user authentication and authorization logic into it’s own component&lt;/li&gt;
&lt;li&gt;move the persisting of the new contact logic into it’s own component&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From a functional test point of view, each of these refactorings makes it easier to test.
For the first refactoring, instead of having to rely on all functional testing going
through the external interface, which may include costly setup, we can create a local
instance of the new component and test against that.  If interfaces are used for the
remaining two refactorings, then test objects can be used instead of the “live” objects,
otherwise a mocking library can be used to replace those objects with more predictable
objects.&lt;/p&gt;
&lt;h2 id="how-is-each-group-of-tests-measured"&gt;How is each group of Tests Measured?&lt;a class="headerlink" href="#how-is-each-group-of-tests-measured" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;On this winding journey to determine how to measure reliability, I explored the relevant
elements of the four main types of tests.  I believe that I was successful in showing a
clear delineation between the two groups of tests and the benefits each group provides.
To recap, the outside-of-the-box group validates the expectations to be met, matched
against the requirements set out for the project.  The inside-of-the-box group validates
how those exceptions are met, matched against the external interfaces for the project.&lt;/p&gt;
&lt;p&gt;These two distinct foundations are important, as the two distinct groups of tests require
two distinct groups of measurements.&lt;/p&gt;
&lt;p&gt;The first group, scenario tests and end-to-end tests, are measured by scenario coverage.
Scenario coverage measures the number of tests that successfully pass against the total
number of scenario tests and end-to-end tests for that project.  As this group of tests
is measuring the business expectations of the project, this measurement is a simple
fraction: the number of passing tests as the numerator and the number of defined tests
as the denominator.&lt;/p&gt;
&lt;p&gt;The second group, unit tests and functional tests, are measured by source code coverage,
or code coverage for short.  Code coverage can be specified along 6 different axes:
class, method, line, complexity, blocks, and lines.  Different measurement tools will
provide different subsets of those measurements, but in the end they are all relaying
the same thing: the points in the project’s source code that are not properly tested.&lt;/p&gt;
&lt;h2 id="back-to-the-original-question"&gt;Back to the original question&lt;a class="headerlink" href="#back-to-the-original-question" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Does it (the measuring of reliability) have to be a binary choice?&lt;/p&gt;
&lt;p&gt;It depends.&lt;/p&gt;
&lt;p&gt;In an ideal world, the answer to that question is yes, but we do not live in an ideal
world.  In the real world, we have a decision to make for either group of tests on what
is good enough for the project and that group of tests.&lt;/p&gt;
&lt;p&gt;If the suggestions of this article are followed, then a condition of releasing the
project to a production state is 100% scenario coverage.  Anything less than 100% means
that critical use cases for the project are not complete, hence the project itself is not
complete.  &lt;/p&gt;
&lt;p&gt;To achieve the 100% coverage without adding new project code, updated requirements are
needed from the requirements author, say a project manager, to change the composition of
the scenario tests and end-to-end tests.  This may include removing some of these
tests as the release goals for the project are changed.  While changing and removing
goals and their tests, may seem like cheating to some people, the other option is
very risky.&lt;/p&gt;
&lt;p&gt;It should be evident that if a project is released without all scenario tests and
end-to-end tests passing, that team is taking a gamble with their reputation and the
reputation of the project.  It is better to adjust the tests and goals, and communicate
those changes, than to take a risk on releasing something before it meets those goals.&lt;/p&gt;
&lt;p&gt;Following the suggestions of this article for code coverage is a more nuanced goal, and
really does depend on the project and the situation. If architected and designed to
support proper testing from the beginning, I would argue that 95%+ code coverage is easy
and desirable.  If you are adding testing to an already existing project or do not have
the full support of the developers on the project, this number is going to be lower.&lt;/p&gt;
&lt;p&gt;Another factor is the type of project that is being tested and who will use it.  If you
are creating this project to support people inside of your company, it is possible that
one of the requirements is to have a lower initial code coverage target to allow the
project to be used right away and alleviate some internal company pressure.  If the
project is something that will represent you and your company on the international stage,
you will have to balance the time and effort needed to meet a higher bar for code
coverage with the need to get the project out where it can be used.  As with many things,
it is a matter of negotiation and balance between the various requirements.&lt;/p&gt;
&lt;h2 id="what-is-really-important"&gt;What Is Really Important&lt;a class="headerlink" href="#what-is-really-important" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I want to stress that I believe that the important thing is that each project measures
where they are against whatever goals they set for their project. The team doesn’t need
to always maintain a near-100% code coverage measure, but that team needs to know where
they stand.  This will influence and inform the people that author the requirements and
adjust the priorities for the team.  Any negotiations within the team can then cite this
information and use it to help with the balancing act of adding new features, fixing
existing bugs, and enhancing code quality (in this case, increasing code coverage).&lt;/p&gt;
&lt;h2 id="how-to-measure-reliability"&gt;How To Measure Reliability&lt;a class="headerlink" href="#how-to-measure-reliability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To answer the question “Does the software do the task that it is supposed to do?”,
scenario coverage is measured.  Scenario coverage for end-to-end tests and scenario
tests should always be at 100% when a production release of the project is performed.
This measurement is binary.  Until that release (or next production release) is
performed, adding or changing these tests based on the requirements for the next release
will inform the team and any stakeholders of how close the team is to satisfying those
requirements for that release.&lt;/p&gt;
&lt;p&gt;To answer the question “Does the software execute that task in a consistent manner?”,
code coverage is measured.  Code coverage for unit tests and functional tests should
strive for 95% code coverage along all 6 axes with all active tests completing
successfully 100% of the time.  The test completion percentage must be non-negotiable,
but the code coverage percentage must take into account the maturity of the project and
the usage of the project.  This measurement is non-binary.  However, it is important to
know your project’s code coverage measurement, and how it trends over time. While the
measurement is non-binary, it is suggested to create a binary rule that
specifies what the minimum percentage is for each axis, failing the rule if that
specific metric falls below the goal percentage.&lt;/p&gt;
&lt;h2 id="wrapping-it-up"&gt;Wrapping It Up&lt;a class="headerlink" href="#wrapping-it-up" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;By breaking down the types of tests that are expected for a given project, the two
different types of measurements of reliably become more evident.  Scenario coverage
is determined by outlining the major scenarios for using a project and writing
end-to-end tests and scenario tests against them.  Scenario coverage must be a binary
measurement at release time.  Code coverage is determined by using tools to measure
which parts of the code are executed when running functional tests and unit tests.
Code coverage is a non-binary metric that must have a minimum bar for coverage that is
met for the project, and determined on the merits of the project itself.&lt;/p&gt;
&lt;p&gt;By using these two measurements, I hope that I have shown that it is possible to provide
a way to empirically measure reliability.  By having a project be transparent about how
it is reaching those measurements and what they are, any team can provide meaningful and
understandable measurements of reliability.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:defense"&gt;
&lt;p&gt;If asked, I could easily defend the percentages.  For the success case, I would assume that half the 60% number will come from first try successes and half the number will come from success that occurred after people fixed errors returned from the other two tests and resubmitted the data.  While the other two categories are somewhat guesswork, from my experience validation errors are 2-3 times more common than an “existing contact” processing error.  Note that in the absence of real data, these are estimates that do not have to be perfect, just reasonable. &lt;a class="footnote-backref" href="#fnref:defense" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:errorHandling"&gt;
&lt;p&gt;In designing any type of project, you should seek to have clear and consistent interfaces between your project and the users of the project.  An extension of that statement is that any responses you return to your user should be grouped with related responses and returned in a common data structure or UI element to avoid confusion. &lt;a class="footnote-backref" href="#fnref:errorHandling" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Software Quality"></category><category term="measuring software quality"></category><category term="software reliability"></category><category term="end-to-end tests"></category><category term="scenario tests"></category><category term="functional tests"></category><category term="unit tests"></category><category term="code coverage"></category><category term="scenario coverage"></category></entry><entry><title>What is Software Quality?</title><link href="https://jackdewinter.github.io/2019/09/15/what-is-software-quality/" rel="alternate"></link><published>2019-09-15T00:00:00-07:00</published><updated>2019-09-15T00:00:00-07:00</updated><author><name>Jack De Winter</name></author><id>tag:jackdewinter.github.io,2019-09-15:/2019/09/15/what-is-software-quality/</id><summary type="html">
&lt;p&gt;When introducing myself to someone professionally, I usually start with the normal
“Hi, my name is …” that is boiler-plated on nametags the world over.  Getting past that
initial point, if the person is so inclined, they ask that always fun lead off question “So,
what do you?”  For me, I …&lt;/p&gt;</summary><content type="html">
&lt;p&gt;When introducing myself to someone professionally, I usually start with the normal
“Hi, my name is …” that is boiler-plated on nametags the world over.  Getting past that
initial point, if the person is so inclined, they ask that always fun lead off question “So,
what do you?”  For me, I always respond with “I am an SDET”&lt;sup id="fnref:SDET"&gt;&lt;a class="footnote-ref" href="#fn:SDET"&gt;1&lt;/a&gt;&lt;/sup&gt;, to which anyone not in the
software industry replies back with “Um.... What is that?”&lt;/p&gt;
&lt;p&gt;Spewing out “It means I am a Software Development Engineer in Test.”, I wait for the response
that most people use: “Oh, so you are a tester.”  Often with gritted teeth, I try and explain
that testing is only a small part of what I do.  If I think they are still listening, I
given them my quick elevator pitch that emphasizes that I focus on helping to produce good
quality software by helping to increase the quality of the teams, the projects, and the
processes that I am tasked to assist with.&lt;/p&gt;
&lt;p&gt;Approximately 60-70% the time I win people over
with the elevator pitch, and a pleasant conversation continues.  The next 20-30% of the time,
usually with people not in the software field, I get blank stares and they fixate on the
“test” in the title rather than the “quality” in my description.  The remaining people are
usually Software Development Engineers or SDEs&lt;sup id="fnref:SDE"&gt;&lt;a class="footnote-ref" href="#fn:SDE"&gt;2&lt;/a&gt;&lt;/sup&gt; that for one reason or another, start to
tune out.&lt;/p&gt;
&lt;p&gt;For the percentage of people that I win over, they seem to understand that I focus on quality,
but the follow up question is almost always: “What does quality software mean to you?”&lt;/p&gt;
&lt;h2 id="where-do-we-start"&gt;Where do we start?&lt;a class="headerlink" href="#where-do-we-start" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;For me, I almost always start at the beginning with requirements.  Whether they are
spoken or written down, each project has a set of requirements.  It could be the requirements
are to “explore my ability to use X” or “fix the X in the Y project” or “create a project
that can help me X”, but every project has requirements.&lt;/p&gt;
&lt;p&gt;In the software development industry, requirements are often presented
to teams that are hard to deal with or are left to the developers to write themselves.  This
practice is so prolific that Scott Adam’s Dilbert site has pages and pages of instance where
&lt;a href="https://dilbert.com/search_results?terms=User%20Requirements"&gt;requirements are talked about&lt;/a&gt;.
One example is when a manager talks to their team and
informs them that some process needs to be faster by 5%.  Do they have enough information from
that manager to understand the context of the requirement?  Do they expect that increase by a
specific time to meet their own goals?  What does that requirement look like?  How do they
know when they have achieved it?  Is it achievable?  If it is achievable, how do they measure
progress towards that goal?  These are some of the core questions that I believe need
answering.&lt;/p&gt;
&lt;p&gt;As those questions are at the front of my mind, when someone asks me how I define software quality, the first thing I immediately think back to is a course that I once took on setting
&lt;a href="https://en.wikipedia.org/wiki/SMART_criteria"&gt;S.M.A.R.T. requirements&lt;/a&gt;.
In that class, the main focus was on taking unrefined requirements and curating them to a
point where they could be more readily be acted upon.  The instructor made a very good
argument that each requirement must be Specific, Measurable, Assignable, Realistic, and
Time-Related.&lt;/p&gt;
&lt;p&gt;When it comes to software quality, I believe those same questions needs to be asked with
regards to any of the requirements teams put on their software.  But to ask those questions
properly, we need to have some context in which to ask those questions.  To establish that
context, it is helpful to have some guidelines to provide a framework for the requirements.&lt;/p&gt;
&lt;h2 id="establishing-some-guidelines-the-four-pillars"&gt;Establishing Some Guidelines: The Four Pillars&lt;a class="headerlink" href="#establishing-some-guidelines-the-four-pillars" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A good general article for anyone interested in software quality is the
&lt;a href="https://en.wikipedia.org/wiki/Software_quality#Measurement"&gt;Wikipedia article on Software Quality&lt;/a&gt;.
In fact, when asked by people where to get started in the software quality area, I often refer
them to this article solely because of the excellent diagram in the Measurements section on the
right side of the page.&lt;sup id="fnref:Pillars"&gt;&lt;a class="footnote-ref" href="#fn:Pillars"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The diagram in the Measurements section correlates very closely to what I believe are the four
pillars of software quality: Reliability, Maintainability, Efficiency, and Security.  The
diagram then shows how their pillars relate to other attributes: Application Architecture
Standards, Coding Practices, Complexity, Documentation, Portability, and Technical/Functional
Volumes.  From there, it provides more lists of how to break things down, with many references
to other articles.  In short, it is a great place to start from.&lt;/p&gt;
&lt;h2 id="measuring-software-quality"&gt;Measuring Software Quality&lt;a class="headerlink" href="#measuring-software-quality" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Before proceeding to talk about the pillars themselves, I feel strongly that we need to
discuss the categories that I use for measuring the metrics talked about in the Wikipedia
article.  My intention is that by talking about the metrics before discussing each of the
pillars, you can start building a mental model of how to apply them to your projects as you
are reading about them.  From my point of view, making that mental transition from something
abstract that you read about to something concrete that applies to your work is
essential to serious forward momentum on software quality.&lt;/p&gt;
&lt;p&gt;These metrics typically fall into two categories: seldom violated metrics and positive
momentum metrics.&lt;/p&gt;
&lt;p&gt;The seldom violated metrics category contains rules that define rules that are pivotal to the
quality of your project.  Each rule are a combination of a given metric and a maximum
or minimum weighed against that metric.  As a guideline, teams should only ignore
these rules on a case by case basis after providing a reason that is good, defensible, and
documented.
Examples of such metrics are Service Level Agreements (SLAs), Static Code Analysis
(SCA) results, and Test Failure Rates.  Examples of rules are “the TP99 for the X
API is Y millisecond” or “all PMD warnings (Java SCA tool) must be following with
a minimal of suppressions”.&lt;/p&gt;
&lt;p&gt;Furthermore, to make these rules useful and to keep your
team honest, your team needs to publish the selected metrics, with a description of what the
metrics are, how your team measures those metrics, and why your team is measuring them.&lt;/p&gt;
&lt;p&gt;The positive momentum metrics category is usually reserved for metrics that are being
introduced to an already existing project.  When introducing software quality metrics into an
already existing project, it is not realistic to expect those metrics to be adhered to in an
instant.  It is more realistic to expect positive momentum towards the goal
until the point when your team achieves it, at which point is moves to the desired seldom
violated metrics category.  As such, a measure of the momentum of these metrics is used, and is
hopefully in a positive direction. Similar to the previous category, your team should publish
information about the selected metrics, with the added information on when your team feels
they will translate it from the positive momentum category to the seldom violated category.&lt;/p&gt;
&lt;p&gt;Being consistent on these chosen metrics is very important.  While dropping a metric looks
better on any reporting in the short term, it usually negatively impacts the software quality,
perhaps in a way that is not obvious until later. Adding a new metric will show lower the
measured quality in the short term, but increases the measured quality in the long
term.  Your team can negate the short term impact by paying the immediate cost of making the
new metric a seldom violated metric, but that has to be weighed against the other priorities
for your project.  As with everything, it is a balancing act that needs to be negotiated with
your team.&lt;/p&gt;
&lt;h2 id="exploring-the-four-pillars"&gt;Exploring The Four Pillars&lt;a class="headerlink" href="#exploring-the-four-pillars" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Having established that S.M.A.R.T. requirements and the two categories for metrics
from the previous sections are useful in measuring software quality, the focus of the article
can return to the guidelines: the four pillars.  Each one of these pillars will look at
your software project from a different angle, with the goal of providing a set of data points
to formulate a coherent measurement of software quality for that project.&lt;/p&gt;
&lt;p&gt;In the following sections, I strive to describe each of the four pillars, providing a jumping
off point to another article that describes that pillar in a more comprehensive manner.  I
firmly believe that by providing metrics for each pillar that are specific to your project,
with each of those metrics properly categorized into the two measurement categories documented
above, that your team will take a decent step forward in clearly defining software quality for
your project.&lt;/p&gt;
&lt;h3 id="reliability"&gt;Reliability&lt;a class="headerlink" href="#reliability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The essence of this pillar can be broken down into two questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the software do the task that it is supposed to do?&lt;/li&gt;
&lt;li&gt;Does the software execute that task in a consistent manner?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reliability is one of the areas in which “pure” testing shines.  A lot of the tests that
SDEs, SDETs, and testers are asked to write specifically verify if a given object does
what it is supposed to do.  Unit tests determine whether an individual software unit, such as
a class, performs they way it is supposed to.  Functional tests or integration tests take that
a step higher, determining whether a group of related software units do what they are supposed
to do.  Another step higher are the scenario tests, which determine whether the software
project, as a whole, responds properly to various use cases or scenarios that are considered
critical to its operation.  Finally, end-to-end tests or acceptance tests determine whether or
not a group of projects respond properly from an end user’s perspective.&lt;/p&gt;
&lt;p&gt;This pattern is so widely used, any search for
&lt;a href="https://www.bing.com/images/search?q=test+pyramid"&gt;test pyramid&lt;/a&gt;,
will find many variations of the same theme. Different articles on the subject will stress
different points about the pyramid, but they will all generally look like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="test pyramid" src="https://jackdewinter.github.io/images/quality-1/test-pyramid.png"/&gt;&lt;/p&gt;
&lt;p&gt;This pyramid, or other similar pyramids, are interpreted by authors to indicate a specific
things about the tests, to highlight the position of their article.  Some of these
interpretations are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An article on test volume will typically stress that ~70-80% of the tests should be at the unit test level, ~10-15% at the functional test level, ~5-10% at the scenario level, and ~1-3% at the end-to-end level.&lt;/li&gt;
&lt;li&gt;An article on test frequency will typically stress that tests near the bottom of the pyramid should complete within 60 seconds and be executed every time the source code is checked in.  Tests near the top of the pyramid may take minutes or hours and should be executed once a week.&lt;/li&gt;
&lt;li&gt;An article on test fragility will typically stress that tests near the bottom of the pyramid are closer to their components, the expectation is that they will not fail.  Tests near the top of the pyramid require more orchestration between projects and teams, and therefore, are more likely to failure do to environmental or other reasons.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While all of these interpretations have merit, the critical point for me is the issue of
boiling down that information to a small number of bite sized observations that can be easily
measured and communicated. In the upcoming article &lt;code&gt;Software Quality: Reliability&lt;/code&gt;, I will
delve more into breaking the Reliability pillar into S.M.A.R.T. requirements and I provide
suggestions on how it can be measured.&lt;/p&gt;
&lt;h3 id="maintainability"&gt;Maintainability&lt;a class="headerlink" href="#maintainability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The essence of this pillar can be broken down into one question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you are asked to change the software to fix a bug or introduce a new feature, how easy is it to change the software, how many surprises do you expect to encounter, and how confident will you be about the change afterwards?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The best, and most comedic, form of asking this question is captured by this cartoon
from &lt;a href="https://www.osnews.com/"&gt;OSNews&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="WTFs per minute" src="https://mk0osnewswb2dmu4h0a.kinstacdn.com/images/comics/wtfm.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Maintainability is a single pillar that encompasses the most diverse types of processes and
measurements of any of the pillars.  The reason for this is that maintainability is often a
word that is used without a lot of provided context.  For me, a good way to think about
maintainability is that it is the cleanliness of your project.  Different people will have
different experiences, and after asking different people about how “clean” the project is,
the collected answers will almost certainly by different.&lt;/p&gt;
&lt;p&gt;Try this in your office with your colleagues.  Point to a given area of your office and ask
2-5 people how clean a given area, such as your desk is.  Instead of accepting a single
answer, dig in a bit as to why they answered the way they did.  Most likely, you will
get as many distinct answers as people that you talk to.  This exercise illustrates how
hard it is to give a good answer to how maintainable software a given piece of software is.&lt;/p&gt;
&lt;p&gt;The best way to provide metrics for maintainability is usually with various Static Code
Analysis tools.  Almost every mature language has at least one tool to do this, and each tool
usually measures a fair number of metrics.  These metrics will use established (and sometimes
experimental) industry practices to look at the source code of your project and determine
if there are issues that can be addressed.  In addition to those metrics, those same tools
often look for “problematic” and “sloppy” code.  Problematic code is usually some manner of
pattern that a fair number of experts have agreed is a bad thing, such as appending to a
string within a loop.  Sloppy code is usually things like having a variable or a parameter
that is not being used, or a forgotten comment on a public method.&lt;/p&gt;
&lt;p&gt;In addition to Static Code Analysis, teams must continue to strive to have a good set of
documentation on what the project is doing, and regularly maintain that documentation.  While
the “correctness” of the documentation is harder to measure than source code, it is pivotal
for a project. How much of the information on the various projects that your team supports
is in the head of one or two individuals?  What is going to happen if they leave the team
or leave the company.&lt;/p&gt;
&lt;p&gt;Your team should not need volumes of information on every decision that was made, but as a
team, it is imperative to document the major decisions that affect the flow of the project.
It is also a good idea to have solid documentation on building, deploying, and executing
the project. Imagine yourself as a new team member looking at the software project and any
documentation, and honestly ask yourself “How much would I want to run away from that project?”
If the honest answer from each member of the team is something similar to “I’m good”, you
probably have a decent level of documentation.&lt;/p&gt;
&lt;h4 id="a-note-on-static-code-analysis"&gt;A Note On Static Code Analysis&lt;a class="headerlink" href="#a-note-on-static-code-analysis" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Before delving deeper into maintainability, I want to take a minute to talk about Static Code
Analysis.
Typically, Static Code Analysis is used as a gatekeeper for maintainability, and as such,
any suggestions should be strictly followed.  However, Static Code Analysis tends to be an
outlier to the gatekeeper rule in that the metrics need to be “bent” every so often. This
“bending” is accomplished using some form of suppression specified by the Analyzer itself.&lt;/p&gt;
&lt;p&gt;Static Code Analyzers tend to fall into two main categories: style and correctness.&lt;/p&gt;
&lt;p&gt;Any warnings that are generated by a style analyzer should be addressed without fail.
In terms of stylistics, there are very few times where deviating from a common style are
beneficial, and as such should be avoided.  As stylistics can vary from person to
person when writing code, it is useful to supplement the style analyzer with an IDE
plugin that will reformat the source code to meet the team’s stylistics, with the Static
Code Analyzer acting as a backstop in case the IDE formatting fails.&lt;/p&gt;
&lt;p&gt;Warnings generated by correctness analyzers are more likely to require bending.  Most
correctness analyzers are based on rules that are normally correct, but do have exceptions.
As such, your team should deal with these exception by having a follow up rule on when
it is acceptable to suppress the exceptions, and specifically on a case-by-case basis.
It is also acceptable to suppress the exception after generating a future requirement to
address the exception, if your team is diligent on following up with these requests.&lt;/p&gt;
&lt;p&gt;In both cases, it is important to remember that SCAs are used to help your team
keep the project’s maintainability at a healthy level.&lt;/p&gt;
&lt;h4 id="back-to-maintainability"&gt;Back to Maintainability&lt;a class="headerlink" href="#back-to-maintainability" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;In the upcoming article &lt;code&gt;Software Quality: Maintainability&lt;/code&gt;, I will delve more into breaking
the Maintainability pillar into S.M.A.R.T. requirements and I provide suggestions on how it can
be measured.  I will do this by presenting the 4-5 metrics that I consider to be useful as
well as both patterns and anti-patterns to avoid. [ED: Need to rephrase that last sentence.]&lt;/p&gt;
&lt;h3 id="efficiency"&gt;Efficiency&lt;a class="headerlink" href="#efficiency" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The essence of this pillar can be broken down into one question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the software execute that task in a timely manner?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similar to my analogy of maintainability being the cleanliness of your software, efficiency
is whether or not your software is executing “fast enough”.  Coming up with an answer to a
question on whether or not something is “fast enough” is usually pretty easy.  But when
you ask for a definition of what “fast enough” means, that is when people start to have issues
coming up with a solid answer.  In my experience, a large part of the reason for that
vagueness is usually not having a good set of requirements.&lt;/p&gt;
&lt;p&gt;As an example, let’s figure out what “fast enough” means for two different video games that
my family plays: Civilization and Rocket League.&lt;/p&gt;
&lt;p&gt;For the game Civilization (in multiplayer mode), the big delays in the game are the human
interactions and decisions required before a player ends their turn.  It is
also very important that all of the information get conveyed between turns so that the
multiplayer server can accurately record actions in a fair and just manner.  For this game,
“fast enough” for the software is largely dwarfed by the delays that the players introduce.
However, if we have a game with 12 players, 2 of them human and the other 10 using the game’s
AI players, then we can start to formulate what “fast enough” is for the AI players.  It
really depends on the context.&lt;/p&gt;
&lt;p&gt;Rocket League is a different story.
&lt;a href="https://en.wikipedia.org/wiki/Rocket_League"&gt;Rocket League&lt;/a&gt;
is a sequel to the game “Supersonic Acrobatic Rocket-Powered Battle-Cars” released in 2008.
In this game, you play a game of arena soccer using rocket powered cars, each match consisting
of a series of games between teams of 1-3 players.  Unless there is a LAN tournament between
professional teams, it is very rare for more than one player to be in the immediate vicinity
of their teammates, and often players are from different states/provinces and even countries.
For the client software on the player’s computers, “fast enough” is measured by latency and
packet loss.  With each player’s action being relayed to the server and then back out to the
other players, any packet loss or increase in latency will impact the server’s reaction to
various inputs from the player’s controllers.  For this type of game, “fast enough” depends
on a good network connection and a server that is able to process many actions per second.&lt;/p&gt;
&lt;p&gt;As you can see from the video game example, efficiency greatly depends on what the requirements
of the software are.  In the upcoming article &lt;code&gt;Software Quality: Efficiency&lt;/code&gt;, I will delve
more into breaking the Efficiency pillar into S.M.A.R.T. requirements and I provide
suggestions on how it can be measured.&lt;/p&gt;
&lt;h3 id="security"&gt;Security&lt;a class="headerlink" href="#security" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The essence of this pillar can be broken down into one question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How easy is it for a third party to perform malicious actions with your software?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That is one dramatic question.  “Perform malicious actions.”  Wow!  I have read all sorts
of articles on various news sites about those, but surely they cannot affect my software?
That is usually one of the first reactions of a lot of software developers.  Just 10
minutes with a security researcher can open your eyes to what is possible.&lt;/p&gt;
&lt;p&gt;To understand this better, pretend that your software project is on a slide, being viewed
through a microscope.  If you look at the slide without the microscope, you just see your
software on the slide, pretty much the same as any other slide.  However, if you increase your
magnification by one order of magnitude, you see that your project includes your source code
and components developed by other people.  You may be following proper security practices, but
did they?&lt;/p&gt;
&lt;p&gt;Another order of magnitude down, and you are looking at the low level instructions for your
project and any included components.  Once the component was assembled, could a third party
have added some malicious code to that component, executing normally until they activate it?
Was that malicious code in their from the beginning?  Or maybe it is a vulnerability at the
source code, machine code, or machine levels?  Someone can make a small change to a component
to utilize that vulnerability with little effort if they know what they are doing.&lt;/p&gt;
&lt;p&gt;Reversing our direction, if we expand outwards instead of inwards, we have containerization.
Containerization solutions, such as &lt;a href="https://www.docker.com/resources/what-container"&gt;Docker&lt;/a&gt;,
provides a complete computing environment to execute your software within.  Popular with back
end development, you encapsulate your software with it’s intended operating system platform,
reducing the number of platform’s you need to design your software for to 1.  But with
containerization, we also have to ask the same questions of the platform as we did with the
software.  How secure is the operating system that the container uses as it’s base?&lt;/p&gt;
&lt;p&gt;In today’s world of software development, where componentization is key, the software you
write is not the only place where security issues can be introduced.  However, there are
proactive steps you can take to reduce the vectors than users can
follow to use your software maliciously.
In the upcoming article &lt;code&gt;Software Quality: Security&lt;/code&gt;, I will delve more into breaking
the Security pillar into S.M.A.R.T. requirements and I provide suggestions on how they it
be measured.&lt;/p&gt;
&lt;h2 id="back-to-requirements"&gt;Back To Requirements&lt;a class="headerlink" href="#back-to-requirements" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Having explored the 4 pillars, it is important to bring the discussion back to the definition
of good requirements.  Using the information from each of the individual pillar articles in
concert with the information on S.M.A.R.T. terminology, your team can request requirements
that are more focused.  As any focused requirements will be Specific (the S. in S.M.A.R.T.),
it is reasonable to expect that any impact on our 4 pillars will be noted.  Asking for this
change will almost guarantee some negotiations with the team’s stakeholders.&lt;/p&gt;
&lt;p&gt;In my experience, when your team asks for more focused goals from your stakeholders, there
will typically be some pushback from those stakeholders at the beginning.  If your team
has had some requirements mishaps in the past, highlight each mishap and how the ensuing
loss of time and focus could have been avoided usually sways stakeholders.  Don’t point
fingers, but simply point out something like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hey, when we did the X requirement, we all had a different idea on what to fix, and as such,it took X hours of meeting and Y hours of coding and testing to figure out it was the wrong thing.  We just want to help tune the requirements process a bit to help everyone try and avoid that waste.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most stakeholders are being asked to have their teams do the maximum amount of work possible
in the shortest amount of time. By asking that question in such simple manner, you are asking
if you can spend a small amount of time up front to hopefully eliminate any such missteps.
Most stakeholders will grab on to that as a way for them to look good and for the team to
look good, a win-win.&lt;/p&gt;
&lt;h3 id="what-will-these-requirements-look-like"&gt;What will these requirements look like?&lt;a class="headerlink" href="#what-will-these-requirements-look-like" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The requirements will typically come in two main categories.  The first category, requirements
focused on fixing bugs or adding features, will typically be the bulk of the requirements.
Each requirement should outline any negative impact it will have on any of the metrics.  If
nothing is added on negative impacts, the assumption is that the impact will be neutral or
positive.&lt;/p&gt;
&lt;p&gt;A good example of this is a requirement to add a new feature to the project.  The requirement
should be clearly stated using S.M.A.R.T. terminology, because it will remove any
ambiguity in the requirements.  As any source code added without tests would impact any
reliability metrics, reliability tests should be added to meet any seldom violated
metrics for your project.  In similar ways for the other 3 pillars, it is assumed that any
source code added will be a step forward or neutral in terms of quality, not backward.&lt;/p&gt;
&lt;p&gt;At some point in your project, you should expect that at least a few of the requirements
will appear in the the second category: requirements specifically targeted at one or more of
the pillars.  These requirements allow your team to focus on some aspect of your project where
your team feels that the quality can be improved.  The big caveat with these
requirements is to be mindful of the Achievable and Time-Related aspects of S.M.A.R.T.
requirements.  Make sure that whatever the goal of these requirements are, they are things
that won’t go on forever and are not pipe dreams.&lt;/p&gt;
&lt;p&gt;A good example of this is wanting to improve the efficiency of your project or processes.
Without a good requirements that is Specific, Achievable and Time-Related, this can go on
forever. A bad requirement would state something like “Make the project build faster”.  A good
requirement might state something like “Reduce the unit test time from 20 seconds to
under 15 seconds”, timeboxed to 4 hours.  The good requirement has good guard rails on it
to keep it from exploding on someone who picks up that work.&lt;/p&gt;
&lt;h2 id="publishing-software-quality"&gt;Publishing Software Quality&lt;a class="headerlink" href="#publishing-software-quality" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Having gone through the previous sections and any related articles, you should have a
better idea on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how to write better requirements to ask for software quality to be improved&lt;/li&gt;
&lt;li&gt;what metrics I recommend to use for each of the four pillars&lt;/li&gt;
&lt;li&gt;how to measure those metrics and integrate them into your projects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using this information as tools, your team can improve the quality of the project at it’s own
pace, be that either an immediate focus or a long term focus for your project.&lt;/p&gt;
&lt;p&gt;For any metrics that are in the seldom violated category, the best way to approach them is
to make them gatekeeper metrics for your project.  It should be possible to execute a great
many of the gatekeeper metrics before a commit happens, which is optimal.  For the remaining
metrics in the seldom violated category and metrics in the the positive momentum category,
your team should publish those metrics with every commit or push, giving the submitter that
needed feedback.&lt;/p&gt;
&lt;p&gt;In addition, publishing the metrics to some kind of data store allows your team to
determine how the project quality is trending over time, allowing any stakeholders or project
members to observe any potential software quality issues and take steps to deal with them.
Even for certain seldom violated metrics, it can be useful to track how they are trending,
even if they are trending above the gatekeeper lines set for the project.&lt;/p&gt;
&lt;p&gt;If your team does not publish those metrics in some form, the only data point they have for
the project is a binary one: it passes or it does not.  From my experience, that binary
metric is often a false positive that burns teams due to a lack of information.&lt;/p&gt;
&lt;h2 id="what-does-software-quality-mean-to-me"&gt;What Does Software Quality Mean To Me?&lt;a class="headerlink" href="#what-does-software-quality-mean-to-me" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Software quality means each software project has a plan.  When requirements come in to the
project, they are detailed using the S.M.A.R.T. terminology.  If not specifically geared
towards a given software quality pillar, each requirement may specify what kind of impact it
has on one or more of the pillars.  If not specified, it is assumed that it has a neutral or
positive effect on all of the software quality pillars.  The goals are also specific, not
overly broad, and realistically achieved within a given time frame.&lt;/p&gt;
&lt;p&gt;Software quality means that metrics are well thought out for each project.  Each metric is
both defensible and reasonable for that project and that team.  Any metrics that are not being
used as gatekeepers are published so they can be tracked over time.  For additional benefit,
non-binary gatekeeper metrics are also published, to further improve the project and the
quality of the project.&lt;/p&gt;
&lt;p&gt;Software quality means ensuring that software projects are reliable.  Projects have well
thought out tests that are performed at many levels to ensure that the project’s components
work together to meet the project requirements as well as verify the correctness of the
components themselves.  These tests are executed frequently, and a large number of them are
used as gatekeepers, trying to ensure that only reliable changes are made to the project.
When a project is released, the scenario coverage is 100% and the code coverage is either
at 100% or whatever percentage the team has negotiated and documented for their project.&lt;/p&gt;
&lt;p&gt;Software quality means ensuring that software projects are maintainable.  This entails
sufficient documentation of project goals, architecture, design, and current state. The
documentation is coupled with Static Code Analysis to measure a number of maintainability
metrics and to gatekeep on most of them, ensuring that the project moves in a positive
direction to a higher quality project.&lt;/p&gt;
&lt;p&gt;Software quality means ensuring that software projects and their processes are efficient.
Team process to administrate and maintain the software and the software itself do not have to
be blindingly fast, but they need to be as efficient as they need to be for that project and
for that team.  They do not need to be fast as lightning, only fast enough for the software
project itself.&lt;/p&gt;
&lt;p&gt;Software quality means ensuring that software projects are secure.  If third party components
are used for the project, those components need to be monitored for vulnerabilities, and
any issues that arise must be addressed quickly.  Steps are taken, at a level that is
appropriate for the type of software project, to reduce the possible ways that an user can use
the software project do something malicious.&lt;/p&gt;
&lt;p&gt;To me, software quality is about the journey, continuously improving quality and showing
that progress, while adding new features and fixing bugs at the same time.&lt;/p&gt;
&lt;h2 id="wrapping-it-up"&gt;Wrapping It Up&lt;a class="headerlink" href="#wrapping-it-up" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To put it succinctly, software quality for a project is about having a common nomenclature
describing the various pillars of quality, having a common way of measuring against each of
those pillars, and the publishing of those measures.  &lt;/p&gt;
&lt;p&gt;Therefore, from my point of view, software quality is not a single metric but a collection of
metrics and a philosophy.  That philosophy is that your team can only really answer that
question by having clearly defined goals for your project and it’s quality metrics, and
steering the project towards those goals.  &lt;/p&gt;
&lt;p&gt;Does every project need to be super high quality? No, not even close.  But I firmly believe
that each project needs to have a solid understanding of what level of software quality they
have in order to negotiate the definition of “good enough” for each project.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:SDET"&gt;
&lt;p&gt;In the United States, where I currently live, I am a Software Development Engineer in Test or SDET.  I do not have an engineering degree.  In any other country, including my native Canada, I am a Software Developer in Test or SDT. &lt;a class="footnote-backref" href="#fnref:SDET" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:SDE"&gt;
&lt;p&gt;In the United States, where I currently live, a Software Development Engineer or SDE is the same as a Software Developer in any other country. &lt;a class="footnote-backref" href="#fnref:SDE" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:Pillars"&gt;
&lt;p&gt;Based on my experience, where the article breaks out &lt;code&gt;Size&lt;/code&gt; as it’s own pillar, I would place it in the Maintainability section. Similarly, while I can understand why they place &lt;code&gt;Indentifying Critical Programming Errors&lt;/code&gt; in its own section, I would most likely fold half of the items into the Maintainability section and half of them into the Reliability section. To be clear, I agree with the content they present, it is just the organization that I disagree with on two small points. &lt;a class="footnote-backref" href="#fnref:Pillars" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Software Quality"></category><category term="measuring software quality"></category><category term="software reliability"></category><category term="software maintainability"></category><category term="software efficiency"></category><category term="software security"></category></entry></feed>